<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deciphering Life: One Bit at a Time on Deciphering Life: One Bit at a Time</title>
    <link>/</link>
    <description>Recent content in Deciphering Life: One Bit at a Time on Deciphering Life: One Bit at a Time</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Robert M Flight</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Custom Deployment Script</title>
      <link>/post/custom-deployment-script/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/custom-deployment-script/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Use a short bash script to do deployment from your own computer directly to your &lt;code&gt;*.github.io&lt;/code&gt; domain.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;So Yihui recommends using Netlify, or even Travis-CI in the Blogdown book. I wasn’t willing to setup a custom domain yet, and some of my posts involve a lot of personally created packages, etc, that I don’t want to debug installation on Travis. So, I wanted a simple script I could call on my laptop that would copy the &lt;code&gt;/public&lt;/code&gt; directory to the repo for my &lt;code&gt;github.io&lt;/code&gt; site, and then push the changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Script&lt;/h2&gt;
&lt;p&gt;Here is the simple script I ended up using:&lt;/p&gt;
&lt;pre class=&#34;sh&#34;&gt;&lt;code&gt;#!/bin/bash
org_dir=`pwd`
cd path/to/github.io/repo/
#rm -rf *
cp -Rfu path/to/blogdown/public/* .

git add *
commit_time=`date`
git commit -m &amp;quot;update at $commit_time&amp;quot;
git push origin master

cd $org_dir&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It changes directories, because to push from a &lt;code&gt;git&lt;/code&gt; repo I’m pretty sure you need to be in the directory, so it also makes sure to go back there at the end. It then copies the contents of &lt;code&gt;/public&lt;/code&gt; to the repo, &lt;code&gt;add&lt;/code&gt;s all the files, and then uses the current time-stamp as the commit message, and finally pushes all the updates.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Differences in Posted Date vs sessionInfo()</title>
      <link>/post/differences-in-posted-date-vs-sessioninfo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/differences-in-posted-date-vs-sessioninfo/</guid>
      <description>&lt;p&gt;If you are a newcomer to my weblog, you may notice that some posts that are &lt;code&gt;R&lt;/code&gt; tutorials generally include the output of &lt;code&gt;Sys.time()&lt;/code&gt; at the end. If you look closeley at that time and the &lt;strong&gt;Posted on&lt;/strong&gt; date, you may notice that some posts show disagreement between them. This is because I decided to move &lt;em&gt;all&lt;/em&gt; of my old blog posts from &lt;em&gt;blogspot&lt;/em&gt; to here, and keep the original posted dates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linking to Manually Inserted Images in Blogdown / Hugo</title>
      <link>/post/linking-to-manually-inserted-images-in-hugo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linking-to-manually-inserted-images-in-hugo/</guid>
      <description>&lt;div id=&#34;manual-linking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manual Linking?&lt;/h2&gt;
&lt;p&gt;Using &lt;code&gt;blogdown&lt;/code&gt; for generating websites and blog-posts from &lt;code&gt;Rmarkdown&lt;/code&gt; files with lots of inserted code and figures seems pretty awesome, but sometimes you want to include a figure manually, either because you want to generate something manually and convert it (say for going from SVG of lots of points to hi-res PNG), or because it is a figure from something else (&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;like this figure from wikipedia&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-to&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where to??&lt;/h2&gt;
&lt;p&gt;To do this, you want the text of your &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag to your image to be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src = &amp;quot;/img/image_file.png&amp;quot;&amp;gt;&amp;lt;/img&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then put the image itself in the directory &lt;code&gt;/static/img/image_file.png&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/Standard_deviation_diagram.svg&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By M. W. Toews, &lt;a href=&#34;http://creativecommons.org/licenses/by/2.5&#34;&gt;CC BY 2.5&lt;/a&gt;, via Wikimedia Commons, &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This information is also mentioned in &lt;a href=&#34;https://bookdown.org/yihui/blogdown/static-files.html&#34;&gt;section 2.7 of the Blogdown book&lt;/a&gt;. Obviously I need to do more reading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I was Part of the Problem</title>
      <link>/post/i-was-part-of-the-problem/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/i-was-part-of-the-problem/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;With the recent charges of sexual harassment against some high-profile individuals, and so many women coming forward with #metoo (and the understanding that this is really something almost &lt;em&gt;all&lt;/em&gt; women have faced), I realized that my younger self was #partoftheproblem. I think many other men are part of the problem, &lt;strong&gt;even though they might not think so&lt;/strong&gt;. I didn’t think I was part of the problem either. I hope that other men might read this and critically evaluate if they are #partoftheproblem. I also hope and pray that my own sons will do better at this if I teach them right.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-could-i-be&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Could I Be?&lt;/h2&gt;
&lt;p&gt;Let me be up front. I have never &lt;strong&gt;sexually assaulted&lt;/strong&gt; anyone, let alone considered such a thing. But that’s not really the problem, because the way I acted towards women, I think they may have been scared that I might, as I have put tons of &lt;strong&gt;unwanted attention&lt;/strong&gt; on several women over the years, starting with when I was 12 years old, in the sixth grade.&lt;/p&gt;
&lt;p&gt;I also want to be clear, I was a horrible guy friend to women (even if they didn’t think so). If I knew a girl had a boyfriend, well then, I would &lt;strong&gt;not&lt;/strong&gt; even consider trying to hit on or express interest in that girl, and I was “friends” with plenty of women over the course of my school years who had boyfriends. But in &lt;strong&gt;most&lt;/strong&gt; cases I secretly hoped they might dump their boyfriends and go out with me instead. Also, if I knew they didn’t have a boyfriend, and I found them remotely attractive, then I would do all I could to try to become friends with them in the &lt;strong&gt;hope to eventually become their boyfriend&lt;/strong&gt;. So, my sole reason for being friends with women, really, was to eventually become romantically involved. That was my primary motivation. Looking back on it now, it makes me sick.&lt;/p&gt;
&lt;p&gt;I’ve never had a woman tell me she was assaulted by anyone either, but given my past behavior, even if someone I knew had, I don’t think my actions made me someone that a woman would trust to tell.&lt;/p&gt;
&lt;p&gt;Let me give you some examples of my behavior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grade-school&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grade School&lt;/h2&gt;
&lt;p&gt;In 6th grade, I decided that I wanted a girlfriend, and I picked out one girl in my class who I wanted to be my girlfriend. I am very sure I never asked her out, to be my girlfriend, but I made sure to spend tons of time with her, and if I recall correctly, she eventually got the gist of my interest, and told me &lt;strong&gt;very clearly she wasn’t interested&lt;/strong&gt;. But &lt;strong&gt;her telling me no did not stop my unwanted advances&lt;/strong&gt; or attention. I am sure that I made her very uncomfortable the rest of that grade.&lt;/p&gt;
&lt;p&gt;In middle school (7-9 at the time), I pretty much continued this process unabated. I would latch onto a woman that I found attractive, and make her the target of my affections, and pour out my unwanted attention upon her, &lt;strong&gt;not taking no for an answer&lt;/strong&gt;. I only stopped after long periods of continued rejection, or when that person acquired a significant other. Although not an excuse for my actions, my tactics and hopes were largely fueled by rampaging hormones, way too many romantic comedies where the nice guy always got the girl by virtue of sheer persistence (this was the 90’s), and nascent exposure to pornography.&lt;/p&gt;
&lt;p&gt;I would find out girls numbers and call them without being asked. I would know where these girls were at all times through the day, even during lunch and between classes. I would find any excuse to be near them. Every sock-hop (weekly lunch time dance on gym floor) I would ask these girls to dance with me. I would give them valentines cards, Christmas cards, etc, in &lt;strong&gt;the hopes that they would realize what a great guy I was and go out with me.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Just so we are clear, none of this got me any dates in grade school.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;undergraduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Undergraduate&lt;/h2&gt;
&lt;p&gt;Now I’ve graduated high-school, I’m heading off to a local university, with lots of girls. I made lots of friends with girls who had boyfriends, in fact I think my circle of friends had way more girls in it than guys. But, I was always finding one girl who I wanted to date, and would make sure to spend extra time around them, helping them whenever possible, etc, and dropping subtle and not so subtle hints that I wanted to be their boyfriend. And there was always the hope that someone would break-up with their current boyfriend and find me, the faithful friend, waiting to comfort them.&lt;/p&gt;
&lt;p&gt;Over the course of this time, I had three women agree to be my date. Two of those did not result in an actual date, because I started acting like a stalker after they said yes, and they wisely stayed away. In the third case, we went out twice, but me calling at random hours, and showing up at her house un-announced because I thought she was really sad freaked her out, and she stopped talking to my creepy, stalkerish, clingy self.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-graduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-Graduate&lt;/h2&gt;
&lt;p&gt;Somehow, it seems, by the time I got to my PhD, I had &lt;strong&gt;mostly&lt;/strong&gt; given up on finding a girlfriend, settling down and getting married (really, that was my goal). I say mostly. I don’t know if I hadn’t met my now spouse in the first couple of months of my PhD that I would not have continued making unwanted advances on the women in my PhD program. (By the way, I met my spouse outside of work, at a Church actually, and was introduced by a mutual friend. In the 13 years I’ve known her, there are only a handful of days we haven’t talked to each other since we went on our first date).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-real-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Real Problem&lt;/h2&gt;
&lt;p&gt;And this is the &lt;strong&gt;real&lt;/strong&gt; problem. Too many men, my past self included, think women owe them something for being their friend, for being a &lt;strong&gt;nice guy&lt;/strong&gt;. For giving them any kind of attention, or any kind of help. Too many men believe these things, and then use their power and prestige, to demand things of women. Guys, &lt;strong&gt;women don’t owe you anything.&lt;/strong&gt; They definitely don’t owe you sex or reciprocated romantic interest because of something you did for them. They are another person worthy of respect, simply because they are a person.&lt;/p&gt;
&lt;p&gt;In addition, real life is not a romantic comedy. Non-romantic friendships are a good thing, because we need other peoples perspectives in our lives. So, if a woman tells you &lt;strong&gt;no, she doesn’t want to date you&lt;/strong&gt;, accept it, and move on. Don’t make it awkward, especially if you are in the same work environment. &lt;strong&gt;Don’t assume that a woman is romantically interested just because she is friendly.&lt;/strong&gt; I know, radical thought. Maybe try being friends, colleagues, whatever with no romantic intentions, and no expectations of them either. Don’t be #partoftheproblem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solutions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solutions&lt;/h2&gt;
&lt;p&gt;Teach your children that they can be friends with people of the opposite sex without being romantically involved, especially as they hit puberty. Teach them that &lt;strong&gt;no means no&lt;/strong&gt;, not &lt;strong&gt;no means maybe in 3 weeks&lt;/strong&gt;, or &lt;strong&gt;no means maybe if I try hard enough&lt;/strong&gt;. And if you see other men engaging in putting unwanted attention on women, call them out on it, &lt;strong&gt;whatever form it may take&lt;/strong&gt;. I wish someone had said something to me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;I realize that our general culture is really #partoftheproblem, when we have highly sexualized advertising (especially of women to men), and the idea that &lt;strong&gt;boys will be boys&lt;/strong&gt;, tell jokes about sexual assault, and propagate the idea that &lt;strong&gt;women want it&lt;/strong&gt;, based on how they act or dress. Those are all wrong too, and our culture needs to change.&lt;/p&gt;
&lt;p&gt;I also realize that some of what I describe about myself is rather mild in comparison to much of what gets reported, but that’s not the point. It is still unwanted attention, and I didn’t know how to take no for an answer. Those women &lt;strong&gt;didn’t want my attention&lt;/strong&gt;, and I couldn’t accept that. If I had a different temperament, I don’t know what I would have done. Enough people realized it that some friends in Undergrad stopped being around me, but no one ever told me that what I was doing was wrong, and my parents weren’t involved enough in my so-called love life to know what was going on. If they had, I think they would have told me to knock it off and stop being an idiot.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Criticizing a Publication, and Lying About It</title>
      <link>/post/criticizing-a-publication-and-lying-about-it/</link>
      <pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/criticizing-a-publication-and-lying-about-it/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Other researchers &lt;a href=&#34;https://dx.doi/org/10.1002/prot.25024&#34;&gt;directly criticized&lt;/a&gt; a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;recent publication of ours&lt;/a&gt; in a “research article”. Although they raised valid points, they &lt;strong&gt;outright lied&lt;/strong&gt; about the availability of our results. In addition, they did not provide access to their own results. We have published &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;new work&lt;/a&gt; supporting our original results, and a direct rebuttal of their critique in a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;perspective article&lt;/a&gt;. The peer reviewers of their “research article” must have been asleep at the wheel to allow the major point, lack of access to our results, to stand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;original-publication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Original Publication&lt;/h2&gt;
&lt;p&gt;Back in the summer of 2015, I was second author on a publication (&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.24834/full&#34;&gt;Yao et al., 2015&lt;/a&gt;, hereafter YS2015) describing an automated method to characterize zinc ion coordination geometries (CGs). Applying our automated method to all zinc sites in the worldwide Protein Data Bank (wwPDB), we found &lt;em&gt;abberrant&lt;/em&gt; zinc CGs that don’t fit the canonical CGs. We were pretty sure that these aberrant CGs are real, and they have always existed, but had not been previously characterized because methods assumed that only the &lt;em&gt;canonical&lt;/em&gt; geometries should be observed in biological systems, and were excluding the &lt;em&gt;abberrant&lt;/em&gt; ones because they didn’t have good methods to detect and characterize them.&lt;/p&gt;
&lt;p&gt;Also of note, the proteins with aberrant zinc geometries showed enrichment for different types of enzyme classifications than those with canonical zinc geometries.&lt;/p&gt;
&lt;p&gt;For this publication, we made &lt;strong&gt;all&lt;/strong&gt; of our code and results available in a tarball that could be downloaded from our &lt;a href=&#34;http://bioinformatics.cesb.uky.edu/bin/view/Main/SoftwareDevelopment#Metal_ion_coordination_analysis_software&#34;&gt;website&lt;/a&gt;. This data went up while the paper was in review, on Dec 7, 2015 (with a correction on Dec 15). Recently, we’ve also put a copy of the tarball on &lt;a href=&#34;https://figshare.com/articles/Zn_metalloprotein_paper/4229333&#34;&gt;FigShare&lt;/a&gt;. Every draft of the publication, from initial submission through to accepted publication, included the link to the tarball on the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Critique&lt;/h2&gt;
&lt;p&gt;Less than a year later, &lt;a href=&#34;http://sci-hub.cc/doi/10.1002/prot.25024&#34;&gt;Raczynska, Wlodawer, and Jaskolski&lt;/a&gt; (RJW2016) published a &lt;em&gt;critique&lt;/em&gt; of YS2015 as a “research article”. In their publication, they questioned the existence of the &lt;em&gt;abberrant&lt;/em&gt; sites completely, based on the examination and remodeling of four aberrant structures highlighted in YS2015. To be fair, they did have some valid criticisms of the methods, and Sen Yao did a lot of work in our latest paper to address them.&lt;/p&gt;
&lt;p&gt;As part of the critique, however, they claimed that they could only evaluate the four structures listed in two figures &lt;strong&gt;because we didn’t provide all of our results&lt;/strong&gt;. However, we had previously made our full results available as a tarball from our website. As you can see in the below figure, &lt;strong&gt;all&lt;/strong&gt; of the results were really available in that tarball.&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/ys2017_figure1.png&#34;, width = &#34;600&#34;&gt;&lt;/p&gt;
&lt;p&gt;In addition, although RWJ2016 went to all the trouble to actually remodel those four structures by going back to the original X-ray density, they &lt;strong&gt;didn’t make any of their models available&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, no one from RWJ2016 ever contacted our research group to see if the results might be available.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;response&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Response&lt;/h2&gt;
&lt;div id=&#34;follow-up-paper-on-5-metals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Follow-Up Paper on 5 Metals&lt;/h3&gt;
&lt;p&gt;By the time the critiques appeared in RJW2016, Sen was already hard at work showing that the previously developed methods could be modified and then applied to other metal ion CGs, and that they also contained aberrant CGs (see &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;YS2017-1&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique-direct-response&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Critique Direct Response&lt;/h3&gt;
&lt;p&gt;In addition to YS2017-1, we felt that the critique deserved separate response (&lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;YS2017-2&lt;/a&gt;). To that end, we began drafting a response, wherein we pointed out some of the problems with RJW2016, the first being that we did indeed provide the &lt;strong&gt;full&lt;/strong&gt; set of results from YS2015, and therefore it was possible to evaluate our full work. We also addressed each of their other criticisms of YS2015, in many cases going beyond the original criticism, and explaining how it was being addressed in YS2017-1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-results-and-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open Results and Code&lt;/h3&gt;
&lt;p&gt;A major part of the conclusions in YS2017-2 was also devoted to the idea that code and results in science need to be shared, highlighting the fact that RJW2016 &lt;strong&gt;did not share their models&lt;/strong&gt; they used to try and discredit our work, lied about the fact that we did not share our own results, and pointing out some other projects in this research area that have shared well and others that have shared badly, and that the previous attitude of competition among research groups does not move science forward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;peer-review&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer Review&lt;/h2&gt;
&lt;p&gt;Let’s just say that the &lt;em&gt;peer-review&lt;/em&gt; of both of the papers was &lt;strong&gt;interesting&lt;/strong&gt;. Both manuscripts had the same set of reviewers. YS2017-1, the five metal paper, had some rather rigorous peer review, and was definitely improved by the reviewer’s comments. YS2017-2, our perspective, in contrast, was attacked by one peer reviewer right from submission, and was questioned almost continually as to whether it should even be published. I am thankful that one reviewer saw the need for it to be published, and that the Editor ultimately decided that it should be published, and that we were able to rebut each of the reviewer’s criticisms.&lt;/p&gt;
&lt;p&gt;Finally, I really don’t know what happened in the peer review of RWJ2016. The first major claim was that our data wasn’t available, it should have taken a reviewer 10 minutes to verify and debunk that claim. I would have expected a much different critique from the authors had they actually examined our full data set. But, because of traditional closed peer review, that record is closed to us.&lt;/p&gt;
&lt;p&gt;Overall though, I’m very happy both of our publications are now out, and we can move on to new stages of our analyses. Looking forward to continuing to work with my co-authors to move the work forward.&lt;/p&gt;
&lt;div id=&#34;papers-discussed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Papers Discussed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Original Zinc CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;Yao et al 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Critique of Zinc CGs: Raczynska, Wlodawer &amp;amp; Jaskolski 2016, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25024&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://sci-hub.cc/10.1002/prot.25024&#34;&gt;sci-hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5 Metal CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;Yao et al 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Response to critique: Yao et al 2017, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;copy on figshare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PubmedCommons API</title>
      <link>/post/pubmedcommons-api/</link>
      <pubDate>Tue, 22 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/pubmedcommons-api/</guid>
      <description>&lt;p&gt;The announcements are out, Pubmed is introducing a commenting system &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmedcommons/&#34;&gt;pubmedcommons&lt;/a&gt;, theoretically providing a single location for true post-publication peer review. This is a really good idea, as NCBI is likely to be around for a lot longer than a given publisher, and the requirement for all NIH funded research to be deposited into Pubmed.&lt;/p&gt;
&lt;p&gt;There are some detractors, and they may have some valid points &lt;a href=&#34;https://twitter.com/caseybergman/status/392729744640577537&#34;&gt;link&lt;/a&gt;. However, the alternative, &lt;a href=&#34;https://pubpeer.com/&#34;&gt;pubpeer&lt;/a&gt;, I had not heard about. &lt;a href=&#34;http://f1000.com/&#34;&gt;F1000&lt;/a&gt; puts the comments in one location, in a way that is no better than a single publisher site.&lt;/p&gt;
&lt;p&gt;What would truly let the pubmedcommons proliferate would be if it became the equivalent of &lt;a href=&#34;disqus.com&#34;&gt;disqus&lt;/a&gt;, but for scientific articles. For those who don’t know, disqus is essentially a remote commenting platform that provides commenting on other websites. For example, &lt;a href=&#34;http://rmflight.github.io&#34;&gt;my blog&lt;/a&gt; is a static site, and comments are provided by disqus. This is enabled with an account on disqus and a javascript snippet on my website.&lt;/p&gt;
&lt;p&gt;It would be neat if pubmedcommons became a similar platform for scientific articles, with a little bit more flexibility. i.e. someone would be able to include some javascript, with relevant pubmed IDs, and the comments for that article would be displayed, and others would be able to make comments on that site.&lt;/p&gt;
&lt;div id=&#34;let-me-illustrate-with-an-example.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Let me illustrate with an example.&lt;/h3&gt;
&lt;p&gt;Imagine I publish a paper in PLOS One (or &lt;strong&gt;any&lt;/strong&gt; other journal, for that matter). Upon acceptance, the paper also gets a pubmed id. This instantly creates a forum for comments using pubmedcommons. In addition, PLOS One adds a little bit of javascript to their page for the paper that enables the same commenting system. Therefore, any comments made on the paper at pubmed or PLOS One appear in both places (maybe with a tag for where they originated?). In addition, I post a copy of the paper on my own blog, and add the same javascript, and readers will see the comments made at pubmed, PLOS One, and can comment on the paper on my own blog. In addition, someone else could do the same to display the comments on their own site, etc, etc.&lt;/p&gt;
&lt;p&gt;I think the above example might have the potential to revolutionize post-pub peer review, because the actual comments are hosted in a central location, but are accessible in many different locations where the publication might exist.&lt;/p&gt;
&lt;p&gt;Now, it could still go awry. They could take too long to open things up, they might make the policies regarding comments too restrictive, they may police the comments too heavily, etc, etc. But&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Open vs Closed Analysis Languages</title>
      <link>/post/open-vs-closed-analysis-languages/</link>
      <pubDate>Mon, 21 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/open-vs-closed-analysis-languages/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;I think data scientists should choose to learn &lt;strong&gt;open&lt;/strong&gt; languages such as &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt; because they are &lt;strong&gt;open&lt;/strong&gt; in the sense that anyone can obtain them, use them and modify them for free, and this has lead to large, robust groups of users, making it more likely that packages exist that you can use, and others can easily build on your own work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-the-debate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why the debate?&lt;/h2&gt;
&lt;p&gt;This was sparked by a comment on twitter suggesting that data scientists and analysts need to be &lt;em&gt;polyglots&lt;/em&gt;, that they should know more than &lt;strong&gt;one&lt;/strong&gt; programming language or analysis framework (the full conversation of tweets can be found &lt;a href=&#34;https://twitter.com/rmflight/status/387910932250517504&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;
&lt;p&gt;
Data Scientists need to be Polyglots - know 2 or more of &lt;a href=&#34;https://twitter.com/search?q=%23python&amp;amp;src=hash&#34;&gt;#python&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23rstats&amp;amp;src=hash&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23sas&amp;amp;src=hash&#34;&gt;#sas&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23spss&amp;amp;src=hash&#34;&gt;#spss&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23matlab&amp;amp;src=hash&#34;&gt;#matlab&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23julia&amp;amp;src=hash&#34;&gt;#julia&lt;/a&gt; &lt;a href=&#34;https://twitter.com/search?q=%23octave&amp;amp;src=hash&#34;&gt;#octave&lt;/a&gt; &lt;a href=&#34;http://t.co/LtzIOzZ4XH&#34;&gt;http://t.co/LtzIOzZ4XH&lt;/a&gt;
&lt;/p&gt;
— Gregory Piatetsky (&lt;span class=&#34;citation&#34;&gt;@kdnuggets&lt;/span&gt;) &lt;a href=&#34;https://twitter.com/kdnuggets/statuses/387706109509005312&#34;&gt;October 8, 2013&lt;/a&gt;
&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;p&gt;The commenter suggested knowing at least two of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Matlab&lt;/li&gt;
&lt;li&gt;SPSS&lt;/li&gt;
&lt;li&gt;SAS&lt;/li&gt;
&lt;li&gt;Julia&lt;/li&gt;
&lt;li&gt;Octave&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My comment back was that one should really evaluate whether &lt;code&gt;SPSS&lt;/code&gt;, &lt;code&gt;SAS&lt;/code&gt; and &lt;code&gt;Matlab&lt;/code&gt; should be on this list, as they are &lt;em&gt;closed&lt;/em&gt; languages, not open, or free.&lt;/p&gt;
&lt;p&gt;I want to expand on &lt;em&gt;why&lt;/em&gt; I made that comment. Let me be forthright, I have not used &lt;code&gt;SPSS&lt;/code&gt;, nor &lt;code&gt;SAS&lt;/code&gt;, but I have programmed in &lt;code&gt;MatLab&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; extensively, and dabbled in &lt;code&gt;Python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I also think it is a good thing for data scientists to know more than one language.&lt;/strong&gt; Just to be clear, I am &lt;strong&gt;NOT&lt;/strong&gt; arguing that point.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closed&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Closed&lt;/h2&gt;
&lt;p&gt;What is a closed analysis language? I would say that there are three types of closed languages:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;those that are not free (but still may their source code openly available)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;those where the underlying engine is closed source&lt;/li&gt;
&lt;li&gt;those where one cannot write their own functions to expand on what is already available&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, &lt;code&gt;MatLab&lt;/code&gt; fits the first two categories. It is rather expensive to get a license for, the license &lt;strong&gt;can&lt;/strong&gt; be restrictive (I know they have had a lot of abuse of licenses in the past, and they are trying to avoid that), and you are not expected to poke around in the internals of the &lt;code&gt;MatLab&lt;/code&gt; engine. Oh, and if you want more than the base engine, expect to pay heavily for add-on packages.&lt;/p&gt;
&lt;p&gt;However, it is possible to write add-on’s for &lt;code&gt;MatLab&lt;/code&gt;. I have previously &lt;a href=&#34;http://www.mathworks.com/matlabcentral/fileexchange/authors/19294&#34;&gt;written a few&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;closed-problem-checking-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closed problem: Checking results&lt;/h3&gt;
&lt;p&gt;So why are closed languages a problem? A closed language that does not make it &lt;strong&gt;possible&lt;/strong&gt; to examine the underlying functionality of the analysis engine has two problems:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;surety that calculations are done correctly&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;the ability of others to run and check results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both of these are issues that are really important in &lt;strong&gt;science&lt;/strong&gt;. I would consider a &lt;strong&gt;data scientist&lt;/strong&gt; to be doing actual &lt;strong&gt;science&lt;/strong&gt;, so others should be able to scrutinize their work. The best scrutiny, is for others to be able to actually run their code. If I can’t run your code, then how do I know what you did is right?? If I can’t afford a copy of &lt;code&gt;MatLab&lt;/code&gt; to run your code (assuming you made it available, you did provide the source for the analysis, right?), that is a bad thing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closed-problem-re-using-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closed problem: Re-using code&lt;/h3&gt;
&lt;p&gt;Of course, the other problem is that with a closed language you have made it impossible for others to easily make use of your analysis. Sure, they could code it up in another language, but unless it is the be-all and end-all of analysis methods, I’m not going to bother. I don’t have a license for &lt;code&gt;MatLab&lt;/code&gt;, or &lt;code&gt;SPSS&lt;/code&gt;, or &lt;code&gt;SAS&lt;/code&gt;, and I can’t afford it; therefore I’m not going to use your method / code nor give you a citation or credit.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-open-languages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution: Open languages&lt;/h2&gt;
&lt;p&gt;Languages like &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt;, they don’t have these problems. If I wonder how a function in the base distribution of &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;python&lt;/code&gt; works, I can go look at the source. If I find a bug, I can suggest a fix, or fix it myself and tell others about it. In addition, if I write code to do an analysis, I can make it available and know that others &lt;strong&gt;should&lt;/strong&gt; have the ability to examine it, including re-running it, in addition to using it for themselves, if it is licensed appropriately. This is the way science should work.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-should-you-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What should you use?&lt;/h2&gt;
&lt;p&gt;Some would argue that you should use &lt;code&gt;MatLab&lt;/code&gt;, &lt;code&gt;SAS&lt;/code&gt;, and &lt;code&gt;SPSS&lt;/code&gt; because they have been around for a while, and are the standard. I would argue that you should not use them because they are controlled by single corporate entities, who are only interested in what will get people to buy their product and use it. You should use software that &lt;a href=&#34;http://r4stats.com/articles/popularity/&#34;&gt;others are using&lt;/a&gt;, and that others will be able to use, regardless of income.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is being used in lots of different places, by lots of different people for statistics, bioinformatics, visualization, and as a general functional language. &lt;code&gt;Python&lt;/code&gt; is a great general purpose language that provides a lot of &lt;strong&gt;functional&lt;/strong&gt; glue for doing lots of different things.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pre-Calculating Large Tables of Values</title>
      <link>/post/pre-calculating-large-tables-of-values/</link>
      <pubDate>Thu, 17 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/pre-calculating-large-tables-of-values/</guid>
      <description>&lt;p&gt;I’m currently working on a project where we want to know, based on a euclidian distance measure, what is the probability that the value is a match to the another value. &lt;em&gt;i.e.&lt;/em&gt; given an actual value, and a theoretical value from calculation, what is the probability that they are the same? This can be calculated using a &lt;strong&gt;chi-square&lt;/strong&gt; distribution with one degree-of-freedom, easily enough by considering how much of the chi-cdf we are taking up.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;pMatch &amp;lt;- 1 - pchisq(distVal, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The catch is, we want to do this a whole lot of times, in &lt;code&gt;c++&lt;/code&gt;. We could use the &lt;code&gt;boost&lt;/code&gt; library to calculate the &lt;strong&gt;chi-square&lt;/strong&gt; each time we need it. Or we could generate a lookup table that is able to find the p-value simply based on the distance. This is especially attractive if we have a limit past which we consider the probability of a match as being zero, and if we use enough decimal points that we don’t suffer too much in precision.&lt;/p&gt;
&lt;p&gt;Although our goal is to implement this in &lt;strong&gt;c++&lt;/strong&gt;, I also want to prototype, demonstrate and evaluate the approach in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R&lt;/h2&gt;
&lt;div id=&#34;random-number-set&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random number set&lt;/h3&gt;
&lt;p&gt;We are going to consider 25 (5 standard deviations squared) as our cutoff for saying the probability is zero. So to make sure we are doing all calculations using the exact same thing, we will pre-generate the values for testing on &lt;strong&gt;real&lt;/strong&gt; data, in this case a set of 1000000 random numbers from zero to 25.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nPoint &amp;lt;- 1000000
randomData &amp;lt;- abs(rnorm(nPoint, mean=5, sd=5)) # take absolute so we have only positive values
randomData[randomData &amp;gt; 25] &amp;lt;- 25
hist(randomData, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2013-10-17-pre-calculating-large-tables-of-values_files/figure-html/genRandomData-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-r-way&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The R way&lt;/h3&gt;
&lt;p&gt;Of course, the way to calculate &lt;strong&gt;p-values&lt;/strong&gt; for all this data in &lt;code&gt;R&lt;/code&gt; is to do the whole vector at once. How long does that take?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bTime &amp;lt;- Sys.time()
actPVals &amp;lt;- 1 - pchisq(randomData, 1)
eTime &amp;lt;- Sys.time()
rwayDiff &amp;lt;- difftime(eTime, bTime)
rwayDiff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 0.5137203 secs&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;naive-way&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Naive way&lt;/h3&gt;
&lt;p&gt;A naive way to do this is to do it piecemeal, in a for loop. I will pre-allocate the result vector so we don’t take a hit on memory allocation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;naiveRes &amp;lt;- numeric(nPoint)
bTime &amp;lt;- Sys.time()
for (iP in 1:nPoint){
  naiveRes[iP] &amp;lt;- 1 - pchisq(randomData[iP], 1)
}
eTime &amp;lt;- Sys.time()
naiveDiff &amp;lt;- difftime(eTime, bTime)
naiveDiff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 1.418367 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this takes almost 10 times longer. Which of course, is why you are encouraged to do vectorized calculations whenever possible in &lt;code&gt;R&lt;/code&gt;, but you already knew that, didn’t you?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lookup-table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lookup table&lt;/h3&gt;
&lt;p&gt;What if we now store a set of p-value results in a table, doing it in such a way as to use the actual distance as an index into the table?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nDivision &amp;lt;- 10000
dof &amp;lt;- 1
nSD &amp;lt;- 25

nElements &amp;lt;- nSD * nDivision

chiVals &amp;lt;- seq(0, nElements, 1) / nDivision # the chi-squared values (or distances), also used as indices when multiplied by 10000

pTable &amp;lt;- 1 - pchisq(chiVals, 1) # the actual chi-square p-values for those distances&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To find a value, we just multiply the distance by 10000 (the number of divisions), and add 1 (because &lt;code&gt;R&lt;/code&gt; uses 1 based indexing instead of zero).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testVal &amp;lt;- sample(chiVals, 1) # grab a value from the chiVals previously generated
pTable[(testVal * nDivision) + 1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1577358&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 - pchisq(testVal, 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1577358&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How long does this take compared to the other approaches?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tableRes &amp;lt;- numeric(nPoint)
bTime &amp;lt;- Sys.time()
for (iP in 1:nPoint){
  tableRes[iP] &amp;lt;- pTable[(randomData[iP] * nDivision) + 1]
}
eTime &amp;lt;- Sys.time()
tableDiff &amp;lt;- difftime(eTime, bTime)
tableDiff&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Time difference of 0.09745979 secs&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So somewhere in-between the two. So not as good as doing a vectorized call, but better than making a call each time. Which is actually what I expected. What about any loss in precision of the values returned?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tableRawPrecision &amp;lt;- abs(tableRes - actPVals) / actPVals * 100

precTable &amp;lt;- data.frame(org=actPVals, table=tableRes, percError=tableRawPrecision)
ggplot(precTable, aes(x=org, y=table)) + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2013-10-17-pre-calculating-large-tables-of-values_files/figure-html/tablePres-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(precTable, aes(x=org, y=percError)) + geom_point()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2013-10-17-pre-calculating-large-tables-of-values_files/figure-html/tablePres-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, according to this, we are only introducing error at 0.8037041%, which isn’t much. And the values look like the are well correlated, so we should be good.&lt;/p&gt;
&lt;p&gt;Now, how do these approaches compare when using &lt;code&gt;c++&lt;/code&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;c&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;C++&lt;/h2&gt;
&lt;p&gt;So it’s a fair comparison, the code below actually writes the &lt;code&gt;c++&lt;/code&gt; program we are going to use, with the random numbers for the &lt;strong&gt;p-value&lt;/strong&gt; calculation stored as part of the code file.&lt;/p&gt;
&lt;p&gt;A couple of notes:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To be fair, both versions of the code have the set of random numbers and the lookup table as &lt;code&gt;float&lt;/code&gt; variables, so that there is no difference in each for memory allocation.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Neither one stores the results of the calculation, we don’t need it for this demonstration.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;raw-calculations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Raw calculations&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cppRaw &amp;lt;- c(&amp;#39;#include &amp;lt;iostream&amp;gt;&amp;#39;,
               &amp;#39;#include &amp;lt;boost/math/distributions/chi_squared.hpp&amp;gt;&amp;#39;,
               &amp;#39;int nVal = 1000000;&amp;#39;,
               &amp;#39;double dof = 1.0;&amp;#39;,
               &amp;#39;int i;&amp;#39;,
               paste(&amp;#39;float randVals[1000000] = {&amp;#39;, paste(as.character(randomData), sep=&amp;quot;&amp;quot;, collapse=&amp;quot;, &amp;quot;), &amp;#39;};&amp;#39;, sep=&amp;quot;&amp;quot;, collapse=&amp;quot;&amp;quot;),
               paste(&amp;#39;float pTable[250001] = {&amp;#39;, paste(as.character(pTable), sep=&amp;quot;&amp;quot;, collapse=&amp;quot;, &amp;quot;), &amp;#39;};&amp;#39;, sep=&amp;quot;&amp;quot;, collapse=&amp;quot;&amp;quot;),
               &amp;#39;int main() {&amp;#39;,
               &amp;#39;using boost::math::chi_squared_distribution;&amp;#39;,
               &amp;#39;chi_squared_distribution&amp;lt;&amp;gt; myChi(dof);&amp;#39;,
               &amp;#39;for (i = 0; i &amp;lt; nVal; i++){&amp;#39;,
               &amp;#39;1 - cdf(myChi, randVals[i]);&amp;#39;,
               &amp;#39;};&amp;#39;,
               &amp;#39;return(0);&amp;#39;,
               &amp;#39;};&amp;#39;)
cat(cppRaw, sep=&amp;quot;\n&amp;quot;, file=&amp;quot;cppRaw.cpp&amp;quot;)

system(&amp;quot;g++ cppRaw.cpp -o cppRaw.out&amp;quot;)
system(&amp;quot;time ./cppRaw.out&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cppLookup &amp;lt;- c(&amp;#39;#include &amp;lt;iostream&amp;gt;&amp;#39;,
               &amp;#39;#include &amp;lt;boost/math/distributions/chi_squared.hpp&amp;gt;&amp;#39;,
               &amp;#39;int nVal = 1000000;&amp;#39;,
               &amp;#39;double dof = 1.0;&amp;#39;,
               &amp;#39;int i;&amp;#39;,
               paste(&amp;#39;float randVals[1000000] = {&amp;#39;, paste(as.character(randomData), sep=&amp;quot;&amp;quot;, collapse=&amp;quot;, &amp;quot;), &amp;#39;};&amp;#39;, sep=&amp;quot;&amp;quot;, collapse=&amp;quot;&amp;quot;),
               paste(&amp;#39;float pTable[250001] = {&amp;#39;, paste(as.character(pTable), sep=&amp;quot;&amp;quot;, collapse=&amp;quot;, &amp;quot;), &amp;#39;};&amp;#39;, sep=&amp;quot;&amp;quot;, collapse=&amp;quot;&amp;quot;),
               &amp;#39;int main() {&amp;#39;,
               &amp;#39;using boost::math::chi_squared_distribution;&amp;#39;,
               &amp;#39;chi_squared_distribution&amp;lt;&amp;gt; myChi(dof);&amp;#39;,
               &amp;#39;for (i = 0; i &amp;lt; nVal; i++){&amp;#39;,
               &amp;#39;pTable[(int(randVals[i] * nVal))];&amp;#39;,
               &amp;#39;};&amp;#39;,
               &amp;#39;return(0);&amp;#39;,
               &amp;#39;};&amp;#39;)
cat(cppLookup, sep=&amp;quot;\n&amp;quot;, file=&amp;quot;cppLookup.cpp&amp;quot;)
system(&amp;quot;g++ cppLookup.cpp -o cppLookup.out&amp;quot;)
system(&amp;quot;time ./cppLookup.out&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So bypassing &lt;code&gt;boost&lt;/code&gt; in this case is a good thing, we get some extra speed, and reduce a dependency. We have to generate the lookup table first, but the &lt;code&gt;cpp&lt;/code&gt; file can be generated once, with a static variable in a class that is initialized to the lookup values. We do have some error, but in our case we can live with it, as the relative rankings should still be pretty good.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Portable, Peronal Packages</title>
      <link>/post/portable-peronal-packages/</link>
      <pubDate>Mon, 23 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/portable-peronal-packages/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.programmingr.com/&#34;&gt;ProgrammingR&lt;/a&gt; had an interesting &lt;a href=&#34;http://www.programmingr.com/content/creating-personal-portable-r-code-library-github/&#34;&gt;post&lt;/a&gt; recently about keeping a set of &lt;code&gt;R&lt;/code&gt; functions that are used often as a &lt;a href=&#34;https://gist.github.com/discover&#34;&gt;&lt;code&gt;gist&lt;/code&gt;&lt;/a&gt; on &lt;a href=&#34;https://github.com&#34;&gt;Github&lt;/a&gt;, and &lt;code&gt;source&lt;/code&gt;ing that file at the beginning of &lt;code&gt;R&lt;/code&gt; analysis scripts. There is nothing inherently wrong with this, but it does end up cluttering the user workspace, and there is no real documentation on the functions, and no good way to implement &lt;a href=&#34;http://cran.r-project.org/web/packages/testthat/index.html&#34;&gt;unit testing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, the best way to have &lt;strong&gt;sets&lt;/strong&gt; of &lt;code&gt;R&lt;/code&gt; functions is as a &lt;a href=&#34;http://adv-r.had.co.nz/Package-basics.html&#34;&gt;&lt;code&gt;package&lt;/code&gt;&lt;/a&gt;, that can then be installed and loaded by anyone. Normally packages are compiled and hosted on &lt;a href=&#34;http://cran.r-project.org/&#34;&gt;&lt;code&gt;CRAN&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;http://r-forge.r-project.org/&#34;&gt;&lt;code&gt;R-forge&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&#34;http://bioconductor.org&#34;&gt;&lt;code&gt;Bioconductor&lt;/code&gt;&lt;/a&gt;. However, Github is becoming a common place to host packages, and thanks to Hadley Wickham’s &lt;a href=&#34;http://www.rdocumentation.org/packages/devtools/functions/install_github&#34;&gt;&lt;code&gt;install_github&lt;/code&gt;&lt;/a&gt; function in the &lt;a href=&#34;http://www.rdocumentation.org/packages/devtools&#34;&gt;&lt;code&gt;devtools&lt;/code&gt; package&lt;/a&gt;, it is rather easy to install a package directly from Github. This does require that you have &lt;a href=&#34;http://cran.r-project.org/bin/windows/Rtools/&#34;&gt;r-tools&lt;/a&gt; installed if you are using Windows (I know that can take a bit of work, but it’s not impossible), and do some extra work to create a &lt;a href=&#34;http://adv-r.had.co.nz/Package-basics.html&#34;&gt;proper package&lt;/a&gt;, but the overhead is probably worth it if you are using these functions all the time.&lt;/p&gt;
&lt;p&gt;Once you have the package created and hosted on Github, it is simple to install it once, and load it when needed. If there is a particular version of the package that is required, it is even possible to tell &lt;code&gt;install_github&lt;/code&gt; to install a particular version based on the &lt;em&gt;commit&lt;/em&gt;, or a &lt;em&gt;tag&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Some examples of this type of package can be found on Github: &lt;a href=&#34;https://github.com/kbroman/broman&#34;&gt;1&lt;/a&gt; &lt;a href=&#34;https://github.com/childsish/lhc-R&#34;&gt;2&lt;/a&gt; &lt;a href=&#34;https://github.com/juba/r-perso&#34;&gt;3&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K-12 Wants Scientists!!</title>
      <link>/post/k-12-wants-scientists/</link>
      <pubDate>Thu, 19 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/k-12-wants-scientists/</guid>
      <description>&lt;p&gt;It seems that the PostDoc committee here at UK has an interest in providing some information on alternative careers for PostDocs outside of academia. We all know that there are not enough PI slots at universities for all of the PhDs who are going on to do PostDocs, and many will end up in &lt;em&gt;alternative&lt;/em&gt; (I use that term loosely) careers.&lt;/p&gt;
&lt;p&gt;Yesterday (2013-09-18), Scott Diamond gave a &lt;em&gt;seminar&lt;/em&gt; to PostDocs in the Medical Center at UK on getting involved in teaching science at the K-12 and college level. Scott was previously a professor here at UK, who got involved in science teaching at local schools, saw how miserable many students were in science classes, and wanted to change that. So he quit academia, got his teaching certification, and is now teaching science at a school for high-risk youth, &lt;a href=&#34;http://www.tlc.fcps.net/&#34;&gt;The Learning Center&lt;/a&gt;, here in Fayette County.&lt;/p&gt;
&lt;p&gt;This school takes 180 of the kids who are extremely truant, i.e. they like to skip school. Of course, we know that poverty is a big factor in the decision to skip school, but so is boredom from lack of engagement with the material. At The Learning Center they have been finding that if you find ways to make the material engaging (especially science), then the kids will &lt;em&gt;want&lt;/em&gt; to be at school, learning, and they will still do well on the standard testing. Scott mentioned that these kids in regular school were truant at least 50% of the time, and at The Learning Center they have an attendance rate of 95%.&lt;/p&gt;
&lt;p&gt;As scientists, we are out to make discoveries. We look at something happening in the natural world, make hypotheses, and test them. Even in bioinformatics, the goal is to use computers to analyze biological data so we can understand better how these biological systems work, and make better hypotheses and test them. This is not how science is taught in K-12 right now here in the US. In many school systems, it is merely recitation of science facts, with no actual “investigational” science going on.&lt;/p&gt;
&lt;p&gt;Scott (and others) are working to change this. But to do it right, they need trained scientists in the system. So if you are thinking about doing something other than academia following your PostDoc, and want to help kids be truly interested in science and how it works, maybe you should consider going into teaching K-12??&lt;/p&gt;
&lt;p&gt;If you want more info about The Learning Center, about their model, or about getting into teaching at this level, you can reach Scott at &lt;code&gt;scott&lt;/code&gt; dot &lt;code&gt;diamond&lt;/code&gt; near &lt;code&gt;uky&lt;/code&gt; circular shape &lt;code&gt;edu&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>R, RStudio, and Release and Dev Bioconductor</title>
      <link>/post/r-rstudio-and-release-and-dev-bioconductor/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/r-rstudio-and-release-and-dev-bioconductor/</guid>
      <description>&lt;p&gt;I have one &lt;a href=&#34;http://bioconductor.org/packages/release/bioc/html/categoryCompare.html&#34;&gt;&lt;code&gt;Bioconductor&lt;/code&gt; package&lt;/a&gt; that I am currently responsible for. Each &lt;a href=&#34;http://master.bioconductor.org/developers/release-schedule/&#34;&gt;bi-annual release&lt;/a&gt; of &lt;code&gt;Bioconductor&lt;/code&gt; requires testing and squashing errors, warnings and bugs in a given package. Doing this means being able to work with multiple versions of &lt;code&gt;R&lt;/code&gt; and multiple versions of &lt;code&gt;Bioconductor&lt;/code&gt; libraries on a single system (assuming that you do production work and development on the same machine, right?).&lt;/p&gt;
&lt;p&gt;I really, really like &lt;a href=&#34;http://rstudio.org&#34;&gt;&lt;code&gt;RStudio&lt;/code&gt;&lt;/a&gt; as my working &lt;code&gt;R&lt;/code&gt; environment, as some of you have read before. So how do we get &lt;code&gt;RStudio&lt;/code&gt; on our Linux system to respect which version of &lt;code&gt;R&lt;/code&gt; and libraries we want to use?&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;This assumes that you have your &lt;code&gt;R&lt;/code&gt; installs set somewhere properly, and a &lt;strong&gt;normal&lt;/strong&gt; library for production level packages. You should install whichever &lt;code&gt;Bioconductor&lt;/code&gt; packages you want into the &lt;strong&gt;normal&lt;/strong&gt; library, and then make a copy of that. This copy will be your &lt;strong&gt;development&lt;/strong&gt; library.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cp -R productionLibrary developmentLibrary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also assume that you are using a local (i.e. sits in your &lt;em&gt;home&lt;/em&gt; directory) &lt;code&gt;.Renviron&lt;/code&gt; file to control where &lt;code&gt;R&lt;/code&gt; installs the packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;changing-versions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Changing Versions&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;RStudio&lt;/code&gt; really needs the environment variable &lt;code&gt;RSTUDIO_WHICH_R&lt;/code&gt; set to know where &lt;code&gt;R&lt;/code&gt; is, and &lt;code&gt;R&lt;/code&gt; looks at &lt;code&gt;R_LIBS&lt;/code&gt; in the &lt;code&gt;.Renviron&lt;/code&gt; file. So I simply create two shell scripts that get sourced.&lt;/p&gt;
&lt;div id=&#34;userdev.sh&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;useRDev.sh&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/sh
export RSTUDIO_WHICH_R=/pathToDevR/bin/R
echo &amp;quot;R_LIBS=pathtoDevLibs&amp;quot; &amp;gt; .Renviron&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I can simply do &lt;code&gt;source useRDev.sh&lt;/code&gt; when I need to use the development &lt;code&gt;R&lt;/code&gt; and library. &lt;strong&gt;Note&lt;/strong&gt; that you will need to start &lt;code&gt;RStudio&lt;/code&gt; from the shell for it to respect this environment variable. &lt;code&gt;RStudio&lt;/code&gt; generally seems to install to &lt;code&gt;/usr/lib/rstudio/bin/rstudio&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resetr.sh&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;resetR.sh&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/sh
export RSTUDIO_WHICH_R=/pathtoReleaseR/bin/R
echo &amp;quot;R_LIBS=pathtoReleaseLibs&amp;quot; &amp;gt; .Renviron&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This resets my variables by doint &lt;code&gt;source resetR.sh&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bioconductor-dev-version&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bioconductor Dev Version&lt;/h2&gt;
&lt;p&gt;To setup &lt;code&gt;Bioconductor&lt;/code&gt; to use the develoment version, simply:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;source useRDev.sh
rstudio

# once in RStudio
library(BiocInstaller)
useDev()
biocLite(ask=F) # this will update all the installed bioconductor packages&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I know this is not the most ideal situation, because I am rewriting over files, but it is working for me, and I thought it might help somone else.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Reproducible Methods</title>
      <link>/post/reproducible-methods/</link>
      <pubDate>Fri, 16 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/reproducible-methods/</guid>
      <description>&lt;p&gt;Science is built on the whole idea of being able to reproduce results, i.e. if I publish something, it should be possible for someone else to reproduce it, using the description of the methods used in the publication. As biological sciences have become increasingly reliant on computational methods, this has become a bigger and bigger issue, especially as the results of experiments become dependent on independently developed computational code, or use rather sophisticated computer packages that have a variety of settings that can affect output, and multiple versions. For further discussion on this issue, you might want to read &lt;a href=&#34;http://bytesizebio.net/index.php/2012/08/24/can-we-make-research-software-accountable/&#34;&gt;1&lt;/a&gt;, &lt;a href=&#34;http://ivory.idyll.org/blog/anecdotal-science.html&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I recently read a couple of different publications that really made me realize how big a problem this is. I want to spend some time showing what the problem is in these publications, and why we should be concerned about the current state of computational analytical reproducibility in life-sciences.&lt;/p&gt;
&lt;p&gt;In both the articles mentioned below, I do not believe that I, or anyone not associated with the project, would be able to generate even approximately similar results based solely on the raw data and the description of methods provided. Ultimately, this is a failure of both those doing the analysis, and the reviewers who reviewed the work, and is a rather deplorable situation for a field that prides itself verification of results. This is why I’m saying these are &lt;strong&gt;bad bioinformatics methods sections&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;puthanveettil-et-al.-synaptic-transcriptome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Puthanveettil et al., Synaptic Transcriptome&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1073/pnas.1304422110&#34;&gt;Puthanveettil et al, 2013&lt;/a&gt; had a paper out earlier titled &lt;a href=&#34;http://doi.org/10.1073/pnas.1304422110&#34;&gt;“A strategy to capture and characterize the synaptic transcriptome”&lt;/a&gt; in PNAS. Although the primary development reported is a new method of characterizing RNA complexes that are carried by kinesin, much of the following analysis is bioinformatic in nature.&lt;/p&gt;
&lt;p&gt;For example, they used BLAST searches to identify the RNA molecules, a cutoff value is reported in the results. However, functional characterization using Gene Ontology (GO) was carried out by “Bioinformatics analyses” (see the top of pg3 in the PDF). No mention of where the GO terms came from, which annotation source was used, or any software mentioned. Not in the results, discussion, or methods, or the supplemental methods. The microarray data analysis isn’t too badly described, but the 454 sequencing data processing isn’t really described at all.&lt;/p&gt;
&lt;p&gt;My point is, that even given their raw data, I’m not sure I would be able to even approximate their results based on the methods reported in the methods section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gulsuner-et-al.-schizophrenia-snps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gulsuner et al., Schizophrenia SNPs&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dx.doi.org/10.1016/j.cell.2013.06.049&#34;&gt;Gulsuner et al&lt;/a&gt; published a paper in Cell in August 2013 titled &lt;a href=&#34;http://dx.doi.org/10.1016/j.cell.2013.06.049&#34;&gt;“Spatial and Temporal Mapping of De Novo Mutations in Schizophrenia to a Fetal Prefrontal Cortical Network”&lt;/a&gt;. This one also looks really nice, they look for &lt;em&gt;de novo&lt;/em&gt; mutations (i.e. new mutations in offspring not present in parents or siblings) that mess up genes that are in a heavily connected network, and also examine gene co-expression over brain development time-scales. Sounds really cool, and the results seem like they are legit, based on my reading of the manuscript. I was really impressed that they even used &lt;strong&gt;randomly generated&lt;/strong&gt; networks to control the false discovery rate!&lt;/p&gt;
&lt;p&gt;However, almost all of the analysis again depends on a lot of different bioinformatic software. I do have to give the authors props, they actually give the &lt;strong&gt;full&lt;/strong&gt; version of each tool used. But no mention of tool specific settings (which can generate vastly different results, see &lt;strong&gt;Exome Sequencing&lt;/strong&gt; of the methods).&lt;/p&gt;
&lt;p&gt;Then there is this bombshell: “The predicted functional impact of each candidate de novo missense variant was assessed with in silico tools.” (near top of pg 525 of the PDF). Rrrreeeaaaalllly now. No actual quote of which tools were used, although the subsequent wording and references provided imply that they were &lt;a href=&#34;http://www.nature.com/nmeth/journal/v7/n4/full/nmeth0410-248.html&#34;&gt;PolyPhen2&lt;/a&gt;, &lt;a href=&#34;http://sift.jcvi.org/&#34;&gt;SIFT&lt;/a&gt;, and the &lt;a href=&#34;http://www.sciencemag.org/content/185/4154/862.long&#34;&gt;Grantham Method&lt;/a&gt;. But shouldn’t that have been stated up front? Along with any settings that were changed from default??&lt;/p&gt;
&lt;p&gt;There is no raw data available, only their reported SNPs. Not even a list of &lt;strong&gt;all&lt;/strong&gt; the SNPs that were potentially considered, so that I could at least go from those and re-run the later analysis. I have to take their word for it (although I am glad at least the SNPs they used in later analyses are reported).&lt;/p&gt;
&lt;p&gt;Finally, the random network generation. I’d like to be able to see that code, go over it, and see what exactly it was doing to verify it was done correctly. It likely was, based on the histograms provided, but still, these are where small errors creep in and result in invalid results.&lt;/p&gt;
&lt;p&gt;As above, even if the raw data was available (didn’t see an SRA accession or any other download link), I’m not sure I could reproduce or verify the results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-to-do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What to do??&lt;/h2&gt;
&lt;p&gt;How do we fix this problem? I think scripts and workflows used to run any type of bioinformatic analyses have to become first class research objects. And we have to teach scientists to write them and use them in a way that makes them first class research objects. So in the same way that a biologist might ask for verification of immunostaining, etc, bioinformaticians should ask that given known input, a script generates &lt;em&gt;reasonable&lt;/em&gt; output.&lt;/p&gt;
&lt;p&gt;I know there has been discussion on this before, and disagreement, especially with the exploratory nature of research. However, once you’ve got something working &lt;em&gt;right&lt;/em&gt;, you should be able to &lt;em&gt;test&lt;/em&gt; it. Reviewers should be asking if it is testable, or the code should be available for others to test.&lt;/p&gt;
&lt;p&gt;I also think we as a community should do more to point out the problem. i.e. when we see it, point it out to others. I’ve done that here, but I don’t know how much should be formal. Maybe we need a new hashtag, #badbioinfomethodsection, and point links to papers that do this. Conversely, we should also point to examples when it is done right (#goodbioinfomethodsection??), and if you are bioinformatician or biologist who does a lot of coding, share your code, and at least supply it as supplemental materials. Oh, and maybe take a &lt;a href=&#34;http://softwarecarpentry.org&#34;&gt;SoftwareCarpentry&lt;/a&gt; class, and look up &lt;a href=&#34;http://git-scm.com/&#34;&gt;git&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Posted on August 16, 2013 at &lt;a href=&#34;http://robertmflight.blogspot.com/2013/08/reproducible-methods-or-bad.html&#34; class=&#34;uri&#34;&gt;http://robertmflight.blogspot.com/2013/08/reproducible-methods-or-bad.html&lt;/a&gt;, raw markdown at &lt;a href=&#34;https://github.com/rmflight/blogPosts/blob/master/reproducible_methods.md&#34; class=&#34;uri&#34;&gt;https://github.com/rmflight/blogPosts/blob/master/reproducible_methods.md&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R Interface for Teaching</title>
      <link>/post/r-interface-for-teaching/</link>
      <pubDate>Thu, 11 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/r-interface-for-teaching/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://twitter.com/intent/user?screen_name=kaythaney&amp;amp;tw_i=354635159447941120&amp;amp;tw_p=tweetembed&#34;&gt;Kaitlin Thaney&lt;/a&gt; asked on Twitter last week about using &lt;a href=&#34;https://twitter.com/intent/user?screen_name=ramnath_vaidya&amp;amp;tw_i=354599459868508160&amp;amp;tw_p=tweetembed&#34;&gt;Ramnath Vaidyanathan’s&lt;/a&gt; new &lt;code&gt;interactive R notebook&lt;/code&gt; &lt;a href=&#34;http://ramnathv.github.io/rNotebook/&#34;&gt;1&lt;/a&gt; &lt;a href=&#34;https://github.com/ramnathv/rNotebook&#34;&gt;2&lt;/a&gt; for teaching.&lt;/p&gt;
&lt;p&gt;Now, to be clear up front, I am &lt;strong&gt;not&lt;/strong&gt; trying to be mean to Ramnath, discredit his work, or the effort that went into that project. I think it is really cool, and has some rather interesting potential applications, but I don’t really think it is the right interface for teaching &lt;code&gt;R&lt;/code&gt;. I would argue that the best interface for teaching &lt;code&gt;R&lt;/code&gt; right now is &lt;a href=&#34;http://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt;. Keep reading to find out why.&lt;/p&gt;
&lt;div id=&#34;ipython-notebook&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;iPython Notebook&lt;/h2&gt;
&lt;p&gt;First, I believe Ramnath when he says he was inspired by the &lt;a href=&#34;http://ipython.org/notebook.html&#34;&gt;&lt;code&gt;iPython Notebook&lt;/code&gt;&lt;/a&gt; that makes it so very nice to do interactive, reproducible Python coding. Software Carpentry has been very successfully using them for helping to teach Python to scientists.&lt;/p&gt;
&lt;p&gt;However, the iPython Notebook is an interesting beast for this purpose. You are able to mix &lt;code&gt;markdown&lt;/code&gt; blocks and &lt;code&gt;code&lt;/code&gt; blocks. In addition, it is extremely simple to break up your calculations into &lt;strong&gt;units&lt;/strong&gt; of related code, and re-run those units as needed. This is particularly useful when writing new functions, because you can write the function definition, and a test that displays output in one block, and then the actual computations in subsequent blocks. It makes it very easy to keep re-running the same block of code over and over until it is correct, which allows one to interactively explore changes to functions. This is &lt;strong&gt;awesome&lt;/strong&gt; for learning Python and prototyping functions.&lt;/p&gt;
&lt;p&gt;In addition to being able to repeatedly &lt;code&gt;write -&amp;gt; run -&amp;gt; modify&lt;/code&gt; in a loop, you can also insert prose describing what is going on in the form of &lt;code&gt;markdown&lt;/code&gt;. This is a nice lightweight syntax that generates html. So it becomes relatively easy to document the &lt;em&gt;why&lt;/em&gt; of something.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-notebook&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;R Notebook&lt;/h2&gt;
&lt;p&gt;Unfortunately, the &lt;code&gt;R notebook&lt;/code&gt; that Ramnath has put up is not quite the same beast. It is an &lt;a href=&#34;http://ajaxorg.github.io/ace/#nav=about&#34;&gt;Ace editor&lt;/a&gt; window coupled to an R process that knits the markdown and displays the resultant html. This is really cool, and I think will be useful in many other applications, but &lt;strong&gt;not&lt;/strong&gt; for teaching in an interactive environment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rstudio-as-a-teaching-environment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;RStudio as a Teaching Environment&lt;/h2&gt;
&lt;p&gt;Lets think. We want something that lets us repeatedly &lt;code&gt;write -&amp;gt; run -&amp;gt; modify&lt;/code&gt; &lt;strong&gt;on small code blocks&lt;/strong&gt; in &lt;code&gt;R&lt;/code&gt;, but would be great if it was some kind of document that could be shared, and re-run.&lt;/p&gt;
&lt;p&gt;I would argue that the editor environment in &lt;a href=&#34;http://www.rstudio.com/&#34;&gt;RStudio&lt;/a&gt; when writing &lt;a href=&#34;http://www.rstudio.com/ide/docs/authoring/using_markdown&#34;&gt;R markdown (Rmd)&lt;/a&gt; files is the solution. &lt;code&gt;R&lt;/code&gt; code blocks behave much the same as in iPython notebook, in that they are colored differently, set apart, have syntax highlighting, and can be easily repeatedly run using the &lt;code&gt;code chunk&lt;/code&gt; menu. Outside of code blocks is assumed to be markdown, making it easy to insert documentation and explanation. The code from the code blocks is sent to an attached &lt;code&gt;R&lt;/code&gt; session, where objects can be further investigated if required, and results are displayed.&lt;/p&gt;
&lt;p&gt;This infrastructure supplies an interactive back and forth between editor and execution environment, with the ability to easily group together units of code.&lt;/p&gt;
&lt;p&gt;In addition, RStudio has git integration baked in, so it becomes easy to get started with some basic version control.&lt;/p&gt;
&lt;p&gt;Finally, RStudio is cross-platform, has tab completion among other standard IDE goodies, and its free.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feedback&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feedback&lt;/h2&gt;
&lt;p&gt;I’ve gotten some feedback on twitter about this, and I want to update this post to address it.&lt;/p&gt;
&lt;div id=&#34;hard-to-install&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hard to Install&lt;/h3&gt;
&lt;p&gt;One comment was that installing R, RStudio and necessary packages might be hard. True, it might be. However, I have done multiple installs of R, RStudio, Python, and iPython Notebook in both Linux and Windows, and I would argue that the level of difficulty is at least the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;moving-from-presentation-to-coding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Moving from Presentation to Coding&lt;/h3&gt;
&lt;p&gt;I think this is always difficult, especially if you have a powerpoint, and your code is in another application. However, the latest dev version of RStudio (&lt;a href=&#34;http://www.rstudio.com/ide/download/preview&#34;&gt;download&lt;/a&gt;) now includes the ability to view markdown based presentations in an &lt;a href=&#34;http://www.rstudio.com/ide/docs/presentations/overview&#34;&gt;attached window&lt;/a&gt;. This is probably one of the potentially nicest things for doing presentations that actually involve editing actual code.&lt;/p&gt;
&lt;p&gt;Edit: added download links for Rstudio preview&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Storing Package Data in Custom Environments</title>
      <link>/post/storing-package-data-in-custom-environments/</link>
      <pubDate>Thu, 30 May 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/storing-package-data-in-custom-environments/</guid>
      <description>&lt;p&gt;If you do &lt;code&gt;R&lt;/code&gt; package development, sometimes you want to be able to store variables specific to your package, without cluttering up the users workspace. One way to do this is by modifying the global &lt;code&gt;options&lt;/code&gt;. This is done by packages &lt;code&gt;grDevices&lt;/code&gt; and &lt;code&gt;parallel&lt;/code&gt;. Sometimes this doesn’t seem to work quite right (see this &lt;a href=&#34;https://github.com/cboettig/knitcitations/issues/14&#34;&gt;issue&lt;/a&gt; for example.&lt;/p&gt;
&lt;p&gt;Another way to do this is to create an environment within your package, that only package functions will be able to see, and therefore read from and modify. You get a space to put package specific stuff, the user can’t see it or modify it directly, and you just need to write functions that do the appropriate things to that environment (adding variables, reading them, etc). This sounds great in practice, but I wasn’t clear on how to do this, even after reading the &lt;a href=&#34;http://stat.ethz.ch/R-manual/R-devel/library/base/html/environment.html&#34;&gt;help page on environments&lt;/a&gt;, the &lt;a href=&#34;http://cran.r-project.org/doc/manuals/r-release/R-intro.html&#34;&gt;R documentation&lt;/a&gt;, or even &lt;a href=&#34;https://github.com/hadley/devtools/wiki/Environments&#34;&gt;Hadley’s excellent writeup&lt;/a&gt;. From all these sources, I could glean that one can create environments, name them, modify them, etc, but wasn’t sure how to work with this within a package.&lt;/p&gt;
&lt;p&gt;I checked out the &lt;a href=&#34;https://github.com/cboettig/knitcitations&#34;&gt;&lt;code&gt;knitcitations&lt;/code&gt;&lt;/a&gt; package to see how it was done. When I looked, I realized that it was pretty obvious in retrospect. In &lt;code&gt;zzz.R&lt;/code&gt;, initialize the environment, assigning it to a variable. When you need to work with the variables inside, this variable will be accessible to your package, and you simply use the &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;assign&lt;/code&gt; functions like you would if you were doing anything on the command line.&lt;/p&gt;
&lt;p&gt;To make sure I had it figured out, I created a very &lt;a href=&#34;https://github.com/rmflight/testEnvironment&#34;&gt;tiny package&lt;/a&gt; to create a custom environment and functions for modifying it. Please feel free to examine, download, install (using &lt;a href=&#34;https://github.com/hadley/devtools&#34;&gt;&lt;code&gt;devtools&lt;/code&gt;&lt;/a&gt;]) and see for yourself.&lt;/p&gt;
&lt;p&gt;I have at least two projects where I know I will use this, and I’m sure others might find it useful as well.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
