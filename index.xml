<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deciphering Life: One Bit at a Time on Deciphering Life: One Bit at a Time</title>
    <link>/</link>
    <description>Recent content in Deciphering Life: One Bit at a Time on Deciphering Life: One Bit at a Time</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Robert M Flight</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Custom Deployment Script</title>
      <link>/post/custom-deployment-script/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/custom-deployment-script/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Use a short bash script to do deployment from your own computer directly to your &lt;code&gt;*.github.io&lt;/code&gt; domain.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;So Yihui recommends using Netlify, or even Travis-CI in the Blogdown book. I wasn’t willing to setup a custom domain yet, and some of my posts involve a lot of personally created packages, etc, that I don’t want to debug installation on Travis. So, I wanted a simple script I could call on my laptop that would copy the &lt;code&gt;/public&lt;/code&gt; directory to the repo for my &lt;code&gt;github.io&lt;/code&gt; site, and then push the changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Script&lt;/h2&gt;
&lt;p&gt;Here is the simple script I ended up using:&lt;/p&gt;
&lt;pre class=&#34;sh&#34;&gt;&lt;code&gt;#!/bin/bash
org_dir=`pwd`
cd path/to/github.io/repo/
#rm -rf *
cp -Rfu path/to/blogdown/public/* .

git add *
commit_time=`date`
git commit -m &amp;quot;update at $commit_time&amp;quot;
git push origin master

cd $org_dir&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It changes directories, because to push from a &lt;code&gt;git&lt;/code&gt; repo I’m pretty sure you need to be in the directory, so it also makes sure to go back there at the end. It then copies the contents of &lt;code&gt;/public&lt;/code&gt; to the repo, &lt;code&gt;add&lt;/code&gt;s all the files, and then uses the current time-stamp as the commit message, and finally pushes all the updates.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Differences in Posted Date vs sessionInfo()</title>
      <link>/post/differences-in-posted-date-vs-sessioninfo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/differences-in-posted-date-vs-sessioninfo/</guid>
      <description>&lt;p&gt;If you are a newcomer to my weblog, you may notice that some posts that are &lt;code&gt;R&lt;/code&gt; tutorials generally include the output of &lt;code&gt;Sys.time()&lt;/code&gt; at the end. If you look closeley at that time and the &lt;strong&gt;Posted on&lt;/strong&gt; date, you may notice that some posts show disagreement between them. This is because I decided to move &lt;em&gt;all&lt;/em&gt; of my old blog posts from &lt;em&gt;blogspot&lt;/em&gt; to here, and keep the original posted dates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linking to Manually Inserted Images in Blogdown / Hugo</title>
      <link>/post/linking-to-manually-inserted-images-in-hugo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linking-to-manually-inserted-images-in-hugo/</guid>
      <description>&lt;div id=&#34;manual-linking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manual Linking?&lt;/h2&gt;
&lt;p&gt;Using &lt;code&gt;blogdown&lt;/code&gt; for generating websites and blog-posts from &lt;code&gt;Rmarkdown&lt;/code&gt; files with lots of inserted code and figures seems pretty awesome, but sometimes you want to include a figure manually, either because you want to generate something manually and convert it (say for going from SVG of lots of points to hi-res PNG), or because it is a figure from something else (&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;like this figure from wikipedia&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-to&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where to??&lt;/h2&gt;
&lt;p&gt;To do this, you want the text of your &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag to your image to be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src = &amp;quot;/img/image_file.png&amp;quot;&amp;gt;&amp;lt;/img&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then put the image itself in the directory &lt;code&gt;/static/img/image_file.png&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/Standard_deviation_diagram.svg&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By M. W. Toews, &lt;a href=&#34;http://creativecommons.org/licenses/by/2.5&#34;&gt;CC BY 2.5&lt;/a&gt;, via Wikimedia Commons, &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This information is also mentioned in &lt;a href=&#34;https://bookdown.org/yihui/blogdown/static-files.html&#34;&gt;section 2.7 of the Blogdown book&lt;/a&gt;. Obviously I need to do more reading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I was Part of the Problem</title>
      <link>/post/i-was-part-of-the-problem/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/i-was-part-of-the-problem/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;With the recent charges of sexual harassment against some high-profile individuals, and so many women coming forward with #metoo (and the understanding that this is really something almost &lt;em&gt;all&lt;/em&gt; women have faced), I realized that my younger self was #partoftheproblem. I think many other men are part of the problem, &lt;strong&gt;even though they might not think so&lt;/strong&gt;. I didn’t think I was part of the problem either. I hope that other men might read this and critically evaluate if they are #partoftheproblem. I also hope and pray that my own sons will do better at this if I teach them right.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-could-i-be&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Could I Be?&lt;/h2&gt;
&lt;p&gt;Let me be up front. I have never &lt;strong&gt;sexually assaulted&lt;/strong&gt; anyone, let alone considered such a thing. But that’s not really the problem, because the way I acted towards women, I think they may have been scared that I might, as I have put tons of &lt;strong&gt;unwanted attention&lt;/strong&gt; on several women over the years, starting with when I was 12 years old, in the sixth grade.&lt;/p&gt;
&lt;p&gt;I also want to be clear, I was a horrible guy friend to women (even if they didn’t think so). If I knew a girl had a boyfriend, well then, I would &lt;strong&gt;not&lt;/strong&gt; even consider trying to hit on or express interest in that girl, and I was “friends” with plenty of women over the course of my school years who had boyfriends. But in &lt;strong&gt;most&lt;/strong&gt; cases I secretly hoped they might dump their boyfriends and go out with me instead. Also, if I knew they didn’t have a boyfriend, and I found them remotely attractive, then I would do all I could to try to become friends with them in the &lt;strong&gt;hope to eventually become their boyfriend&lt;/strong&gt;. So, my sole reason for being friends with women, really, was to eventually become romantically involved. That was my primary motivation. Looking back on it now, it makes me sick.&lt;/p&gt;
&lt;p&gt;I’ve never had a woman tell me she was assaulted by anyone either, but given my past behavior, even if someone I knew had, I don’t think my actions made me someone that a woman would trust to tell.&lt;/p&gt;
&lt;p&gt;Let me give you some examples of my behavior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grade-school&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grade School&lt;/h2&gt;
&lt;p&gt;In 6th grade, I decided that I wanted a girlfriend, and I picked out one girl in my class who I wanted to be my girlfriend. I am very sure I never asked her out, to be my girlfriend, but I made sure to spend tons of time with her, and if I recall correctly, she eventually got the gist of my interest, and told me &lt;strong&gt;very clearly she wasn’t interested&lt;/strong&gt;. But &lt;strong&gt;her telling me no did not stop my unwanted advances&lt;/strong&gt; or attention. I am sure that I made her very uncomfortable the rest of that grade.&lt;/p&gt;
&lt;p&gt;In middle school (7-9 at the time), I pretty much continued this process unabated. I would latch onto a woman that I found attractive, and make her the target of my affections, and pour out my unwanted attention upon her, &lt;strong&gt;not taking no for an answer&lt;/strong&gt;. I only stopped after long periods of continued rejection, or when that person acquired a significant other. Although not an excuse for my actions, my tactics and hopes were largely fueled by rampaging hormones, way too many romantic comedies where the nice guy always got the girl by virtue of sheer persistence (this was the 90’s), and nascent exposure to pornography.&lt;/p&gt;
&lt;p&gt;I would find out girls numbers and call them without being asked. I would know where these girls were at all times through the day, even during lunch and between classes. I would find any excuse to be near them. Every sock-hop (weekly lunch time dance on gym floor) I would ask these girls to dance with me. I would give them valentines cards, Christmas cards, etc, in &lt;strong&gt;the hopes that they would realize what a great guy I was and go out with me.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Just so we are clear, none of this got me any dates in grade school.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;undergraduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Undergraduate&lt;/h2&gt;
&lt;p&gt;Now I’ve graduated high-school, I’m heading off to a local university, with lots of girls. I made lots of friends with girls who had boyfriends, in fact I think my circle of friends had way more girls in it than guys. But, I was always finding one girl who I wanted to date, and would make sure to spend extra time around them, helping them whenever possible, etc, and dropping subtle and not so subtle hints that I wanted to be their boyfriend. And there was always the hope that someone would break-up with their current boyfriend and find me, the faithful friend, waiting to comfort them.&lt;/p&gt;
&lt;p&gt;Over the course of this time, I had three women agree to be my date. Two of those did not result in an actual date, because I started acting like a stalker after they said yes, and they wisely stayed away. In the third case, we went out twice, but me calling at random hours, and showing up at her house un-announced because I thought she was really sad freaked her out, and she stopped talking to my creepy, stalkerish, clingy self.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-graduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-Graduate&lt;/h2&gt;
&lt;p&gt;Somehow, it seems, by the time I got to my PhD, I had &lt;strong&gt;mostly&lt;/strong&gt; given up on finding a girlfriend, settling down and getting married (really, that was my goal). I say mostly. I don’t know if I hadn’t met my now spouse in the first couple of months of my PhD that I would not have continued making unwanted advances on the women in my PhD program. (By the way, I met my spouse outside of work, at a Church actually, and was introduced by a mutual friend. In the 13 years I’ve known her, there are only a handful of days we haven’t talked to each other since we went on our first date).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-real-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Real Problem&lt;/h2&gt;
&lt;p&gt;And this is the &lt;strong&gt;real&lt;/strong&gt; problem. Too many men, my past self included, think women owe them something for being their friend, for being a &lt;strong&gt;nice guy&lt;/strong&gt;. For giving them any kind of attention, or any kind of help. Too many men believe these things, and then use their power and prestige, to demand things of women. Guys, &lt;strong&gt;women don’t owe you anything.&lt;/strong&gt; They definitely don’t owe you sex or reciprocated romantic interest because of something you did for them. They are another person worthy of respect, simply because they are a person.&lt;/p&gt;
&lt;p&gt;In addition, real life is not a romantic comedy. Non-romantic friendships are a good thing, because we need other peoples perspectives in our lives. So, if a woman tells you &lt;strong&gt;no, she doesn’t want to date you&lt;/strong&gt;, accept it, and move on. Don’t make it awkward, especially if you are in the same work environment. &lt;strong&gt;Don’t assume that a woman is romantically interested just because she is friendly.&lt;/strong&gt; I know, radical thought. Maybe try being friends, colleagues, whatever with no romantic intentions, and no expectations of them either. Don’t be #partoftheproblem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solutions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solutions&lt;/h2&gt;
&lt;p&gt;Teach your children that they can be friends with people of the opposite sex without being romantically involved, especially as they hit puberty. Teach them that &lt;strong&gt;no means no&lt;/strong&gt;, not &lt;strong&gt;no means maybe in 3 weeks&lt;/strong&gt;, or &lt;strong&gt;no means maybe if I try hard enough&lt;/strong&gt;. And if you see other men engaging in putting unwanted attention on women, call them out on it, &lt;strong&gt;whatever form it may take&lt;/strong&gt;. I wish someone had said something to me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;I realize that our general culture is really #partoftheproblem, when we have highly sexualized advertising (especially of women to men), and the idea that &lt;strong&gt;boys will be boys&lt;/strong&gt;, tell jokes about sexual assault, and propagate the idea that &lt;strong&gt;women want it&lt;/strong&gt;, based on how they act or dress. Those are all wrong too, and our culture needs to change.&lt;/p&gt;
&lt;p&gt;I also realize that some of what I describe about myself is rather mild in comparison to much of what gets reported, but that’s not the point. It is still unwanted attention, and I didn’t know how to take no for an answer. Those women &lt;strong&gt;didn’t want my attention&lt;/strong&gt;, and I couldn’t accept that. If I had a different temperament, I don’t know what I would have done. Enough people realized it that some friends in Undergrad stopped being around me, but no one ever told me that what I was doing was wrong, and my parents weren’t involved enough in my so-called love life to know what was going on. If they had, I think they would have told me to knock it off and stop being an idiot.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Criticizing a Publication, and Lying About It</title>
      <link>/post/criticizing-a-publication-and-lying-about-it/</link>
      <pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/criticizing-a-publication-and-lying-about-it/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Other researchers &lt;a href=&#34;https://dx.doi/org/10.1002/prot.25024&#34;&gt;directly criticized&lt;/a&gt; a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;recent publication of ours&lt;/a&gt; in a “research article”. Although they raised valid points, they &lt;strong&gt;outright lied&lt;/strong&gt; about the availability of our results. In addition, they did not provide access to their own results. We have published &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;new work&lt;/a&gt; supporting our original results, and a direct rebuttal of their critique in a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;perspective article&lt;/a&gt;. The peer reviewers of their “research article” must have been asleep at the wheel to allow the major point, lack of access to our results, to stand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;original-publication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Original Publication&lt;/h2&gt;
&lt;p&gt;Back in the summer of 2015, I was second author on a publication (&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.24834/full&#34;&gt;Yao et al., 2015&lt;/a&gt;, hereafter YS2015) describing an automated method to characterize zinc ion coordination geometries (CGs). Applying our automated method to all zinc sites in the worldwide Protein Data Bank (wwPDB), we found &lt;em&gt;abberrant&lt;/em&gt; zinc CGs that don’t fit the canonical CGs. We were pretty sure that these aberrant CGs are real, and they have always existed, but had not been previously characterized because methods assumed that only the &lt;em&gt;canonical&lt;/em&gt; geometries should be observed in biological systems, and were excluding the &lt;em&gt;abberrant&lt;/em&gt; ones because they didn’t have good methods to detect and characterize them.&lt;/p&gt;
&lt;p&gt;Also of note, the proteins with aberrant zinc geometries showed enrichment for different types of enzyme classifications than those with canonical zinc geometries.&lt;/p&gt;
&lt;p&gt;For this publication, we made &lt;strong&gt;all&lt;/strong&gt; of our code and results available in a tarball that could be downloaded from our &lt;a href=&#34;http://bioinformatics.cesb.uky.edu/bin/view/Main/SoftwareDevelopment#Metal_ion_coordination_analysis_software&#34;&gt;website&lt;/a&gt;. This data went up while the paper was in review, on Dec 7, 2015 (with a correction on Dec 15). Recently, we’ve also put a copy of the tarball on &lt;a href=&#34;https://figshare.com/articles/Zn_metalloprotein_paper/4229333&#34;&gt;FigShare&lt;/a&gt;. Every draft of the publication, from initial submission through to accepted publication, included the link to the tarball on the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Critique&lt;/h2&gt;
&lt;p&gt;Less than a year later, &lt;a href=&#34;http://sci-hub.cc/doi/10.1002/prot.25024&#34;&gt;Raczynska, Wlodawer, and Jaskolski&lt;/a&gt; (RJW2016) published a &lt;em&gt;critique&lt;/em&gt; of YS2015 as a “research article”. In their publication, they questioned the existence of the &lt;em&gt;abberrant&lt;/em&gt; sites completely, based on the examination and remodeling of four aberrant structures highlighted in YS2015. To be fair, they did have some valid criticisms of the methods, and Sen Yao did a lot of work in our latest paper to address them.&lt;/p&gt;
&lt;p&gt;As part of the critique, however, they claimed that they could only evaluate the four structures listed in two figures &lt;strong&gt;because we didn’t provide all of our results&lt;/strong&gt;. However, we had previously made our full results available as a tarball from our website. As you can see in the below figure, &lt;strong&gt;all&lt;/strong&gt; of the results were really available in that tarball.&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/ys2017_figure1.png&#34;, width = &#34;600&#34;&gt;&lt;/p&gt;
&lt;p&gt;In addition, although RWJ2016 went to all the trouble to actually remodel those four structures by going back to the original X-ray density, they &lt;strong&gt;didn’t make any of their models available&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, no one from RWJ2016 ever contacted our research group to see if the results might be available.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;response&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Response&lt;/h2&gt;
&lt;div id=&#34;follow-up-paper-on-5-metals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Follow-Up Paper on 5 Metals&lt;/h3&gt;
&lt;p&gt;By the time the critiques appeared in RJW2016, Sen was already hard at work showing that the previously developed methods could be modified and then applied to other metal ion CGs, and that they also contained aberrant CGs (see &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;YS2017-1&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique-direct-response&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Critique Direct Response&lt;/h3&gt;
&lt;p&gt;In addition to YS2017-1, we felt that the critique deserved separate response (&lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;YS2017-2&lt;/a&gt;). To that end, we began drafting a response, wherein we pointed out some of the problems with RJW2016, the first being that we did indeed provide the &lt;strong&gt;full&lt;/strong&gt; set of results from YS2015, and therefore it was possible to evaluate our full work. We also addressed each of their other criticisms of YS2015, in many cases going beyond the original criticism, and explaining how it was being addressed in YS2017-1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-results-and-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open Results and Code&lt;/h3&gt;
&lt;p&gt;A major part of the conclusions in YS2017-2 was also devoted to the idea that code and results in science need to be shared, highlighting the fact that RJW2016 &lt;strong&gt;did not share their models&lt;/strong&gt; they used to try and discredit our work, lied about the fact that we did not share our own results, and pointing out some other projects in this research area that have shared well and others that have shared badly, and that the previous attitude of competition among research groups does not move science forward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;peer-review&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer Review&lt;/h2&gt;
&lt;p&gt;Let’s just say that the &lt;em&gt;peer-review&lt;/em&gt; of both of the papers was &lt;strong&gt;interesting&lt;/strong&gt;. Both manuscripts had the same set of reviewers. YS2017-1, the five metal paper, had some rather rigorous peer review, and was definitely improved by the reviewer’s comments. YS2017-2, our perspective, in contrast, was attacked by one peer reviewer right from submission, and was questioned almost continually as to whether it should even be published. I am thankful that one reviewer saw the need for it to be published, and that the Editor ultimately decided that it should be published, and that we were able to rebut each of the reviewer’s criticisms.&lt;/p&gt;
&lt;p&gt;Finally, I really don’t know what happened in the peer review of RWJ2016. The first major claim was that our data wasn’t available, it should have taken a reviewer 10 minutes to verify and debunk that claim. I would have expected a much different critique from the authors had they actually examined our full data set. But, because of traditional closed peer review, that record is closed to us.&lt;/p&gt;
&lt;p&gt;Overall though, I’m very happy both of our publications are now out, and we can move on to new stages of our analyses. Looking forward to continuing to work with my co-authors to move the work forward.&lt;/p&gt;
&lt;div id=&#34;papers-discussed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Papers Discussed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Original Zinc CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;Yao et al 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Critique of Zinc CGs: Raczynska, Wlodawer &amp;amp; Jaskolski 2016, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25024&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://sci-hub.cc/10.1002/prot.25024&#34;&gt;sci-hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5 Metal CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;Yao et al 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Response to critique: Yao et al 2017, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;copy on figshare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 -0500</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Authentication of Key Resources for Data Analysis</title>
      <link>/post/authentication-of-key-resources-for-data-analysis/</link>
      <pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/authentication-of-key-resources-for-data-analysis/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;NIH recently introduced a reproducibility initiative, extending to including the “Authentication of Key Resources” page in grant applications from Jan 25, 2016. Seems to be intended for grants involving biological reagents, but we included it in our recent R03 grant developing new data analysis methods. We believe that this type of thing should become common for all grants, not just those that use biological/chemical resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nih-and-reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NIH and Reproducibility&lt;/h2&gt;
&lt;p&gt;There has been a lot of things published recently about the &lt;em&gt;reproducibility crisis&lt;/em&gt; in science (see refs). The federal funding agencies are starting to respond to this, and beginning with grants submitted after January 25, 2016, grants are &lt;a href=&#34;http://grants.nih.gov/reproducibility/index.htm&#34;&gt;supposed to address the reproducibility&lt;/a&gt; of the work proposed, including the presence of various confounding factors (i.e. sex of animals, the source of cell lines, etc). In addition to this, there is a new document that can be added to grants, the &lt;a href=&#34;http://nexus.od.nih.gov/all/2016/01/29/authentication-of-key-biological-andor-chemical-resources-in-nih-grant-applications/&#34;&gt;&lt;strong&gt;Authentication Plan&lt;/strong&gt;&lt;/a&gt;, which as far as I can tell is intended specifically for:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;key biological and/or chemical resources used in the proposed studies&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, this makes sense. Some sources of irreproducibility include, but are not limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unvalidated antibodies&lt;/li&gt;
&lt;li&gt;cell lines that are not what was thought&lt;/li&gt;
&lt;li&gt;impure chemicals&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think this is a &lt;strong&gt;good thing&lt;/strong&gt;. What does it have to do with data analysis?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-code-authentication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data / Code Authentication&lt;/h2&gt;
&lt;p&gt;When we were submitting a recent R03 proposal for developing novel data analysis methods and statistical tools, the grant management office asked us about the &lt;strong&gt;Authentication of Key Resources&lt;/strong&gt; attachment, which we completely missed. Upon review of the guidelines, we initially determined that this document did not apply. However, we decided to go ahead and take some initiative.&lt;/p&gt;
&lt;div id=&#34;data-authentication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Authentication?&lt;/h3&gt;
&lt;p&gt;When dealing with multiple samples from high-throughput samples, there are frequently a few easy ways to examine the data quality, and although it can be hard to verify that the data &lt;strong&gt;is what the supplier says it is&lt;/strong&gt;, which would be true &lt;strong&gt;authentication&lt;/strong&gt;, there are some ways to verify that the various samples in the dataset are at least self-consistent within each sample class (normal and disease, condition 1 and condition 2).&lt;/p&gt;
&lt;p&gt;My go-to for data self-consistency are principal components analysis (PCA) and correlation heat-maps. Correlation heat-maps involve calculating all of the pairwise sample to sample correlations using all of the non-zero sample features (those that are non-zero in the two pairs being compared). These heatmaps, combined with the sample class information, and clustering within each class, are a nice visual way to eyeball samples that have potential problems. A simple example for RNA-seq transcriptomics was shown in &lt;a href=&#34;https://dx.doi.org/10.1093/bioinformatics/btv425&#34;&gt;Gierliński et al., Statistical models for RNA-seq data derived from a two-condition 48-replicate experiment Bioinformatics (2015) 31 (22): 3625-3630, Figure 1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/yeast_48rep_cor_heatmap.jpg&#34; alt=&#34;Gierlinkski et al heatmap, Figure 1&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Gierlinkski et al heatmap, Figure 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The other measures they used in this paper are also very nice, in plotting the median correlation of a sample against all other samples, and the fraction of outlier features in a given sample (see figure 2 of Gierlinkski et al). The final measure they propose is not generally applicable to all -omics data however.&lt;/p&gt;
&lt;p&gt;PCA on the data, followed by visualizing the scores on the first few principal components, and colored by sample class (or experimental condition) is similar in spirit to the correlation heat-map. In fact, it is very similar, because PCA is actually decomposing on the covariance of the samples, which is very related to the correlations (an early algorithm actually used the correlation matrix).&lt;/p&gt;
&lt;p&gt;Both of these methods can highlight possible problems with individual samples, and make sure that the set of data going into the analysis is at least self-consistent, which is important when doing classification or differential abundance analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code-authentication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code Authentication&lt;/h3&gt;
&lt;p&gt;The other thing we highlighted in the document was &lt;strong&gt;code&lt;/strong&gt; authentication. In this case, we highlighted the use of unit-testing in the R packages that we are planning to develop. Even though this is software coming out of a research lab, we need to have confidence that the functions we write return the correct values given various inputs. In addition, code testing coverage helps evaluate that we are testing &lt;em&gt;all&lt;/em&gt; of the functionality by checking that all of the lines in our code are run by the tests. Finally, we are also planning to write tests for core functions provided by others (i.e. functions in other R packages), in that they work as we expect, by returning correct values given specific inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Going forward, I think it would be a good thing if people writing research grants for data analysis methods would discuss how they are going to look at the data to assess it’s quality, and how they are going to do unit testing, and will have to start saying that they are going to do unit testing of their analysis method.&lt;/p&gt;
&lt;p&gt;I’d be interested in others’ thoughts on this as well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Random Forest vs PLS on Random Data</title>
      <link>/post/random-forest-vs-pls-on-random-data/</link>
      <pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/random-forest-vs-pls-on-random-data/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Partial least squares (PLS) discriminant-analysis (DA) can ridiculously over fit even on completely random data. The quality of the PLS-DA model can be assessed using cross-validation, but cross-validation is not typically performed in many metabolomics publications. Random forest, in contrast, because of the &lt;em&gt;forest&lt;/em&gt; of decision tree learners, and the out-of-bag (OOB) samples used for testing each tree, automatically provides an indication of the quality of the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;I’ve recently been working on some machine learning work using &lt;strong&gt;random forests&lt;/strong&gt; (RF) &lt;a href=&#34;https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf&#34;&gt;Breimann, 2001&lt;/a&gt; on metabolomics data. This has been relatively successful, with decent sensitivity and specificity, and hopefully I’ll be able to post more soon. However, PLS (Wold, 1975) is a standard technique used in metabolomics due to the prevalence of analytical chemists in metabolomics and a long familiarity with the method. Importantly, my collaborators frequently use PLS-DA to generate plots to show that the various classes of samples are separable.&lt;/p&gt;
&lt;p&gt;However, it has long been known that PLS (and all of it’s variants, PLS-DA, OPLS, OPLS-DA, etc) can easily generate models that over fit the data, and that over fitting of the model needs to be assessed if the model is going to be used in subsequent analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random Data&lt;/h2&gt;
&lt;p&gt;To illustrate the behavior of both RF and PLS-DA, we will generate some random data where each of the samples are randomly assigned to one of two classes.&lt;/p&gt;
&lt;div id=&#34;feature-intensities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Feature Intensities&lt;/h3&gt;
&lt;p&gt;We will generate a data set with 1000 features, where each feature’s mean value is from a uniform distribution with a range of 1-10000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;cowplot&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     ggsave&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fakeDataWithError)
set.seed(1234)
n_point &amp;lt;- 1000
max_value &amp;lt;- 10000
init_values &amp;lt;- runif(n_point, 0, max_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init_data &amp;lt;- data.frame(data = init_values)
ggplot(init_data, aes(x = data)) + geom_histogram() + ggtitle(&amp;quot;Initial Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/plot_initial-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each of these features, their distribution across samples will be based on a random normal distribution where the mean is the initial feature value and a standard deviation of 200. The number of samples is 100.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_sample &amp;lt;- 100
error_values &amp;lt;- add_uniform_noise(n_sample, init_values, 200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just for information, the &lt;code&gt;add_uniform_noise&lt;/code&gt; function is this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;add_uniform_noise&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (n_rep, value, sd, use_zero = FALSE) 
## {
##     n_value &amp;lt;- length(value)
##     n_sd &amp;lt;- n_rep * n_value
##     out_sd &amp;lt;- rnorm(n_sd, 0, sd)
##     out_sd &amp;lt;- matrix(out_sd, nrow = n_value, ncol = n_rep)
##     if (!use_zero) {
##         tmp_value &amp;lt;- matrix(value, nrow = n_value, ncol = n_rep, 
##             byrow = FALSE)
##         out_value &amp;lt;- tmp_value + out_sd
##     }
##     else {
##         out_value &amp;lt;- out_sd
##     }
##     return(out_value)
## }
## &amp;lt;environment: namespace:fakeDataWithError&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I created it as part of a package that is able to add different kinds of noise to data.&lt;/p&gt;
&lt;p&gt;The distribution of values for a single feature looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error_data &amp;lt;- data.frame(feature_1 = error_values[1,])
ggplot(error_data, aes(x = feature_1)) + geom_histogram() + ggtitle(&amp;quot;Error Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/plot_error-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we will assign the first 50 samples to &lt;strong&gt;class_1&lt;/strong&gt; and the second 50 samples to &lt;strong&gt;class_2&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_class &amp;lt;- rep(c(&amp;quot;class_1&amp;quot;, &amp;quot;class_2&amp;quot;), each = 50)
sample_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##   [8] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [15] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [22] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [29] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [36] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [43] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [50] &amp;quot;class_1&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [57] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [64] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [71] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [78] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [85] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [92] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [99] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PCA&lt;/h2&gt;
&lt;p&gt;Just to show that the data is pretty random, lets use principal components analysis (PCA) to do a decomposition, and plot the first two components:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp_pca &amp;lt;- prcomp(t(error_values), center = TRUE, scale. = TRUE)
pca_data &amp;lt;- as.data.frame(tmp_pca$x[, 1:2])
pca_data$class &amp;lt;- as.factor(sample_class)
ggplot(pca_data, aes(x = PC1, y = PC2, color = class)) + geom_point(size = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/pca_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random Forest&lt;/h2&gt;
&lt;p&gt;Let’s use RF first, and see how things look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-12&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;randomForest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_model &amp;lt;- randomForest(t(error_values), y = as.factor(sample_class))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confusion matrix comparing actual &lt;em&gt;vs&lt;/em&gt; predicted classes based on the out of bag (OOB) samples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(rf_model$confusion)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;class_1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;class_2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;class.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;class_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.44&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;class_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;27&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.46&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And an overall error of 0.4866246.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pls-da&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PLS-DA&lt;/h2&gt;
&lt;p&gt;So PLS-DA is really just PLS with &lt;strong&gt;y&lt;/strong&gt; variable that is binary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pls_model &amp;lt;- plsda(t(error_values), as.factor(sample_class), ncomp = 2)
pls_scores &amp;lt;- data.frame(comp1 = pls_model$scores[,1], comp2 = pls_model$scores[,2], class = sample_class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the PLS scores:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(pls_scores, aes(x = comp1, y = comp2, color = class)) + geom_point(size = 4) + ggtitle(&amp;quot;PLS-DA of Random Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/plot_plsda-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And voila! Perfectly separated data! If I didn’t tell you that it was random, would you suspect it?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validated-pls-da&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-validated PLS-DA&lt;/h2&gt;
&lt;p&gt;Of course, one way to truly assess the worth of the model would be to use cross-validation, where a fraction of data is held back, and the model trained on the rest. Predictions are then made on the held back fraction, and because we know the truth, we will then calculate the &lt;strong&gt;area under the reciever operator curve&lt;/strong&gt; (AUROC) or area under the curve (AUC) created by plotting true positives &lt;em&gt;vs&lt;/em&gt; false positives.&lt;/p&gt;
&lt;p&gt;To do this we will need two functions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generates all of the CV folds&lt;/li&gt;
&lt;li&gt;Generates PLS-DA model, does prediction on hold out, calculates AUC&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cvTools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: robustbase&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_cv &amp;lt;- function(xdata, ydata, nrep, kfold){
  n_sample &amp;lt;- length(ydata)
  all_index &amp;lt;- seq(1, n_sample)
  cv_data &amp;lt;- cvFolds(n_sample, K = kfold, R = nrep, type = &amp;quot;random&amp;quot;)
  
  rep_values &amp;lt;- vapply(seq(1, nrep), function(in_rep){
    use_rep &amp;lt;- cv_data$subsets[, in_rep]
    cv_values &amp;lt;- vapply(seq(1, kfold), function(in_fold){
      test_index &amp;lt;- use_rep[cv_data$which == in_fold]
      train_index &amp;lt;- all_index[-test_index]
      
      plsda_cv(xdata[train_index, ], ydata[train_index], xdata[test_index, ],
               ydata[test_index])
    }, numeric(1))
  }, numeric(kfold))
}

plsda_cv &amp;lt;- function(xtrain, ytrain, xtest, ytest){
  pls_model &amp;lt;- plsda(xtrain, ytrain, ncomp = 2)
  pls_pred &amp;lt;- predict(pls_model, xtest, type = &amp;quot;prob&amp;quot;)
  
  use_pred &amp;lt;- pls_pred[, 2, 1]
  
  pred_perf &amp;lt;- ROCR::prediction(use_pred, ytest)
  pred_auc &amp;lt;- ROCR::performance(pred_perf, &amp;quot;auc&amp;quot;)@y.values[[1]]
  return(pred_auc)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now lets do a bunch of replicates (100).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_vals &amp;lt;- gen_cv(t(error_values), factor(sample_class), nrep = 100, kfold = 5)

mean(cv_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4182644&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(cv_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1086778&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_frame &amp;lt;- data.frame(auc = as.vector(cv_vals))
ggplot(cv_frame, aes(x = auc)) + geom_histogram(binwidth = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/rep_plsda-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we get an average AUC of 0.4182644, which is pretty awful. This implies that even though there was good separation on the scores, maybe the model is not actually that good, and we should be cautious of any predictions being made.&lt;/p&gt;
&lt;p&gt;Of course, the PCA at the beginning of the analysis shows that there is no &lt;em&gt;real&lt;/em&gt; separation in the data in the first place.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::session_info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Session info -------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  setting  value                       
##  version  R version 3.4.2 (2017-09-28)
##  system   x86_64, linux-gnu           
##  ui       X11                         
##  language en_US                       
##  collate  en_US.UTF-8                 
##  tz       posixrules                  
##  date     2017-12-28&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Packages -----------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  package           * version    date      
##  assertthat          0.2.0      2017-04-11
##  backports           1.1.1      2017-09-25
##  base              * 3.4.2      2017-10-10
##  bindr               0.1        2016-11-13
##  bindrcpp            0.2        2017-06-17
##  bitops              1.0-6      2013-08-17
##  blogdown            0.4        2017-12-12
##  bookdown            0.5        2017-08-20
##  caret             * 6.0-77     2017-09-07
##  caTools             1.17.1     2014-09-10
##  class               7.3-14     2015-08-30
##  codetools           0.2-15     2016-10-05
##  colorspace          1.3-2      2016-12-14
##  compiler            3.4.2      2017-10-10
##  cowplot           * 0.9.1      2017-11-16
##  CVST                0.2-1      2013-12-10
##  cvTools           * 0.3.2      2012-05-14
##  datasets          * 3.4.2      2017-10-10
##  ddalpha             1.3.1      2017-09-27
##  DEoptimR            1.0-8      2016-11-19
##  devtools            1.13.3     2017-08-02
##  digest              0.6.12     2017-01-27
##  dimRed              0.1.0      2017-05-04
##  dplyr               0.7.4      2017-09-28
##  DRR                 0.0.2      2016-09-15
##  evaluate            0.10.1     2017-06-24
##  fakeDataWithError * 0.0.1      2017-12-29
##  foreach             1.4.3      2015-10-13
##  gdata               2.18.0     2017-06-06
##  ggplot2           * 2.2.1.9000 2017-12-13
##  glue                1.1.1      2017-06-21
##  gower               0.1.2      2017-02-23
##  gplots            * 3.0.1      2016-03-30
##  graphics          * 3.4.2      2017-10-10
##  grDevices         * 3.4.2      2017-10-10
##  grid                3.4.2      2017-10-10
##  gtable              0.2.0      2016-02-26
##  gtools              3.5.0      2015-05-29
##  highr               0.6        2016-05-09
##  htmltools           0.3.6      2017-04-28
##  ipred               0.9-6      2017-03-01
##  iterators           1.0.8      2015-10-13
##  kernlab             0.9-25     2016-10-03
##  KernSmooth          2.23-15    2015-06-29
##  knitr               1.17       2017-08-10
##  labeling            0.3        2014-08-23
##  lattice           * 0.20-35    2017-03-25
##  lava                1.5.1      2017-09-27
##  lazyeval            0.2.1      2017-10-29
##  lubridate           1.7.1      2017-11-03
##  magrittr            1.5        2014-11-22
##  MASS                7.3-47     2017-04-21
##  Matrix              1.2-11     2017-08-16
##  memoise             1.1.0      2017-04-21
##  methods           * 3.4.2      2017-10-10
##  ModelMetrics        1.1.0      2016-08-26
##  munsell             0.4.3      2016-02-13
##  nlme                3.1-131    2017-02-06
##  nnet                7.3-12     2016-02-02
##  pkgconfig           2.0.1      2017-12-07
##  pls                 2.6-0      2016-12-18
##  plyr                1.8.4      2016-06-08
##  prodlim             1.6.1      2017-03-06
##  purrr               0.2.4      2017-10-18
##  R6                  2.2.2      2017-06-17
##  randomForest      * 4.6-12     2015-10-07
##  Rcpp                0.12.14    2017-11-23
##  RcppRoll            0.2.2      2015-04-05
##  recipes             0.1.0      2017-07-27
##  reshape2            1.4.3      2017-12-11
##  rlang               0.1.2      2017-08-09
##  rmarkdown           1.8        2017-11-17
##  robustbase        * 0.92-7     2016-12-09
##  ROCR              * 1.0-7      2015-03-26
##  rpart               4.1-11     2017-04-21
##  rprojroot           1.2        2017-01-16
##  scales              0.5.0.9000 2017-10-12
##  sfsmisc             1.1-1      2017-06-08
##  splines             3.4.2      2017-10-10
##  stats             * 3.4.2      2017-10-10
##  stats4              3.4.2      2017-10-10
##  stringi             1.1.5      2017-04-07
##  stringr             1.2.0      2017-02-18
##  survival            2.41-3     2017-04-04
##  tibble              1.3.4      2017-08-22
##  timeDate            3012.100   2015-01-23
##  tools               3.4.2      2017-10-10
##  utils             * 3.4.2      2017-10-10
##  withr               2.1.0.9000 2017-12-05
##  yaml                2.1.15     2017-12-01
##  source                                     
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  local                                      
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  local                                      
##  cran (@0.9.1)                              
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  local                                      
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  Github (rmflight/fakeDataWithError@ccd8714)
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  Github (tidyverse/ggplot2@bfff1d8)         
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  local                                      
##  local                                      
##  local                                      
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  cran (@0.2.1)                              
##  cran (@1.7.1)                              
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  local                                      
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  Github (gaborcsardi/pkgconfig@96a1413)     
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  cran (@0.2.4)                              
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  cran (@0.12.14)                            
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  cran (@1.4.3)                              
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  Github (hadley/scales@d767915)             
##  CRAN (R 3.4.2)                             
##  local                                      
##  local                                      
##  local                                      
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  CRAN (R 3.4.2)                             
##  local                                      
##  local                                      
##  Github (jimhester/withr@fe81c00)           
##  cran (@2.1.15)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Novel Zinc Coordination Geometries</title>
      <link>/post/novel-zinc-coordination-geometries/</link>
      <pubDate>Wed, 17 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/novel-zinc-coordination-geometries/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Currently available methods to discover metal geometries make too many assumptions. We were able to discover novel zinc coordination geometries using a &lt;strong&gt;less-biased&lt;/strong&gt; method that makes fewer assumptions. These novel geometries seem to also have specific functionality. This work was recently published under an #openaccess license in Proteins Journal: Yao, S., Flight, R. M., Rouchka, E. C. and Moseley, H. N. B. (2015), A less-biased analysis of metalloproteins reveals novel zinc coordination geometries. Proteins. &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.24834/full&#34;&gt;doi: 10.1002/prot.24834&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;zinc-coordination&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Zinc Coordination&lt;/h2&gt;
&lt;p&gt;As I’m sure many people know, zinc is a very important metal for biology. It is one of the most numerous metal ions, and plays a role in many different types of proteins. Zinc ions in protein structures can have anywhere from 4 to 6 ligands, in many different coordination geometries.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/prot24834-fig-0001.png&#34; alt=&#34;zinc coordination geometries&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;zinc coordination geometries&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure 1 from Yao et al., 2015&lt;/p&gt;
&lt;p&gt;How the zinc ion is coordinated is going to depend on the protein sequence of amino acids that surround it, and knowledge of the protein sequence should enable knowledge of how the zinc ions are coordinated. Therefore, being able to characterize zinc ion coordination geometries (CGs) from structural data (such as protein structures in the world-wide protein data bank) and associate them with protein sequences is very important.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;previous-attempts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Previous Attempts&lt;/h2&gt;
&lt;p&gt;Other groups had done this previously, and we now have hidden-markov models for determining zinc binding thanks to this work. However, in all the cases that we could find, the determination of CG was made by comparison to &lt;strong&gt;previously known&lt;/strong&gt; geometries that have been described from zinc-compound crystal structures.&lt;/p&gt;
&lt;p&gt;When the biologically based CGs are compared to previously known, there will be a bunch of CGs that are &lt;strong&gt;outliers&lt;/strong&gt; or will remain &lt;strong&gt;unclassified&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;our-initial-attempt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Our Initial Attempt&lt;/h2&gt;
&lt;p&gt;When Sen (the lead author, currently a third year PhD student in the University of Louisville Bioinformatics program) first tried to use bootstrapping and a expectation-maximization algorithm to automatically classify the zinc CGs, she started getting rather &lt;strong&gt;funny&lt;/strong&gt; results, as in, why is the algorithm not converging and we are getting these &lt;strong&gt;huge variances&lt;/strong&gt; in the various measures that characterize the CGs.&lt;/p&gt;
&lt;p&gt;A single visualization was key in unlocking what was going on. Figure 2 in the paper is a histogram of the minimum angle (where angle is ligand-zinc-ligand) in degrees.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/prot24834-fig-0002.png&#34; alt=&#34;minimum angle histogram&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;minimum angle histogram&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Based on Figure 1, we would expect the minimum angle to be &lt;strong&gt;90&lt;/strong&gt;, but as you can see in Figure 2, there are an awful lot of angles less than &lt;strong&gt;90&lt;/strong&gt;. We are very sure that they are real given the definition of zinc-ligand bond-length that was used to define potential ligands, and the statistical decision used to define &lt;strong&gt;how many&lt;/strong&gt; ligands a given zinc ion has.&lt;/p&gt;
&lt;p&gt;So we have a bunch of zinc-ions that make ligand-zinc-ligand bond angles at 60, and even 30 degrees! It turns out that these small angles are largely (but not all) due to bidentate ligands, where for example the zinc ion forms bonds with two oxygen’s from an aspartate or glutamate amino acid.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;separate-out-compressed-and-cluster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Separate Out Compressed and Cluster!&lt;/h2&gt;
&lt;p&gt;To make the problem tractable, we first have to separate out the compressed angle zinc sites, and then we can do clustering on the set of ligand-zinc-ligand angles to determine in a mostly un-biased fashion what CGs are present and which zinc-site belongs to which CG.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;p&gt;The use of k-means clustering on the angles also required developing a way to order the ligand-zinc-ligand angles in such a way that they are comparable across all of the different CGs. The final method used in the paper was to order them using &lt;em&gt;largest-sortedmiddle-opposite&lt;/em&gt;, which is the largest angle, the middle angles sorted in order, and lastly the opposite of the largest angle. This keeps them from being scrambled from site to site, and allows them to be comparable across zinc sites.&lt;/p&gt;
&lt;p&gt;Also, because the number of true clusters was unknown as we are trying to discover in an unbiased way all CGs, the number of clusters was varied, and for each &lt;em&gt;k&lt;/em&gt;-clusters, replicate clusterings done, and the stability of the clusters was assessed by cluster membership and locations across replicates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;novel-cgs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Novel CGs&lt;/h2&gt;
&lt;p&gt;Based on all this work, we were able to compare generated clusters from both the &lt;em&gt;normal&lt;/em&gt; and &lt;em&gt;compressed&lt;/em&gt; groups with canonical CGs to determine if a CG from clustering corresponds to a &lt;strong&gt;known&lt;/strong&gt; CG or a &lt;strong&gt;novel&lt;/strong&gt; CG. The &lt;strong&gt;normal&lt;/strong&gt; clusters mostly corresponded to &lt;strong&gt;known&lt;/strong&gt; CGs or unsurprising variants thereof. What was really interesting is that there were multiple clusters corresponding to a tetrahedral CG. To determine if the &lt;em&gt;compressed&lt;/em&gt; CGs were merely a &lt;em&gt;compressed&lt;/em&gt; variant of the canonical, the compressed angle was removed from the comparison to generate a probability of assignment. At least one of the &lt;em&gt;compressed&lt;/em&gt; groups appear to be novel, in that it has not been described before, either structurally or functionally.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;functional-characterization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Functional Characterization&lt;/h2&gt;
&lt;p&gt;We also wanted to determine if there was a functional component to the CGs, &lt;em&gt;i.e.&lt;/em&gt; different CGs have different functionality (this is my primary but not only contribution to this manuscript). To do this we annotated all of the PDB sequences using &lt;a href=&#34;http://www.ebi.ac.uk/Tools/pfa/iprscan5/&#34;&gt;&lt;em&gt;InterProScan&lt;/em&gt;&lt;/a&gt;, and kept annotations that intersected the range of amino-acids that potentially interact with the zinc ion. This bit is tricky, because many different CGs can have many different functionality, merely doing hypergeometric-enrichment of annotations in each cluster doesn’t lead to a convincing picture; as we quickly found out (it was the first thing I tried). However, if we generate a measure of functional similarity between clusters (based on a faux covariance, really, read the paper, it is rather neat) and compare this functional cluster similarity measure with a cluster distance based on the angles, there is an extremely high Spearman correlation of 0.88 for the &lt;em&gt;normal&lt;/em&gt; and 0.66 for the &lt;em&gt;compressed&lt;/em&gt; CGs, implying that CG and function are intimately related.&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/prot24834-fig-0009.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/prot24834-fig-0010.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Finally, we considered &lt;strong&gt;all&lt;/strong&gt; &lt;em&gt;normal&lt;/em&gt; and &lt;em&gt;compressed&lt;/em&gt; clusters as separate groups and performed hypergeometric-enrichment on both the &lt;em&gt;InterProScan&lt;/em&gt; and &lt;em&gt;EC&lt;/em&gt; number annotations, finding enriched annotations specific to each group of clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Making too many assumptions about biological structure can be a bad thing. Given that, however, if you are using canonical structures for assignment and you get a ton of &lt;strong&gt;outliers&lt;/strong&gt;, maybe you need to re-examine your data and methods.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Machine learning methods such as &lt;em&gt;random-forest&lt;/em&gt;, &lt;em&gt;k-means clustering&lt;/em&gt;, and statistical classification can be readily used to discover and assign metal-ion CG. If you are careful, and separate out the &lt;em&gt;compressed&lt;/em&gt; from &lt;em&gt;normal&lt;/em&gt; first.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is a rather tight relationship between a zinc-ion’s CG and the functionality of the protein it is embedded within.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducibility&lt;/h2&gt;
&lt;p&gt;Although we have admittedly dropped the ball on this, there will be a tarball of all of the scripts used to generate the results including a README explaining how to run them available soon.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mouse / Human Transcriptomics and Batch Effects</title>
      <link>/post/mouse-human-transcriptomics-and-batch-effects/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/mouse-human-transcriptomics-and-batch-effects/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;This &lt;a href=&#34;http://dx.doi.org/10.1073/pnas.1413624111&#34;&gt;2014 PNAS paper by S. Lin et al (Lin et al., PNAS, 2014)&lt;/a&gt; that compares transcription of tissues between species has a flawed experimental design, where species is almost perfectly confounded with machine / lane on which the sequencing was done. Y. Golad and O. Mizrahi-Man have &lt;a href=&#34;http://f1000research.com/articles/4-121/v1&#34;&gt;published a manuscript&lt;/a&gt; describing the confounding and the results of removing it. This was possible because the original authors supplied the information about which publically available files were used in the original analysis. The data from this experiment is probably only suitable as an example of what not to do in high-throughput biology experimental design, and that there &lt;em&gt;may&lt;/em&gt; be similarities in human and mouse transcriptional programs.&lt;/p&gt;
&lt;p&gt;We discussed these papers in the Systems Biology and Omics Integration University of Kentucky Journal Club on June 1, 2015. I lead that discussion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mouse-human-transcriptomic-differences&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mouse / Human Transcriptomic Differences&lt;/h2&gt;
&lt;p&gt;In 2014, two papers were published by members of the ENCODE project purporting that tissue gene expression clustered more by species than by tissue. In the first (&lt;a href=&#34;http://www.nature.com/nature/journal/v515/n7527/full/nature13992.html&#34;&gt;ENCODE Consortium, Nature, 2014, doi:10.1038/nature13992 2014&lt;/a&gt;), a large number of experiments were combined and compared. &lt;a href=&#34;http://www.nature.com/nature/journal/v515/n7527/fig_tab/nature13992_F2.html&#34;&gt;Figure 2a&lt;/a&gt; shows a PCA plot of expression in across 10 tissues in human and mouse.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://www.nature.com/nature/journal/v515/n7527/images/nature13992-f2.jpg&#34; alt=&#34;ENCODE paper figuer 2&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ENCODE paper figuer 2&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly enough, if you collapse PC1 (definitely species), then the tissues start to look quite similar. Which does not seem that unexpected, there are species specific differences, but the tissues are doing something similar in each species.&lt;/p&gt;
&lt;p&gt;This was not the end of the story, however. A subsequent publication (&lt;a href=&#34;http://www.pnas.org/content/111/48/17224&#34;&gt;S Lin et al., PNAS, 2014, doi: 10.1073/pnas.1413624111&lt;/a&gt;) went further in doing fresh sequencing of 13 tissues in both species, still showing bigger differences between species than between tissues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-everything-as-it-seems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is Everything As it Seems?&lt;/h2&gt;
&lt;p&gt;On April 28, Y. Gilad sent out &lt;a href=&#34;https://twitter.com/Y_Gilad/status/593088451462963202&#34;&gt;this tweet&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We reanalyzed the data from &lt;a href=&#34;http://t.co/Fv7z9WwLJ4&#34;&gt;http://t.co/Fv7z9WwLJ4&lt;/a&gt; and found the following: &lt;a href=&#34;http://t.co/37eVs8Kln9&#34;&gt;pic.twitter.com/37eVs8Kln9&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The left figure shows the original data, clustering by species, and on the right, reprocessed data, clustering by tissue. Now, I have to admit when he posted this I was kind of ticked off. Where was the blog-post or manuscript showing what exactly was done? If you look at the comments to Yoav on that tweet, it seems others were wondering the same thing. Thankfully, on May 19, the manuscript hit &lt;a href=&#34;http://f1000research.com/articles/4-121/v1&#34;&gt;F1000Research (Gilad Y and Mizrahi-Man O. A reanalysis of mouse ENCODE comparative gene expression data [v1; ref status: approved with reservations 1, http://f1000r.es/5ez] F1000Research 2015, 4:121 (doi: 10.12688/f1000research.6536.1))&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;batch-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Batch Effects&lt;/h2&gt;
&lt;p&gt;Yoav asked for the list of data files that were used in the PNAS paper, and then examined the read id line to extract the experimental design. This experimental design is captured in Figure 1:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://f1000researchdata.s3.amazonaws.com/manuscripts/7019/9f5f4330-d81d-46b8-9a3f-d8cb7aaf577e_figure1.gif&#34; alt=&#34;Y Golad Fig 1&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Y Golad Fig 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Do you notice a problem with this design?&lt;/p&gt;
&lt;p&gt;. . . . .&lt;/p&gt;
&lt;p&gt;Hopefully you noticed that species (one of the main effects to investigate) is not randomly or even semi-randomly distributed across sequencers and / or lanes, but is almost perfectly confounded with sequencer / lane. It doesn’t matter if one technician handled all the samples, this is potentially a large batch effect / confounding variable.&lt;/p&gt;
&lt;p&gt;And Yoav shows that ignoring the batch effect produces data much like that reported in the Lin et al PNAS pub, while removing the batch effect using &lt;a href=&#34;http://bioinformatics.oxfordjournals.org/content/28/6/882&#34;&gt;ComBat&lt;/a&gt; results in the tissues clustering together.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-the-data-still-useful&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is the Data Still Useful?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1_yA8RHdfodOV-5IoR-yb317KwaWgXFAvJntaIm8HYaU/edit?usp=sharing&#34;&gt;I presented&lt;/a&gt; these papers and the discussion around them at our weekly Systems Biology and Omics Integration (&lt;a href=&#34;http://sboi.bioinformatics.uky.edu&#34;&gt;SBOI&lt;/a&gt;) Journal Club on June 1, 2015. There were a couple of concerns with the overall study:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is ENCODE data still useful?&lt;/li&gt;
&lt;li&gt;some people seemed to be concerned that this brought the general ENCODE project into question. I think by the end we agreed that in general it is probably ok to be using ENCODE data, but this particular study was questionable.&lt;/li&gt;
&lt;li&gt;What does the data tell us?&lt;/li&gt;
&lt;li&gt;based on the correction that Yoav did and reanalysis, is there anything actually telling in the data? Unfortunately, because species is so confounded with machine, I don’t think there is much of a conclusion to draw about species differences &lt;em&gt;vs&lt;/em&gt; tissue similarity based on this data. There was some disagreement on this point.&lt;/li&gt;
&lt;li&gt;Full experimental designs should be published, and requested by reviewers&lt;/li&gt;
&lt;li&gt;Really, why a reviewer on the paper &lt;strong&gt;did not&lt;/strong&gt; request more information about the experimental design is beyond me, especially given the claims of the manuscript. Why it is not the norm to provide this kind of experimental design information in a manuscript is also a good question.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, I think the best use of the 2014 PNAS pub and this dataset is an example of &lt;strong&gt;how not&lt;/strong&gt; to design a biological experiment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addendum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;a id=&#34;addendum&#34;&gt;&lt;/a&gt; Addendum&lt;/h2&gt;
&lt;p&gt;As I looked at the comment section of the Gilad &amp;amp; Man article yesterday (June 1, 2015), I noticed that there were direct replies from S. Lin in a couple of places. In particular is a comment that Lin et al did a second set of sequencing with a new design, and reanalysed the data. Links are provided to two figures, a table of the new design and a new 3D PCA plot:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/slin_table1part2.jpg&#34; alt=&#34;new design&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;new design&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/slin_fig1part2.jpg&#34; alt=&#34;new pca&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;new pca&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The new sequencing design seems much more reasonable, and the PCA plot has many characteristics of the original one from the original comparative analysis by Mouse ENCODE (see above), in that yes, there are species specific differences, but there also appears to be a way to collapse along PC2 and PC3 where the tissues will line up with each other, which I kind of would expect.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>First Open Post-Publication Peer Review, with Credit!</title>
      <link>/post/first-open-post-publication-peer-review-with-credit/</link>
      <pubDate>Wed, 25 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/first-open-post-publication-peer-review-with-credit/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Reviewed &lt;a href=&#34;https://twitter.com/@biodataganache&#34;&gt;Jason McDermott’s&lt;/a&gt; &lt;a href=&#34;http://f1000research.com/articles/4-60/v1&#34;&gt;MDRPred paper&lt;/a&gt; on F1000Research!, where my review is posted along side the paper, &lt;a href=&#34;http://f1000research.com/articles/4-60/v1#referee-response-7889&#34;&gt;with a DOI&lt;/a&gt;, completely in the open with my name attached. Was a pleasant experience, aided by the fact that Jason wrote a good paper.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;f1000research&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;F1000Research!&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://f1000research.com/&#34;&gt;F1000Research!&lt;/a&gt; is a new publishing startup from F1000 that has a model of post-publication peer review, whereby upon submission the manuscript undergoes basic quality checks (no real editorial control), and then is published. Once the article is published, reviewers are invited to review, and they have 10 days to submit their review. Reviews are signed, and given a DOI. Reviewers are asked to assign one of &lt;strong&gt;Approved&lt;/strong&gt;, &lt;strong&gt;Approved with Reservations&lt;/strong&gt;, &lt;strong&gt;Not Approved&lt;/strong&gt; status to the article.&lt;/p&gt;
&lt;p&gt;When the article gets two revieweres giving &lt;strong&gt;Approved&lt;/strong&gt; status (likely after at least one round of review and resubmission), then it will be submitted to PubMed for indexing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-did-i-end-up-reviewing-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Did I End Up Reviewing This??&lt;/h2&gt;
&lt;p&gt;Long story short, I’ve been following Jason on twitter for a while, and I happened to see him tweet and blog about trying to get an F1000Research article up as a citeable supporting publication for a grant going in. I thought it was a nifty idea, and although I actually at the time did not have much of an idea of what Jason’s research actually involved, communicated with him that when the article went live I would be willing to be recommended as a reviewer. Imagine my surprise that I actually had enough domain knowledge that I could actually review the article.&lt;/p&gt;
&lt;p&gt;That is saying something, given that the paper has a neat combination of genetic algorithms, regular expressions, and protein function prediction (really, you should go give it a read). By the way, this is also the first paper I’ve reviewed in a long time that did not have serious methodological problems, or where claims are made with no substantiation, and I could not identify any serious statistical issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-open-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Open Part&lt;/h2&gt;
&lt;p&gt;Although I don’t do a lot of reviewing, I had to admit that this was one of the best reviewing assignments I’ve had in a while. Although I believe that peer review is essential to science, and it needs to be more open (I’ve started signing my reviews for other journals), this was the first time I knew my review was open (my name would be known to the authors’ by default) and public. I have to say that this likely improved the care and thoroughness in doing the actual review as I went through the article, given that both the authors and anyone else who comes across the article and my review can see if the issues I raise are real, or if I’m trying to make myself look good. And my name will be publicly associated with it!&lt;/p&gt;
&lt;p&gt;Note that this &lt;strong&gt;openness&lt;/strong&gt; did not keep me from criticizing particular aspects of the paper. There are lots of things that need to be changed in the article before I will give it an &lt;strong&gt;Approved&lt;/strong&gt; status, and I laid those out in my review.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postpublication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PostPublication&lt;/h2&gt;
&lt;p&gt;I really appreciated that I get to review a &lt;strong&gt;published&lt;/strong&gt; article, because it means that the whole thing is typeset, figures and tables are in their logical place (not at the end of the document!!!), the line spacing is readable, etc. Note to other publishers, at least let authors put the figures in-line for review if you are not going to type-set the article before it goes to reviewers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;disclaimers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Disclaimers&lt;/h2&gt;
&lt;p&gt;I was given an F1000Research! t-shirt as a reward for commenting on a &lt;a href=&#34;http://blog.f1000research.com/2013/08/23/peer-review-credit-where-credits-due/#comment-1020559658&#34;&gt;blog-post&lt;/a&gt; regarding incentives for peer-review. I like the shirt. Having done my review, I am also eligible to get a 50% discount on the article processing charges if I submit an article to F1000Research in the next 12 months.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Being a PhD Student and Post-Doc with Migraines</title>
      <link>/post/being-a-phd-student-and-post-doc-with-migraines/</link>
      <pubDate>Wed, 31 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/being-a-phd-student-and-post-doc-with-migraines/</guid>
      <description>&lt;p&gt;A blog post on the &lt;a href=&#34;http://jabberwocky.weecology.org/2014/10/13/how-technology-can-help-scientists-with-chronic-illnesses-or-technology-ftw/&#34;&gt;Weecology group blog&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/elitabaldridge&#34;&gt;Elita Baldridge&lt;/a&gt; on being a PhD student with fibromyalgia, and how they are working through that, caused me to pause and reflect on my experience as a PhD student and PostDoc with migraines. For those who haven’t read my blog, I do research in bioinformatics, specifically in transcriptomics and metabolomics. I spend almost all of my research hours in front of a computer writing code, generating plots, and trying to make sense of -omics level data.&lt;/p&gt;
&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Being in academia with migraines is a challenge, but probably less challenging than some other fields or disabilities. Moving cities and discovering Excedrin has made my migraines more bearable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;migraine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Migraine&lt;/h2&gt;
&lt;p&gt;For those who don’t know, migraines are a rather unusual neurological event that result in a bunch of symptoms, the most well known generally being aura (frequently visual, although auditory and olfactory are also known) followed by what is described as intense, debilitating pain, which may or may not be accompanied by extreme nausea and / or vomiting and tiredness.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-migraines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My Migraines&lt;/h2&gt;
&lt;p&gt;I have suffered from migraines since at least my early teens (possibly younger), but mine seem to be rather mild in comparison to what I have read about others experiences. In contrast to many other migraineurs, I do not experience visual aura, and in general my pain levels tend to be on the milder side. However, I do experience mood swings during my prodrome (period prior to pain), going from a generally nice guy to someone who considers killing you just for looking at me wrong; as well as confusion, and I also experience &lt;a href=&#34;http://en.wikipedia.org/wiki/Aphasia&#34;&gt;expressive aphasia&lt;/a&gt;, wherein I can no longer remember proper nouns, but can describe properties of the object in question (for example, instead of a &lt;strong&gt;pencil&lt;/strong&gt;, I might say “the writing implement with a lead center”). In addition to the pain of the migraine, which is often a pounding localized to one location of my skull, I often also experience extreme sensitivity to touch, to the point of not being able to sleep on a few occasions; as well as nausea, however never to the point of actually throwing up. Less frequently I will experience sensitivity to light or sound.&lt;/p&gt;
&lt;p&gt;As far as I can tell, my primary migraine trigger is changes in the weather, particularly large or frequent changes in barometric pressure. If the weather is stable (either nice or bad), the frequency and severity of my migraines are greatly reduced.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;phd&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PhD&lt;/h2&gt;
&lt;p&gt;During my undergraduate and masters degrees, I actually didn’t notice that much of an effect from my migraines. At some point, after a lot of investigation of my headaches, and alternative therapies, I did finally get a diagnosis of migraine, and a prescription for &lt;a href=&#34;http://www.rxlist.com/maxalt-drug.htm&#34;&gt;Maxalt&lt;/a&gt;, a triptan used to treat migraines. I used this very sparingly, due to the side effects, and the cost.&lt;/p&gt;
&lt;p&gt;My PhD was at &lt;a href=&#34;http://www.dal.ca/&#34;&gt;Dalhousie University&lt;/a&gt; in Halifax, NS, Canada. If you don’t know, &lt;a href=&#34;http://en.wikipedia.org/wiki/Halifax,_Nova_Scotia&#34;&gt;Halifax&lt;/a&gt; is a harbor city on the east coast of Nova Scotia, and as a result of its location has extremely variable weather, as it gets the west -&amp;gt; east weather from the rest of Canada, as well as weather systems going south -&amp;gt; north up the eastern seaboard of the United States. This results in a large amount of barometric pressure changes, and resulted in a lot of migraines for me. In general, my coping strategy when possible was to go home, take 2 tylenol, and go to bed for the rest of the day and sleep it off. As a PhD student in an analytical chemistry lab doing no wet lab work but only programming, this was often a viable option. My PhD supervisor was very supportive, in that as long as grades were good and project progress was being made, he was not particular about hours spent in the lab.&lt;/p&gt;
&lt;p&gt;Of course, as a PhD student, there were often times this was not possible. I took regular upper level classes in my first 2 years, and I never informed any of my instructors about my migraines, but often just suffered through classes when necessary. In addition, I TA’d first and second year chemistry lab. Given the other symptoms of my migraines, I actually feel sorry for the students I instructed. I know many times I had trouble resolving what were probably trivial mistakes by first year students, and losing my cool over something that was not that difficult to fix. Second year was a bit better in that the students had more experience in a lab setting, however I TA’d analytical chemistry, which essentially involved generating a series of standards, measuring the standards, and then measuring a sample with unknown concentration. There was generally fewer ways for any given lab experiment to fail in the 2nd year lab, putting a lighter cognitive load on me most days.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seminars-and-conferences&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Seminars and Conferences&lt;/h2&gt;
&lt;p&gt;Of course, as a student I had to attend weekly departmental seminars, and present a couple of times as well. I also traveled to a few conferences and presented talks and posters. To my recollection, I have not suffered a severe migraine during my own talks at a conference. However, I have had different occasions where I have had to skip out on conference sessions due to a migraine, and have been even less likely to socialize and network than I usually would because of migraines. This is hard, because one of the primary purposes of attending conferences is exposure to new research and people.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postdoc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PostDoc&lt;/h2&gt;
&lt;p&gt;My first PostDoc was at the University of Louisville in Louisville, KY, USA. My move to the US coincided with two things: 1) The introduction of Excedrin Migraine, and 2) My wife giving birth to our son. &lt;strong&gt;1&lt;/strong&gt; is important because Excedrin has allowed me to &lt;em&gt;manage&lt;/em&gt; my migraines. The side effects from Maxalt were almost as bad as the migraine itself, and it was bloody expensive (even with my wifes great drug plan). As a PostDoc supporting my wife and son, I always stopped to think about whether I actually needed it. Excedrin Migraine worked quite well in alleviating the pain from my migraines without introducing new side effects. I should note at this time that I was not a regular tea or coffee drinker, so the effect is not likely due to a caffeine addiction.&lt;/p&gt;
&lt;p&gt;Then I discovered that the regular Excedrin worked about the same, and was available in a generic form at many big box stores at an extremely low price ($4 for 100 tablets). I haven’t taken any Maxalt in over 3 years, and probably buy the generic Excedrin 100 count for $4 at WalMart every few months. Is this good for my stomach?? Maybe not. Do I need to watch myself if I get a cut within a short amount of time after taking Excedrin (ASA inhibits clotting)?? Yep. But it seems to mess with my head a lot less than the triptans I have taken (I have also tried Zomig in addition to the Maxalt).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt; is important because the arrival of our son resulted in a lot of increased stress (and don’t get me wrong, a lot of happiness and joy as well). Although not a primary trigger for my migraines, it definitely did not help. And having a migraine with a crying baby in the background is a special kind of torture (I was reminded of this again the other day with my 8 month old daughter).&lt;/p&gt;
&lt;p&gt;However, the move to the midwest US has resulted in a general decrease in the severity of my migraines, but I’m not sure about the frequency. Since moving here, I’ve only had to leave work because of a migraine once or twice a year, instead of the once a month that I used to in Nova Scotia.&lt;/p&gt;
&lt;p&gt;Obviously, my productivity at work during a migraine is reduced compared to not being in a migraine. However, there are often periods in academia when things just need to get done (collaborator has a paper or grant deadline, etc), and going home for me means a combined hour of walking and public transit, and a very small likelyhood of getting anything done even after the migraine has passed (having a 4 year old and 8 month old contributes to this a lot, but I wouldn’t have it any other way).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;challenges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Challenges&lt;/h2&gt;
&lt;p&gt;As I previously said, my migraines appear to be rather mild based on the descriptions of other peoples migraines I have encountered (see the &lt;a href=&#34;http://www.reddit.com/r/migraine&#34;&gt;migraine subreddit&lt;/a&gt; for some painful descriptions from other migaineurs). However, they can make some interactions difficult. My normally cheerful and upbeat demeanor can quickly turn sour, and I have been acutely aware of some rather awkward social interactions during a migraine. Thankfully, many times I am able to simply avoid others at work when I have a migraine, and I generally take pains to think twice before speaking as well (which is a good thing to do in general). I don’t teach, and the number of meetings that I &lt;strong&gt;have&lt;/strong&gt; to attend in a given week is rather small, thankfully (not just because of my migraines).&lt;/p&gt;
&lt;p&gt;The pain and mental confusion from my migraines can make getting work done difficult, and although Excedrin seems to do a wonderful job of dulling the pain, nothing can help with the mental confusion. Having a migraine in the middle of working on a large data analysis with many possible directions, or managing many different projects takes an incredible amount of mental effort when I am in a migraine. I have had times when I sit in front of my computer completely stunned because I cannot figure out what needs to be done next. At these points I frequently occupy myself with mindless tasks that need to be done, sending meeting emails, downloading papers to read, and trying to stay off of social media time sucks like Facebook and Reddit, as I tend to get heavily engrossed in those more so than usual.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prophylactic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prophylactic&lt;/h2&gt;
&lt;p&gt;This past year I finally tried CBD (cannibidiol) oil, an oil where only CBD has been extracted from the marijuana plant. Taking this twice a day, every day, has greatly reduced the number of migraines I have, and seems to reduce their intensity as well. It’s greatest effect is on the pain levels I encounter, and not so much the other neurological effects of the migraines.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Updated on 2017-12-28 with information on CBD oil.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Travis-CI to GitHub Pages</title>
      <link>/post/travis-ci-to-github-pages/</link>
      <pubDate>Wed, 05 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/travis-ci-to-github-pages/</guid>
      <description>&lt;p&gt;I don’t remember how I got on this, but I believe I had a recent twitter exchange with some persons (or saw it fly by) about pushing &lt;code&gt;R&lt;/code&gt; package vignettes to the web after building and checking on &lt;em&gt;travis-ci&lt;/em&gt;. &lt;a href=&#34;https://twitter.com/hadleywickham&#34;&gt;Hadley Wickham&lt;/a&gt; pointed to using such a scheme to push the web version of his book after each update and the &lt;code&gt;S3&lt;/code&gt; deploy hooks on &lt;em&gt;travis-ci&lt;/em&gt;. Deploying your html content to &lt;code&gt;S3&lt;/code&gt; is great, but given the availability of the &lt;code&gt;gh-pages&lt;/code&gt; branch on GitHub, I thought it would be neat to work out how to deploy the html output from an &lt;code&gt;R&lt;/code&gt; package vignette to the &lt;code&gt;gh-pages&lt;/code&gt; branch on GitHub. This is useful because more and more packages are being hosted only on GitHub and building and testing use the &lt;em&gt;travis-ci&lt;/em&gt; service, and it takes work to remember to &lt;code&gt;knit&lt;/code&gt; and push stuff separately to the &lt;code&gt;gh-pages&lt;/code&gt; branch. In addition, although it is possible to &lt;code&gt;deploy&lt;/code&gt; one’s html output to other services from within &lt;em&gt;travis-ci&lt;/em&gt;, there is not an &lt;strong&gt;easy&lt;/strong&gt; pushbutton solution to deploying to GitHub pages. After some searching and looking, this is what I have come up with for my own package.&lt;/p&gt;
&lt;p&gt;All subsequent steps assume that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You are hosting your package development on GitHub&lt;/li&gt;
&lt;li&gt;You have html content in the &lt;em&gt;gh-pages&lt;/em&gt; branch of your package repo (see more about &lt;em&gt;gh-pages&lt;/em&gt; &lt;a href=&#34;https://help.github.com/articles/creating-project-pages-manually/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://help.github.com/categories/github-pages-basics/&#34;&gt;here&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;You already have your package doing building and testing using &lt;em&gt;travis-ci&lt;/em&gt;. See the &lt;a href=&#34;https://github.com/craigcitro/r-travis&#34;&gt;&lt;code&gt;r-travis&lt;/code&gt;&lt;/a&gt; project for more information on how to set this up.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;oauth&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;OAUTH&lt;/h2&gt;
&lt;p&gt;To start, you will need to generate an &lt;code&gt;OAUTH&lt;/code&gt; token on GitHub that will be used to allow you to push back to your GitHub repo. This will be some really long alphanumeric string. You can generate one by going to &lt;code&gt;settings&lt;/code&gt; -&amp;gt; &lt;code&gt;applications&lt;/code&gt; -&amp;gt; &lt;code&gt;personal access tokens&lt;/code&gt; -&amp;gt; &lt;code&gt;generate new token&lt;/code&gt;. Make sure to copy this into a file that is &lt;strong&gt;not&lt;/strong&gt; under version control.&lt;/p&gt;
&lt;p&gt;You will also need to install the &lt;code&gt;travis&lt;/code&gt; ruby framework.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gem install travis&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After installing, navigate to your &lt;code&gt;git&lt;/code&gt; repo for the project you want to enable automatic pushing of content for, and then login to &lt;em&gt;travis-ci&lt;/em&gt; and secure the GitHub token.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;travis login

travis encrypt GH_TOKEN=&amp;quot;yourgithubtoken&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will generate output that should be copied to your &lt;code&gt;.travis.yml&lt;/code&gt; file. Essentially we have created an environment variable &lt;code&gt;GH_TOKEN&lt;/code&gt; with your actual GitHub token, that is encrypted on the &lt;em&gt;travis-ci&lt;/em&gt; servers. So this way you don’t expose your actual GitHub token to anyone who looks at your &lt;code&gt;.travis.yml&lt;/code&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deploy-script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deploy Script&lt;/h2&gt;
&lt;p&gt;We also need a bash script that will actually push the content for us. As part of the &lt;code&gt;R&lt;/code&gt; process on &lt;em&gt;travis-ci&lt;/em&gt;, we get a &lt;code&gt;tar.gz&lt;/code&gt; file of the package with the compiled vignette. So we just need to &lt;code&gt;untar&lt;/code&gt; that file, copy the html file, and create the &lt;code&gt;git&lt;/code&gt; repo and push. The code below is what I have done for my own package, &lt;a href=&#34;https://github.com/rmflight/categoryCompare&#34;&gt;&lt;code&gt;categoryCompare&lt;/code&gt;&lt;/a&gt;. I saved this code in the file &lt;code&gt;.push_gh_pages.sh&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash

rm -rf out || exit 0;
mkdir out;

GH_REPO=&amp;quot;@github.com/rmflight/categoryCompare.git&amp;quot;

FULL_REPO=&amp;quot;https://$GH_TOKEN$GH_REPO&amp;quot;

for files in &amp;#39;*.tar.gz&amp;#39;; do
        tar xfz $files
done

cd out
git init
git config user.name &amp;quot;rmflight-travis&amp;quot;
git config user.email &amp;quot;travis&amp;quot;
cp ../categoryCompare/inst/doc/categoryCompare_vignette.html index.html

git add .
git commit -m &amp;quot;deployed to github pages&amp;quot;
git push --force --quiet $FULL_REPO master:gh-pages&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that we remove the directory where we want to create our &lt;code&gt;git&lt;/code&gt; repo, create it, setup the &lt;code&gt;remote&lt;/code&gt; repo with the token string, and then we &lt;code&gt;untar&lt;/code&gt; and &lt;code&gt;unzip&lt;/code&gt; the previously built package, and copy over the file that we want to be the index.html page on the &lt;code&gt;gh-pages&lt;/code&gt; branch. Finally we add it, commit it, and do a force push.&lt;/p&gt;
&lt;p&gt;This script is actually part of the package repo, but does not get included in the built &lt;code&gt;tar.gz&lt;/code&gt; file (add it to &lt;code&gt;.Rbuildignore&lt;/code&gt;). This makes it easy to keep it in sync with any changes to the overall package itself.&lt;/p&gt;
&lt;p&gt;Note that this is completely overriding the current contents of the &lt;code&gt;gh-pages&lt;/code&gt; branch. If you wanted to do something nicer (&lt;em&gt;i.e.&lt;/em&gt; preserving commits or working with an index page pointing to multiple vignettes), you could &lt;code&gt;pull&lt;/code&gt; just the &lt;code&gt;gh-pages&lt;/code&gt; branch first, and then make modifications.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modifying-.travis.yml&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modifying .travis.yml&lt;/h2&gt;
&lt;p&gt;In addition, we need to add three lines to the &lt;code&gt;.travis.yml&lt;/code&gt; file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# under env: global:
  - secure: &amp;quot;yoursecurestring&amp;quot;

# under before_install:
  - chmod 755 ./.push_gh_pages.sh

# under after_success:
  - ./.push_gh_pages.sh&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Adding the &lt;code&gt;GH_TOKEN&lt;/code&gt; to the global environment variables&lt;/li&gt;
&lt;li&gt;Making the deploy script executable&lt;/li&gt;
&lt;li&gt;Adding the running of the deploy script &lt;code&gt;after_success&lt;/code&gt;, so only when build and check and tests run successfully&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And it seems to work quite nicely. As an example, my &lt;a href=&#34;https://github.com/rmflight/categoryCompare&#34;&gt;&lt;code&gt;categoryCompare&lt;/code&gt;&lt;/a&gt; package now has it’s vignette on the &lt;a href=&#34;http://rmflight.github.io/categoryCompare/&#34;&gt;&lt;code&gt;gh-pages&lt;/code&gt; branch&lt;/a&gt;, and this will get updated every time I push a commit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;update&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Update&lt;/h2&gt;
&lt;p&gt;As &lt;a href=&#34;https://twitter.com/cpsievert&#34;&gt;Carson&lt;/a&gt; pointed out below, there was an error in the second code block. &lt;code&gt;travis secure&lt;/code&gt; should be &lt;code&gt;travis encrypt&lt;/code&gt;, as you are &lt;strong&gt;encrypting&lt;/strong&gt; the credentials. &lt;strong&gt;secure&lt;/strong&gt; is for decrypting something that was already encrypted. Thanks for catching it!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>My Career Goals</title>
      <link>/post/my-career-goals/</link>
      <pubDate>Fri, 08 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/my-career-goals/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;I don’t want to be a PI because I enjoy spending time with my family, and don’t think I can handle the stress of juggling multiple grants, people, and deadlines. I want to be a staff member in a group that affords relative autonomy, while providing some security. If I’m lucky enough, my current position will enable that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bad-news&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bad News&lt;/h2&gt;
&lt;p&gt;If you keep up with the news in academia, this is a horrible time to be a &lt;em&gt;postdoc&lt;/em&gt; (post-doctoral) or &lt;em&gt;PI&lt;/em&gt; (principal investigator). The amount of money available for grants is at an all time low, and the likelihood of getting a grant is dismal. If you want to be successful, then you will need to work incredibly hard, sacrifice everything, and it is likely that you still won’t get grants that allow you to have an independent research lab at a decent university.&lt;/p&gt;
&lt;p&gt;Of course, this is worst in the biomedical field, due to a glut of NIH research money and universities allowing researchers salary to be composed largely of grant money, with very little money from the university itself. In addition grant money has been allowed to be used to support trainees; undergraduate, graduate, and postdoctoral. The compensation for the trainees was relatively minimal, leading to a glut of highly trained, poorly paid people who hope to move up the academic ladder. The large numbers of which has likely contributed to the current funding situation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-good-news&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Good News&lt;/h2&gt;
&lt;p&gt;The NIH and NSF are &lt;em&gt;starting&lt;/em&gt; to encourage research labs to hire staff at reasonable pay, and support trainees from specific grants. Although I keep hearing about this in general, I don’t know of any specific changes in grant applications that are causing any real changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;my-own-path&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;My Own Path&lt;/h2&gt;
&lt;p&gt;During my senior undergraduate and masters work, I realized that I probably couldn’t be a student forever, but I had hopes. During my PhD, however, I saw many PI’s who sacrified their time with families for their teaching, research, and administrative duties. This was even more pronounced during my first PostDoc, and made me question my continued progress in academia. I actually had a job offer as a research programmer with a research hospital during my first PostDoc, which I turned down because I realized that it was likely in that position (at least as advertised) there would be limited opportunity to pursue my own research ideas. However, grant funding success rate was continuing to plummet (see above), and I was not able to push out a first author paper before finishing my first PostDoc, so being successful in a faculty search was looking grim.&lt;/p&gt;
&lt;p&gt;When I started my second PostDoc position (transitioning from transcriptomics to metabolomics), I was up front with my supervisor that I did not feel that I was the type of person who could manage their own lab with independent funding, but that I wanted to find a place where I could stay on as staff while contributing research. My PI was sympathetic, and we worked well together, so he was immediately amenable to that, as long as the funding situation would provide.&lt;/p&gt;
&lt;p&gt;Fortunately, within a year of joining the lab, we and our collaborators landed a large center grant that provides stable funding for 5 years, and everyone involved (my PI, the other PI’s, myself) would be happy if I stayed on as staff after my time as a PostDoc. Already, as a PostDoc in this group I have a lot of latitude and freedom to pursue avenues of research, as long as they are somehow aligned with the overall goals of the lab. I am also finding myself involved in supervising various undergraduate and graduate projects, as well as proposing new avenues for research.&lt;/p&gt;
&lt;p&gt;I know the opportunity to stay on as staff in a research group is relatively rare in the current funding climate, and my job will be largely to develop novel methods and software to advance our understanding of metabolism in various biological states, as well as provide means to secure additional funding for the lab.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Analyses as Packages</title>
      <link>/post/analyses-as-packages/</link>
      <pubDate>Mon, 28 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/analyses-as-packages/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Instead of writing an analysis as a single or set of &lt;code&gt;R&lt;/code&gt; scripts, use a &lt;code&gt;package&lt;/code&gt; and include the analysis as a &lt;code&gt;vignette&lt;/code&gt; of the package. Read below for the why, the &lt;a href=&#34;/post/vignette-analysis&#34;&gt;how is in the next post&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analyses-and-reports&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analyses and Reports&lt;/h2&gt;
&lt;p&gt;As data science or statistical researchers, we tend to do a lot of analyses, whether for our own research or as part of a collaboration, or even for supervisors depending on where we work. As I have continued working in &lt;code&gt;R&lt;/code&gt;, I have progressed from having a simple &lt;code&gt;.R&lt;/code&gt; script (or collection of related scripts) to using a package to structure as much of my research as possible, including analyses that generate reports.&lt;/p&gt;
&lt;p&gt;Note that I have been meaning to write this post for a while, but the tipping point was seeing &lt;a href=&#34;https://twitter.com/hspter/statuses/489075887460339712&#34;&gt;these tweets&lt;/a&gt; from &lt;a href=&#34;https://twitter.com/hspter&#34;&gt;Hilary Parker&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/dnusinow&#34;&gt;David Nusinow&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I am all about the many short scripts rather than one long script when doing an analysis. I think I am alone here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;why-packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why Packages&lt;/h2&gt;
&lt;p&gt;Packages are &lt;code&gt;R&lt;/code&gt;’s method for sharing code in a sensible way, making it possible for others to easily (more often than not) use functions that you have written (I’m looking at you &lt;code&gt;python&lt;/code&gt;!). Why not use them? They also give you access to &lt;code&gt;R&lt;/code&gt;’s facilities for documentation and sharing computable documents. &lt;a href=&#34;https://twitter.com/hadleywickham&#34;&gt;Hadley Wickham&lt;/a&gt; has a nice section on packages in his &lt;a href=&#34;http://adv-r.had.co.nz/Philosophy.html&#34;&gt;Advanced R&lt;/a&gt; book.&lt;/p&gt;
&lt;p&gt;I use a lot of Hadley’s packages in the following sections, because they are useful, and promote practices that make it extremely practical to use packages as a way to make an analysis a self-contained unit.&lt;/p&gt;
&lt;p&gt;Duncan Murdoch has a nice slide deck on why to use packages and vignettes &lt;a href=&#34;http://www.stats.uwo.ca/faculty/murdoch/ism2013/5Vignettes.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;structure&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Structure&lt;/h3&gt;
&lt;p&gt;I want to breifly review the structure of package directories, you can read more about packages in Hadley’s book (link above), and in the official &lt;code&gt;R&lt;/code&gt; &lt;a href=&#34;http://cran.r-project.org/doc/manuals/r-release/R-exts.html&#34;&gt;documentation from &lt;code&gt;CRAN&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Packages impose a relatively simple structure on your project directory. &lt;code&gt;/R&lt;/code&gt; contains the &lt;code&gt;.R&lt;/code&gt; files with your actual functions, and &lt;code&gt;/data&lt;/code&gt; can contain any &lt;code&gt;.RData&lt;/code&gt; or &lt;code&gt;.rda&lt;/code&gt; files that you might need. Other data types (&lt;code&gt;.txt&lt;/code&gt;, &lt;code&gt;.tab&lt;/code&gt;, &lt;code&gt;.csv&lt;/code&gt;) can also go in &lt;code&gt;/data&lt;/code&gt;, or they may go in &lt;code&gt;/inst/extdata&lt;/code&gt;. Note that in &lt;code&gt;/inst/extdata&lt;/code&gt; you can specify any directory structure that seems appropriate.&lt;/p&gt;
&lt;p&gt;Other required files are &lt;a href=&#34;http://cran.r-project.org/doc/manuals/r-release/R-exts.html#The-DESCRIPTION-file&#34;&gt;&lt;code&gt;DESCRIPTION&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;http://cran.r-project.org/doc/manuals/r-release/R-exts.html#Package-namespaces&#34;&gt;&lt;code&gt;NAMESPACE&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You may also have &lt;code&gt;.Rmd&lt;/code&gt; or &lt;code&gt;.Rnw&lt;/code&gt; / &lt;code&gt;.Rtex&lt;/code&gt; files in &lt;code&gt;/vignettes&lt;/code&gt; that generate &lt;code&gt;html&lt;/code&gt; or &lt;code&gt;pdf&lt;/code&gt; output that combines prose and &lt;code&gt;R&lt;/code&gt; code into a single document. This is where things get really interesting in being able to package up an analysis, especially when combined with functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;p&gt;Almost any analysis I have done involves writing at least one function, generally more, because I almost never do anything once in an analysis. Packages are &lt;strong&gt;the&lt;/strong&gt; primary method of sharing functions in &lt;code&gt;R&lt;/code&gt; that make sure that your functions play nice with the &lt;code&gt;R&lt;/code&gt; NAMESPACE, and allow one to define function dependencies from other packages. If you define a function in a package (and &lt;code&gt;export&lt;/code&gt; it), it immediately becomes re-usable in multiple analyses, without worrying about suffering from copypasta.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;function-documentation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Function Documentation&lt;/h3&gt;
&lt;p&gt;The easiest way to document functions is by using &lt;a href=&#34;http://cran.r-project.org/web/packages/roxygen2/index.html&#34;&gt;&lt;code&gt;roxygen2&lt;/code&gt;&lt;/a&gt; (see the intro &lt;a href=&#34;http://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html&#34;&gt;vignette&lt;/a&gt;). This allows you to worry about the documentation right next to the function itself, and not worry about writing separate documentation files in &lt;code&gt;/inst/doc&lt;/code&gt; (really, you don’t want to do it, I have, and it is painful). The &lt;a href=&#34;http://cran.r-project.org/web/packages/roxygen2/vignettes/rd.html&#34;&gt;keywords&lt;/a&gt; in &lt;code&gt;roxygen2&lt;/code&gt; make sense, and are not hard to remember.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;As mentioned above, you can include data with your package. The neat thing about including data, is you can document it and have that documentation available as part of the package.&lt;/p&gt;
&lt;p&gt;I find it really useful to put any raw data that you want to work with in &lt;code&gt;/inst/extdata&lt;/code&gt; in whatever format it exists, and then process the data and save it as an &lt;code&gt;.RData&lt;/code&gt; file in &lt;code&gt;/data&lt;/code&gt;, with associated documentation. It is also really useful if part of the calculations are long running, then you can save the results as an associated data file, and simply load it when needed in the analysis.&lt;/p&gt;
&lt;p&gt;Small note about documenting data sets. You put the &lt;code&gt;roxygen2&lt;/code&gt; comments in another file, and also need to provide &lt;code&gt;@name&lt;/code&gt; explicitly, and follow the documentation block with &lt;code&gt;NULL&lt;/code&gt;. Check the &lt;code&gt;roxygen2&lt;/code&gt; vignette “Generating Rd files” for a specific example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-vignettes-as-a-reporting-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why Vignettes as a Reporting Method&lt;/h2&gt;
&lt;p&gt;One great feature of packages is that one can include multiple &lt;code&gt;vignettes&lt;/code&gt;, long form text mixed with &lt;code&gt;code&lt;/code&gt; (and/or figures) to explain or highlight functionality in a package. Normally these are used to write tutorials, demonstrate features, or group together documentation that wouldn’t normally be together in the general documentation. However, there are no limits as to what can actually be contained within the &lt;code&gt;vignette&lt;/code&gt; as far as content, or how many &lt;code&gt;vignettes&lt;/code&gt; a package can have.&lt;/p&gt;
&lt;p&gt;For packages hosted by CRAN, &lt;code&gt;vignettes&lt;/code&gt; are an optional component. However, the Bioconductor project requires that &lt;code&gt;vignettes&lt;/code&gt; be included in each package.&lt;/p&gt;
&lt;p&gt;So, &lt;code&gt;R&lt;/code&gt; packages have a method to include long form prose that can be mixed with &lt;code&gt;R&lt;/code&gt; code directly as part of the package, within which you have already put your functions and associated data.&lt;/p&gt;
&lt;p&gt;Prior to &lt;code&gt;R&lt;/code&gt; 3.0, one generally had to write vignettes using &lt;code&gt;sweave&lt;/code&gt;, a combination of &lt;code&gt;latex&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; code that generates a PDF file. However, since v3.0, it is possible to write vignettes using &lt;code&gt;R markdown&lt;/code&gt; (and actually some other markup formats), which generates HTML output. The advantages to using &lt;code&gt;R markdown&lt;/code&gt; over &lt;code&gt;sweave&lt;/code&gt; are that the syntax for writing &lt;code&gt;markdown&lt;/code&gt; is much simpler, and much more readable in it’s raw format.&lt;/p&gt;
&lt;p&gt;Given that a package allows us to define sets of related functions, data, and documentation (with dependencies defined) all in one place that others can subsequently install and make use of and build on, why wouldn’t you want to use packages and vignettes to write long form analyses?&lt;/p&gt;
&lt;p&gt;From some of my descriptions above, it may appear that this incurs some overhead. However, thanks to the &lt;strong&gt;#hadleyverse&lt;/strong&gt; and &lt;code&gt;rstudio&lt;/code&gt;, it is rather trivial (note that &lt;code&gt;rstudio&lt;/code&gt; is not essential, but I find it does make it easier). In my &lt;a href=&#34;/posts/2014/07/vignetteAnalysis.html&#34;&gt;next post&lt;/a&gt; I am going to give a worked example from start to finish of generating an analysis that is a vignette as part of a package.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
