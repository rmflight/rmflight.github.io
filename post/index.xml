<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Deciphering Life: One Bit at a Time</title>
    <link>/post/</link>
    <description>Recent content in Posts on Deciphering Life: One Bit at a Time</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Robert M Flight</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Nicer PNG Graphics</title>
      <link>/post/nicer-png-graphics/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/nicer-png-graphics/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you are getting crappy looking &lt;em&gt;png&lt;/em&gt; images from &lt;code&gt;rmarkdown&lt;/code&gt; html or word
documents, try using &lt;code&gt;type=&#39;cairo&#39;&lt;/code&gt; or &lt;code&gt;dev=&#39;CairoPNG&#39;&lt;/code&gt; in your chunk options.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;png-graphics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PNG Graphics??&lt;/h2&gt;
&lt;p&gt;So, I write a &lt;strong&gt;lot&lt;/strong&gt; of reports using &lt;code&gt;rmarkdown&lt;/code&gt; and &lt;code&gt;knitr&lt;/code&gt;, and have been
using &lt;code&gt;knitr&lt;/code&gt; for quite a while. My job involves doing analyses for collaborators
and communicating results. &lt;em&gt;Most&lt;/em&gt; of the time, I will generate a pdf report,
and I get beautiful graphics, thanks to the &lt;code&gt;eps&lt;/code&gt; graphics device. However,
there are times when I want to generate either word or html reports, and
in those cases, I tend to get very crappy looking graphics. See this example
image below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
p = ggplot(mtcars, aes(mpg, wt)) +
  geom_point(size = 3) +
  labs(x=&amp;quot;Fuel efficiency (mpg)&amp;quot;, y=&amp;quot;Weight (tons)&amp;quot;,
       title=&amp;quot;Seminal ggplot2 scatterplot example&amp;quot;,
       subtitle=&amp;quot;A plot that is only useful for demonstration purposes&amp;quot;,
       caption=&amp;quot;Brought to you by the letter &amp;#39;g&amp;#39;&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-06-nicer-png-graphics_files/figure-html/first_image-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note: This was generated on self-compiled R under Ubuntu 16.04. As we can
see, &lt;code&gt;knitr&lt;/code&gt; is using the &lt;code&gt;png&lt;/code&gt; device, because we are generating html output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::opts_chunk$get(&amp;quot;dev&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;png&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;increased-resolution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Increased Resolution&lt;/h2&gt;
&lt;p&gt;Of course, we just need to &lt;strong&gt;increase the resolution&lt;/strong&gt;! So let’s do so. Just to
go whole hog on this, let’s increase it to 300!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + ggtitle(&amp;quot;Seminal ggplot2 scatterplot example, 300 dpi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-06-nicer-png-graphics_files/figure-html/plot_300-1.png&#34; width=&#34;2100&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you compare this one to the previous, you can see that the quality is
&lt;em&gt;marginally&lt;/em&gt; better, but doesn’t seem to be anything like what you should
be able to get.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-svg&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use SVG??&lt;/h2&gt;
&lt;p&gt;Alternatively, we could tell &lt;code&gt;knitr&lt;/code&gt; to use the &lt;code&gt;svg&lt;/code&gt; device instead! Vector
graphics always look nice!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + ggtitle(&amp;quot;Seminal ggplot2 scatterplot example, dev = &amp;#39;svg&amp;#39;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-06-nicer-png-graphics_files/figure-html/plot_svg-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s so crisp! But, for word documents especially, this could be a problem,
as the images might not show up. The nice thing about &lt;em&gt;png&lt;/em&gt; is it should be
usable in just about any format!&lt;/p&gt;
&lt;p&gt;And, if you have a plot with a &lt;em&gt;lot&lt;/em&gt; of points (&amp;gt; 200), the &lt;em&gt;svg&lt;/em&gt; will start to
take up some serious disk space, as every single point is encoded in the &lt;em&gt;svg&lt;/em&gt; file.
This is also a good reason to use &lt;em&gt;png&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;png-via-cairo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PNG via Cairo&lt;/h2&gt;
&lt;p&gt;After pulling out my hair yesterday as I tried to generate nice &lt;em&gt;png&lt;/em&gt; images embedded
in a word report (and settling on converting every figure from svg to png and saving
to a folder to pass on, see &lt;a href=&#34;https://gist.github.com/rmflight/bb61ad1fd8ba6e44f734#make-png-of-high-density-svgs&#34;&gt;this&lt;/a&gt;),
I finally decided to try a different device.&lt;/p&gt;
&lt;p&gt;Now, your R installation does need to have either &lt;code&gt;cairo&lt;/code&gt; capabilities, or be able
to use the &lt;code&gt;Cairo&lt;/code&gt; package. Mine has both.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;capabilities()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        jpeg         png        tiff       tcltk         X11        aqua 
##        TRUE        TRUE        TRUE       FALSE        TRUE       FALSE 
##    http/ftp     sockets      libxml        fifo      cledit       iconv 
##        TRUE        TRUE        TRUE        TRUE       FALSE        TRUE 
##         NLS     profmem       cairo         ICU long.double     libcurl 
##        TRUE       FALSE        TRUE        TRUE        TRUE        TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;packageVersion(&amp;quot;Cairo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;#39;1.5.9&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s change the device (two different ways) and plot it again. First, we will
still use the &lt;code&gt;png&lt;/code&gt; device, but add the &lt;code&gt;type = &amp;quot;cairo&amp;quot;&lt;/code&gt; argument (see &lt;code&gt;?png&lt;/code&gt;). Just for information,
that looks like the below in the chunk options:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;r plot_cairo, dev.args = list(type = &amp;quot;cairo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + ggtitle(&amp;quot;Seminal ggplot2 scatterplot example, type = &amp;#39;cairo&amp;#39;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-06-nicer-png-graphics_files/figure-html/plot_cairo-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wow! This looks great! So much nicer than the other device. Secondly, let’s use
the &lt;code&gt;CairoPNG&lt;/code&gt; device (&lt;code&gt;dev = &amp;quot;CairoPNG&amp;quot;&lt;/code&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + ggtitle(&amp;quot;Seminal ggplot2 scatterplot example, dev = &amp;#39;CairoPNG&amp;#39;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-06-nicer-png-graphics_files/figure-html/plot_cairopng-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can also increase the resolution as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + ggtitle(&amp;quot;Seminal ggplot2 scatterplot example, dev = &amp;#39;CairoPNG&amp;#39;, dpi = 300&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-06-nicer-png-graphics_files/figure-html/plot_cairopng_300-1.png&#34; width=&#34;2100&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So there you have it. Very crisp &lt;em&gt;png&lt;/em&gt; images, with higher resolutions if needed,
and no jaggedness, without resorting to conversion via &lt;code&gt;inkscape&lt;/code&gt; (my previous go to).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-into-reports&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Incorporating Into Reports&lt;/h2&gt;
&lt;p&gt;As I previously mentioned, I often default to pdf reports, but will then generate
a word or html report if necessary. How do you avoid changing the options even
in a setup chunk if you want this to happen every time you specify &lt;code&gt;word_document&lt;/code&gt;
as the output type? This is what I settled on, the setup chunk checks the output type
(based on being called from &lt;code&gt;rmarkdown::render&lt;/code&gt;), and sets it appropriately.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (knitr::opts_knit$get(&amp;quot;rmarkdown.pandoc.to&amp;quot;) != &amp;quot;latex&amp;quot;) {
  knitr::opts_chunk$set(dpi = 300, dev.args = list(type = &amp;quot;cairo&amp;quot;))
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Don&#39;t do PCA After Statistical Testing!</title>
      <link>/post/don-t-do-pca-after-statistical-testing/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/don-t-do-pca-after-statistical-testing/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you do a statistical test &lt;strong&gt;before&lt;/strong&gt; a dimensional reduction method like
PCA, the highest source of variance is likely to be whatever you tested
statistically.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wait-why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wait, Why??&lt;/h2&gt;
&lt;p&gt;Let me describe the situation. You’ve done an &lt;code&gt;-omics&lt;/code&gt; level analysis on your
system of interest. You run a t-test (or ANOVA, etc) on each of the features
in your data (gene, protein, metabolite, etc). Filter down to those things that
were statistically significant, and then finally, you decide to look at the data
using a dimensionality reduction method such as &lt;em&gt;principal components analysis&lt;/em&gt;
(PCA) so you can &lt;strong&gt;see&lt;/strong&gt; what is going on.&lt;/p&gt;
&lt;p&gt;I have seen this published at least once (in a Diabetes metabolomics paper,
if anyone knows it, please send it to me so I can link it), and have seen
collaborators do this after coaching from others in non-statistical departments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;The problem is that PCA is just looking at either feature-feature covariances
or sample-sample covariances. If you have trimmed the data to those things that
have statistically significant differences, then you have completely modified
the covariances, and PCA is likely to pick up on that.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Example&lt;/h2&gt;
&lt;p&gt;Let’s actually do an example where there are no differences initially, and
then see if we can introduce an artificial difference.&lt;/p&gt;
&lt;div id=&#34;random-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random Data&lt;/h3&gt;
&lt;p&gt;We start with completely random data, 10000 features, and 100 samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_feat = 10000
n_sample = 100
random_data = matrix(rnorm(n_feat * n_sample), nrow = n_feat, ncol = n_sample)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we will do a t-test on each row, taking the first 50 samples as class 1
and the other 50 samples as class 2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t_test_res = purrr::map_df(seq(1, nrow(random_data)), function(in_row){
  tidy(t.test(random_data[in_row, 1:50], random_data[in_row, 51:100]))
})&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How many are significant at a p-value of 0.05?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filter(t_test_res, p.value &amp;lt;= 0.05) %&amp;gt;% dim()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 514  10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Obviously, these are false positives, but they are enough for us to illustrate
the problem.&lt;/p&gt;
&lt;p&gt;First, lets do PCA on the whole data set of 10000 features.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_classes = data.frame(class = c(rep(&amp;quot;A&amp;quot;, 50), rep(&amp;quot;B&amp;quot;, 50)))

all_pca = prcomp(t(random_data), center = TRUE, scale. = FALSE)
visqc_pca(all_pca, groups = sample_classes$class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-14-don-t-do-pca-after-statistical-testing_files/figure-html/all_pca-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Obviously, there is no difference in the groups, and the % explained variance
is very low.&lt;/p&gt;
&lt;p&gt;Second, lets do it on just those things that were significant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sig_pca = prcomp(t(random_data[which(t_test_res$p.value &amp;lt;= 0.05), ]), center = TRUE,
                 scale. = FALSE)

visqc_pca(sig_pca, groups = sample_classes$class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-14-don-t-do-pca-after-statistical-testing_files/figure-html/sig_pca-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And look at that! We have separation of the two groups! But …., this is
completely random data, that didn’t have any separation, &lt;strong&gt;until we did the
statistical test&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;take-away&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Take Away&lt;/h2&gt;
&lt;p&gt;Be careful of the order in which you do things. If you want to do dimensionality
reduction to look for issues with the samples, then do that &lt;strong&gt;before&lt;/strong&gt; any statistical
testing on the individual features.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finding Modes Using Kernel Density Estimates</title>
      <link>/post/finding-modes-using-kernel-density-estimates/</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/finding-modes-using-kernel-density-estimates/</guid>
      <description>&lt;div id=&#34;tl-dr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL; DR&lt;/h2&gt;
&lt;p&gt;If you have a unimodal distribution of values, you can use R’s &lt;code&gt;density&lt;/code&gt; or
Scipy’s &lt;code&gt;gaussian_kde&lt;/code&gt; to create density estimates of the data, and then
take the maxima of the density estimate to get the &lt;code&gt;mode&lt;/code&gt;. See below for
actual examples in R and Python.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mode-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mode in R&lt;/h2&gt;
&lt;p&gt;First, lets do this in R. Need some values to work with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
set.seed(1234)
n_point &amp;lt;- 1000
data_df &amp;lt;- data.frame(values = rnorm(n_point))

ggplot(data_df, aes(x = values)) + geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-19-finding-modes-using-kernel-density-estimates_files/figure-html/density_mode-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data_df, aes(x = values)) + geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-19-finding-modes-using-kernel-density-estimates_files/figure-html/density_mode-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do a kernel density, which will return an object with a bunch of peices.
One of these is &lt;code&gt;y&lt;/code&gt;, which is the actual density value for each value of &lt;code&gt;x&lt;/code&gt; that was used! So
we can find the &lt;code&gt;mode&lt;/code&gt; by querying &lt;code&gt;x&lt;/code&gt; for the maxima in &lt;code&gt;y&lt;/code&gt;!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;density_estimate &amp;lt;- density(data_df$values)

mode_value &amp;lt;- density_estimate$x[which.max(density_estimate$y)]
mode_value&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.04599328&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the density estimate with the mode location.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;density_df &amp;lt;- data.frame(value = density_estimate$x, density = density_estimate$y)

ggplot(density_df, aes(x = value, y = density)) + geom_line() + geom_vline(xintercept = mode_value, color = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-19-finding-modes-using-kernel-density-estimates_files/figure-html/plot_density-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Python&lt;/h2&gt;
&lt;p&gt;Lets do something similar in Python. Start by generating a set of random values.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
values = np.random.normal(size = 1000)
plt.hist(values)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-19-finding-modes-using-kernel-density-estimates_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And then use &lt;code&gt;gaussian_kde&lt;/code&gt; to get a kernel estimator of the density, and then
call the &lt;code&gt;pdf&lt;/code&gt; method on the original values.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;kernel = stats.gaussian_kde(values)
height = kernel.pdf(values)
mode_value = values[np.argmax(height)]
print(mode_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -0.09150664440509323&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot to show indeed we have it right. Note we sort the values first so the PDF
looks right.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;values2 = np.sort(values.copy())
height2 = kernel.pdf(values2)
plt.clf()
plt.cla()
plt.close()
plt.plot(values2, height2)
plt.axvline(mode_value)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-19-finding-modes-using-kernel-density-estimates_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Split - Unsplit Anti-Pattern</title>
      <link>/post/split-unsplit-anti-pattern/</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/split-unsplit-anti-pattern/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you notice yourself using &lt;code&gt;split&lt;/code&gt; -&amp;gt; &lt;code&gt;unsplit&lt;/code&gt; / &lt;code&gt;rbind&lt;/code&gt; on two object to
match items up, maybe you should be using &lt;code&gt;dplyr::join_&lt;/code&gt; instead. Read below
for concrete examples.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;I have had a lot of calculations lately that involve some sort of &lt;code&gt;normalization&lt;/code&gt;
or scaling a group of related values, each group by a different factor.&lt;/p&gt;
&lt;p&gt;Lets setup an example where we will have &lt;code&gt;1e5&lt;/code&gt; values in &lt;code&gt;10&lt;/code&gt; groups, each group
of values being &lt;code&gt;normalized&lt;/code&gt; by their own value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
library(profvis)
set.seed(1234)
n_point &amp;lt;- 1e5
to_normalize &amp;lt;- data.frame(value = rnorm(n_point), group = sample(seq_len(10), n_point, replace = TRUE))

normalization &amp;lt;- data.frame(group = seq_len(10), normalization = rnorm(10))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each &lt;code&gt;group&lt;/code&gt; in &lt;code&gt;to_normalize&lt;/code&gt;, we want to apply the normalization factor in
&lt;code&gt;normalization&lt;/code&gt;. In this case, I’m going to do a simple subtraction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;match-them&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Match Them!&lt;/h2&gt;
&lt;p&gt;My initial implementation was to iterate over the groups, and use &lt;code&gt;%in%&lt;/code&gt; to
&lt;code&gt;match&lt;/code&gt; each &lt;code&gt;group&lt;/code&gt; from the normalization factors and the data to be normalized,
and modify in place. &lt;strong&gt;Don’t do this!!&lt;/strong&gt; It was
the slowest method I’ve used in my real package code!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;match_normalization &amp;lt;- function(normalize_data, normalization_factors){
  use_groups &amp;lt;- normalization_factors$group
  
  for (igroup in use_groups) {
    normalize_data[normalize_data$group %in% igroup, &amp;quot;value&amp;quot;] &amp;lt;- 
      normalize_data[normalize_data$group %in% igroup, &amp;quot;value&amp;quot;] - normalization_factors[normalization_factors$group %in% igroup, &amp;quot;normalization&amp;quot;]
  }
  normalize_data
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;micro_results &amp;lt;- summary(microbenchmark(match_normalization(to_normalize, normalization)))
knitr::kable(micro_results)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;expr&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;uq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;neval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;match_normalization(to_normalize, normalization)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37.93602&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40.23026&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.19333&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.44218&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;42.70328&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70.9454&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Not bad for the test data. But can we do better?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;split-them&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split Them!&lt;/h2&gt;
&lt;p&gt;My next thought was to split them by their &lt;code&gt;group&lt;/code&gt;s, and then iterate again over
the groups using &lt;code&gt;purrr::map&lt;/code&gt;, and then unlist them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split_normalization &amp;lt;- function(normalize_data, normalization_factors){
  split_norm &amp;lt;- split(normalization_factors$normalization, normalization_factors$group)
  
  split_data &amp;lt;- split(normalize_data, normalize_data$group)
  
  out_data &amp;lt;- purrr::map2(split_data, split_norm, function(.x, .y){
    .x$value &amp;lt;- .x$value - .y
    .x
  })
  do.call(rbind, out_data)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;micro_results2 &amp;lt;- summary(microbenchmark(match_normalization(to_normalize, normalization),
               split_normalization(to_normalize, normalization)))
knitr::kable(micro_results2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;expr&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;uq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;neval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;match_normalization(to_normalize, normalization)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41.25470&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47.41014&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;52.46320&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51.22454&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;56.02441&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100.2324&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;split_normalization(to_normalize, normalization)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;68.82265&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80.19980&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86.39694&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85.98446&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89.30600&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;148.1777&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;join-them&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Join Them!&lt;/h2&gt;
&lt;p&gt;My final thought was to join the two data.frame’s together using &lt;code&gt;dplyr&lt;/code&gt;, and then
they are automatically matched up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;join_normalization &amp;lt;- function(normalize_data, normalization_factors){
  normalize_data &amp;lt;- dplyr::right_join(normalize_data, normalization_factors,
                                      by = &amp;quot;group&amp;quot;)
  
  normalize_data$value &amp;lt;- normalize_data$value - normalize_data$normalization
  normalize_data[, c(&amp;quot;value&amp;quot;, &amp;quot;group&amp;quot;)]
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;micro_results3 &amp;lt;- summary(microbenchmark(match_normalization(to_normalize, normalization),
               split_normalization(to_normalize, normalization),
               join_normalization(to_normalize, normalization)))
knitr::kable(micro_results3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;expr&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;uq&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;neval&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;match_normalization(to_normalize, normalization)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39.030586&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46.793487&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51.20821&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49.311902&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;53.876978&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;109.9914&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;split_normalization(to_normalize, normalization)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70.381722&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80.093530&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;85.62029&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;84.895922&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;87.885402&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;143.6245&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;join_normalization(to_normalize, normalization)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.043585&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.343492&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.72770&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.479771&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.683511&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;140.9011&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;So on my computer, the &lt;code&gt;split&lt;/code&gt; and &lt;code&gt;match&lt;/code&gt; implementations are mostly comparable,
although on my motivating real world example, I actually got a 3X speedup by
using the &lt;code&gt;split&lt;/code&gt; method. That may be because of issues related to &lt;code&gt;DataFrame&lt;/code&gt;
and matching elements within that structure. The &lt;code&gt;join&lt;/code&gt; method is 10-14X faster
than the others, which is what I’ve seen in my motivating work. I also think
it makes the code easier to read and reason over, because you can see what
is being subtracted from what directly in the code.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using IRanges for Non-Integer Overlaps</title>
      <link>/post/iranges-for-non-integer-overlaps/</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/iranges-for-non-integer-overlaps/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://bioconductor.org/packages/IRanges/&#34;&gt;&lt;code&gt;IRanges&lt;/code&gt;&lt;/a&gt; package implements interval algebra, and is very fast for finding overlaps of two ranges. If you have non-integer data, multiply values by a &lt;strong&gt;large&lt;/strong&gt; constant factor and round them. The constant depends on how much accuracy you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iranges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IRanges??&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bioconductor.org/packages/IRanges/&#34;&gt;&lt;code&gt;IRanges&lt;/code&gt;&lt;/a&gt; is a bioconductor package for interval algebra of &lt;strong&gt;i&lt;/strong&gt;nteger &lt;strong&gt;ranges&lt;/strong&gt;. It is used extensively in the &lt;code&gt;GenomicRanges&lt;/code&gt; package for finding overlaps between various genomic features. For genomic features, &lt;strong&gt;integers&lt;/strong&gt; make sense, because one cannot have fractional base locations.&lt;/p&gt;
&lt;p&gt;However, &lt;code&gt;IRanges&lt;/code&gt; uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Red%E2%80%93black_tree&#34;&gt;red-black trees&lt;/a&gt; as its data structure, which provide very fast searches of overlaps. This makes it very attractive for &lt;strong&gt;any&lt;/strong&gt; problem that involves overlapping ranges.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;My motivation comes from mass-spectrometry data, where I want to count the number of raw data points and / or the number of peaks in a &lt;strong&gt;large&lt;/strong&gt; number of M/Z windows. Large here means on the order of 1,000,000 M/Z windows.&lt;/p&gt;
&lt;p&gt;Generating the windows is not hard, but searching the list of points / peaks for which ones are within the bounds of a window takes &lt;strong&gt;a really long time&lt;/strong&gt;. Long enough that I needed some other method.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iranges-to-the-rescue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IRanges to the Rescue!&lt;/h2&gt;
&lt;p&gt;So my idea was to use &lt;code&gt;IRanges&lt;/code&gt;. But there is a problem, &lt;code&gt;IRanges&lt;/code&gt; is for integer ranges. How do we use this for non-integer data? Simple, multiply and round the fractional numbers to generate integers.&lt;/p&gt;
&lt;p&gt;It turns out that multiplying our mass-spec data by &lt;code&gt;20,000&lt;/code&gt; gives us differences down to the &lt;code&gt;0.00005&lt;/code&gt; place, which is more than enough accuracy for the size of the windows we are interested in. If needed, &lt;code&gt;IRanges&lt;/code&gt; can handle &lt;code&gt;1600 * 1e6&lt;/code&gt;, but currently will crash at &lt;code&gt;1600 * 1e7&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-fast-is-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Fast Is It?&lt;/h2&gt;
&lt;p&gt;Lets actually test differences in speed by counting how many overlapping points there are.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(IRanges)
library(ggplot2)
load(&amp;quot;../../data/iranges_example_data.rda&amp;quot;)

head(mz_points)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IRanges object with 6 ranges and 1 metadata column:
##           start       end     width |               mz
##       &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; |        &amp;lt;numeric&amp;gt;
##   [1]   2970182   2970182         1 | 148.509100524992
##   [2]   2970183   2970183         1 | 148.509161249802
##   [3]   2970184   2970184         1 |  148.50922197465
##   [4]   2970186   2970186         1 | 148.509282699535
##   [5]   3000526   3000526         1 | 150.026298638453
##   [6]   3000527   3000527         1 | 150.026360296201&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mz_windows)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IRanges object with 6 ranges and 2 metadata columns:
##           start       end     width |         mz_start           mz_end
##       &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; |        &amp;lt;numeric&amp;gt;        &amp;lt;numeric&amp;gt;
##   [1]   2960000   2960011        12 |              148 148.000554529159
##   [2]   2960001   2960012        12 | 148.000055452916 148.000609982075
##   [3]   2960002   2960013        12 | 148.000110905832 148.000665434991
##   [4]   2960003   2960014        12 | 148.000166358748 148.000720887907
##   [5]   2960004   2960016        13 | 148.000221811664 148.000776340823
##   [6]   2960006   2960017        12 |  148.00027726458 148.000831793739&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have some &lt;a href=&#34;https://github.com/rmflight/researchBlog_blogdown/blob/master/data/iranges_example_data.rda&#34;&gt;example data&lt;/a&gt; with 3447542 windows, and 991816 points. We will count how many point there are in each window using the below functions, with differing number of windows.&lt;/p&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_overlaps_naive &amp;lt;- function(mz_start, mz_end, points){
  sum((points &amp;gt;= mz_start) &amp;amp; (points &amp;lt;= mz_end))
}

iterate_windows &amp;lt;- function(windows, points){
  purrr::pmap_int(windows, count_overlaps_naive, points)
}

run_times_iterating &amp;lt;- function(windows, points){
  t &amp;lt;- Sys.time()
  window_counts &amp;lt;- iterate_windows(windows, points)
  t2 &amp;lt;- Sys.time()
  run_time &amp;lt;- difftime(t2, t, units = &amp;quot;secs&amp;quot;)
  run_time
}

run_times_countoverlaps &amp;lt;- function(windows, points){
  t &amp;lt;- Sys.time()
  window_counts &amp;lt;- countOverlaps(points, windows)
  t2 &amp;lt;- Sys.time()
  run_time &amp;lt;- difftime(t2, t, units = &amp;quot;secs&amp;quot;)
  run_time
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;define-samples-of-different-sizes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Define Samples of Different Sizes&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

sample_sizes &amp;lt;- c(10, 100, 1000, 10000, 50000, 100000)

window_samples &amp;lt;- purrr::map(sample_sizes, function(x){sample(length(mz_windows), size = x)})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;run-it&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run It&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iranges_times &amp;lt;- purrr::map_dbl(window_samples, function(x){
  run_times_countoverlaps(mz_windows[x], mz_points)
})

window_frame &amp;lt;- as.data.frame(mcols(mz_windows))

naive_times &amp;lt;- purrr::map_dbl(window_samples, function(x){
  run_times_iterating(window_frame[x, ], mz_points)
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-them&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot Them&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_times &amp;lt;- data.frame(size = rep(sample_sizes, 2),
                        time = c(iranges_times, naive_times),
                        method = rep(c(&amp;quot;iranges&amp;quot;, &amp;quot;naive&amp;quot;), each = 6))

p &amp;lt;- ggplot(all_times, aes(x = log10(size), y = time, color = method)) + geom_point() + geom_line() + labs(y = &amp;quot;time (s)&amp;quot;, x = &amp;quot;log10(# of windows)&amp;quot;, title = &amp;quot;Naive &amp;amp; IRanges Timings&amp;quot;) + theme(legend.position = c(0.2, 0.8))
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-23-iranges-for-non-integer-overlaps_files/figure-html/difference_times-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + ylim(c(0, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-23-iranges-for-non-integer-overlaps_files/figure-html/difference_times-2.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the two figures show, the naive solution, while a little faster under 1000 regions, is quickly outperformed by &lt;code&gt;IRanges&lt;/code&gt;, whose time increases much more slowly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Turn Robert&#39;s Beard Purple!</title>
      <link>/post/turn-roberts-beard-purple/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/turn-roberts-beard-purple/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If I &lt;a href=&#34;http://act.alz.org/site/TR?pg=personal&amp;amp;px=14694554&amp;amp;fr_id=11244&#34;&gt;raise $100 by August 25ths The Walk to End Alzheimer’s&lt;/a&gt;, I will have my beard dyed purple in support of Alzheimer’s awareness.&lt;/p&gt;
&lt;p&gt;If you are in another country, donate to your local Alzheimer’s charity and &lt;a href=&#34;http://rmflight.github.io/#contact&#34;&gt;email me&lt;/a&gt; with the subject &lt;em&gt;walk&lt;/em&gt; so I count it towards my total.&lt;/p&gt;
&lt;p&gt;Links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://act.alz.org/site/TR?pg=personal&amp;amp;px=14694554&amp;amp;fr_id=11244&#34;&gt;My donation page&lt;/a&gt; (&lt;a href=&#34;https://www.charitywatch.org/ratings-and-metrics/alzheimers-association-national-office/461&#34;&gt;Charity report on Alzheimer’s Association&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.facebook.com/donate/2012668322318175/?fundraiser_source=external_url&#34;&gt;Facebook Fundraising Page&lt;/a&gt; (if you want to share it!)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://secure2.convio.net/alz/site/Donation2;jsessionid=00000000.app202b?df_id=1683&amp;amp;mfc_pref=T&amp;amp;1683.donation=form1&amp;amp;s_locale=en_CA&amp;amp;NONCE_TOKEN=D46EF58FAD6FA1276A4C99FBED9E9155&#34;&gt;Alzheimer Society Canada&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://secure.alzheimers.org.uk/?gclid=CjwKCAjw9qfZBRA5EiwAiq0AbScJN_IE7JW3arALIYmd2F2gzMk3ZV32sT9FGyR3yx_spMlgUzqMkhoCgUEQAvD_BwE&#34;&gt;Alzheimer’s Society UK&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;updates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Updates!&lt;/h2&gt;
&lt;div id=&#34;july-10&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;July 10&lt;/h3&gt;
&lt;p&gt;Thanks to everyone on &lt;a href=&#34;https://www.facebook.com/donate/2012668322318175/?fundraiser_source=external_url&#34;&gt;Facebook&lt;/a&gt;, we’ve raised $5 for the American Alzheimer’s Association, and $30 for the Alzheimer Society in Canada!&lt;/p&gt;
&lt;p&gt;Back on June 25, I went in to &lt;a href=&#34;http://bak4morestudio.com/&#34;&gt;Bak 4 More Studio&lt;/a&gt; and the awesome &lt;a href=&#34;https://www.facebook.com/Brittany-B-at-Bak-4-More-Studio-809676079219735/&#34;&gt;Brittany B&lt;/a&gt; did some testing to see how hard it would be to lighten my beard so it can be colored. She was great, and thinks it will be able to be done all in one session. I am currently booked to go in on Monday, August 20th to get the coloring done. Thanks again to Helue and Brittany for their help in this!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Video&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/5cmMtfDnqbM?rel=0&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;why-a-purple-beard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why a Purple Beard??&lt;/h2&gt;
&lt;p&gt;I decided to participate in the Walk To End Alzheimer’s this year, coming up on August 25th here in Lexington. I will be walking the 2 miles in support of my Mom, who was diagnosed with early onset Alzheimer’s some time ago (she will turn 68 this summer).&lt;/p&gt;
&lt;p&gt;Why am I walking?? Because I’ve seen a little bit of what Alzheimer’s does, and although my Mom won’t benefit from new treatments, we need to find effective treatments for this devastating disease.&lt;/p&gt;
&lt;p&gt;As an incentive to donate, if I reach my &lt;a href=&#34;http://act.alz.org/site/TR?pg=personal&amp;amp;px=14694554&amp;amp;fr_id=11244&#34;&gt;fundraising goal of $100&lt;/a&gt;, I will have my beard dyed Alzheimer’s Association purple! The wonderful folks at &lt;a href=&#34;http://bak4morestudio.com/&#34;&gt;Bak 4 More studios&lt;/a&gt; have agreed to help me dye my beard purple before the Walk date. I will make sure to take lots of video and pictures of the process, which I am told will probably involve two trips to the studio, one to lighten my beard and a second to actually apply the purple color.&lt;/p&gt;
&lt;p&gt;Using the magic of computers, I’ve tried to show here what that might look like. I’m sure it will look 100X better than these, this is just to give a possible idea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/selfie_walk_lores.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;I also realize that not everyone may be able to donate to the American Alzheimer’s Association, so if you want to support my fundraising for the Walk (and turn my beard purple) by donating to your local Alzheimer’s charity, just &lt;a href=&#34;http://rmflight.github.io/#contact&#34;&gt;send me an email&lt;/a&gt; with the subject &lt;em&gt;walk&lt;/em&gt; with the amount you donated and to which charity, and I will count your donation towards your local charity towards my goal. I will also respond to your email so that you know I’ve counted it.&lt;/p&gt;
&lt;p&gt;I will NOT be trimming my beard between now and the Walk date, so there will be even &lt;strong&gt;more purple beard&lt;/strong&gt; to go around, with 9 weeks between now and then.&lt;/p&gt;
&lt;p&gt;Thank you for helping me raise funds for the Walk to End Alzheimer’s, and feel free to spread this post far and wide.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>knitrProgressBar Package</title>
      <link>/post/knitrprogressbar/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/knitrprogressbar/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you like &lt;code&gt;dplyr&lt;/code&gt; progress bars, and wished you could use them everywhere, including from within Rmd documents, non-interactive shells, etc, then you should check out &lt;code&gt;knitrProgressBar&lt;/code&gt; (&lt;a href=&#34;https://cran.r-project.org/package=knitrProgressBar&#34;&gt;cran&lt;/a&gt; &lt;a href=&#34;https://github.com/rmflight/knitrProgressBar&#34;&gt;github&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-yet-another-progress-bar&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why Yet Another Progress Bar??&lt;/h2&gt;
&lt;p&gt;I didn’t set out to create &lt;strong&gt;another&lt;/strong&gt; progress bar package. But I really liked &lt;code&gt;dplyr&lt;/code&gt;s style of progress bar, and how they worked under the hood (thanks to the &lt;a href=&#34;https://rud.is/b/2017/03/27/all-in-on-r%E2%81%B4-progress-bars-on-first-post/&#34;&gt;examples&lt;/a&gt; from Bob Rudis).&lt;/p&gt;
&lt;p&gt;As I used them, I noticed that no progress was displayed if you did &lt;code&gt;rmarkdown::render()&lt;/code&gt; or &lt;code&gt;knitr::knit()&lt;/code&gt;. That just didn’t seem right to me, as that means you get no progress indicator if you want to use caching facilities of &lt;code&gt;knitr&lt;/code&gt;. So this package was born.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How??&lt;/h2&gt;
&lt;p&gt;These are pretty easy to setup and use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitrProgressBar)

# borrowed from example by @hrbrmstr
arduously_long_nchar &amp;lt;- function(input_var, .pb=NULL) {
  
  update_progress(.pb) # function from knitrProgressBar
  
  Sys.sleep(0.01)
  
  nchar(input_var)
  
}

# using stdout() here so progress is part of document
pb &amp;lt;- progress_estimated(26, progress_location = stdout())

purrr::map(letters, arduously_long_nchar, .pb = pb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
|===                                              |  8% ~1 s remaining     
|=============                                    | 27% ~0 s remaining     
|======================                           | 46% ~0 s remaining     
|================================                 | 65% ~0 s remaining     
|=========================================        | 85% ~0 s remaining     
Completed after 0 s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1
## 
## [[3]]
## [1] 1
## 
## [[4]]
## [1] 1
## 
## [[5]]
## [1] 1
## 
## [[6]]
## [1] 1
## 
## [[7]]
## [1] 1
## 
## [[8]]
## [1] 1
## 
## [[9]]
## [1] 1
## 
## [[10]]
## [1] 1
## 
## [[11]]
## [1] 1
## 
## [[12]]
## [1] 1
## 
## [[13]]
## [1] 1
## 
## [[14]]
## [1] 1
## 
## [[15]]
## [1] 1
## 
## [[16]]
## [1] 1
## 
## [[17]]
## [1] 1
## 
## [[18]]
## [1] 1
## 
## [[19]]
## [1] 1
## 
## [[20]]
## [1] 1
## 
## [[21]]
## [1] 1
## 
## [[22]]
## [1] 1
## 
## [[23]]
## [1] 1
## 
## [[24]]
## [1] 1
## 
## [[25]]
## [1] 1
## 
## [[26]]
## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main difference to &lt;code&gt;dplyr&lt;/code&gt;s progress bars is that here you have the option to set &lt;strong&gt;where&lt;/strong&gt; the progress gets written to, either automatically using the built-in &lt;code&gt;make_kpb_output_decisions()&lt;/code&gt;, or directly. Also, I have provided the &lt;code&gt;update_progress&lt;/code&gt; function to actually do the updating or finalizing properly.&lt;/p&gt;
&lt;p&gt;There are also package specific options to control &lt;strong&gt;how&lt;/strong&gt; the decisions are made.&lt;/p&gt;
&lt;p&gt;See the &lt;a href=&#34;https://rmflight.github.io/knitrProgressBar/&#34;&gt;main&lt;/a&gt; documentation, as well as the &lt;a href=&#34;https://rmflight.github.io/knitrProgressBar/articles/example_progress_bars.html&#34;&gt;included vignette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multi-Processing&lt;/h2&gt;
&lt;p&gt;As of V1.1.0 (should be on CRAN soon), the package also supports indicating progress on multi-processed jobs. See the included &lt;a href=&#34;https://rmflight.github.io/knitrProgressBar/articles/multiprocessing.html&#34;&gt;vignette&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;By the way, I know this method is not ideal, but I could not get the combination of &lt;code&gt;later&lt;/code&gt; and &lt;code&gt;processx&lt;/code&gt; to work in my case. If anyone is willing to help out, that would be great.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Licensing R Packages that Include Others Code</title>
      <link>/post/licensing-r-packages-that-include-others-code/</link>
      <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/licensing-r-packages-that-include-others-code/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you include others code in your own R package, list them as contributors with comments about what they contributed, and add a license statement in the file that includes their code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;I recently created the &lt;a href=&#34;https://CRAN.R-project.org/package=knitrProgressBar&#34;&gt;&lt;code&gt;knitrProgressBar&lt;/code&gt;&lt;/a&gt; package. It is a really simple package, that takes the &lt;code&gt;dplyr&lt;/code&gt; progress bars and makes it possible for them to write progress to a supplied file connection. The &lt;code&gt;dplyr&lt;/code&gt; package itself is licensed under MIT, so I felt fine taking the code directly from &lt;code&gt;dplyr&lt;/code&gt; itself. In addition, I didn’t want my package to depend on &lt;code&gt;dplyr&lt;/code&gt;, so I wanted that code self-contained in my own package, and I wanted to be able to modify underlying mechanics that might have been more complicated if I had just made a new class of progress bar that inherited from &lt;code&gt;dplyr&lt;/code&gt;’s.&lt;/p&gt;
&lt;p&gt;I also wanted to be able to release my code on CRAN, not just on GitHub. I knew to accomplish that I would have to have all the license stuff correct. However, I had not seen any guide on how to &lt;strong&gt;license&lt;/strong&gt; a package and give proper attribution in the &lt;code&gt;Authors@R&lt;/code&gt; field.&lt;/p&gt;
&lt;p&gt;Note that I did ask this question on &lt;a href=&#34;https://stackoverflow.com/questions/48525023/properly-license-r-package-that-includes-other-mit-code&#34;&gt;StackOverflow&lt;/a&gt; and &lt;a href=&#34;https://discuss.ropensci.org/t/licensing-a-new-package-that-uses-code-from-another-source/1046&#34;&gt;ROpenSci&lt;/a&gt; forums as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;information-from-cran&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Information from CRAN&lt;/h2&gt;
&lt;p&gt;I should note here, that the CRAN author guidelines do provide a &lt;strong&gt;small&lt;/strong&gt; hint in this regard in the &lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-exts.html&#34;&gt;Writing R Extensions&lt;/a&gt; guide:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that all significant contributors must be included: if you wrote an R wrapper for the work of others included in the src directory, you are not the sole (and maybe not even the main) author.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, there is not any guidance provided on &lt;strong&gt;how&lt;/strong&gt; these should ideally be listed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-answer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Full Answer&lt;/h2&gt;
&lt;p&gt;I am the main author and maintainer of the new package, that is easy. The original code is MIT licensed, authored by several persons, and has copyright held by RStudio.&lt;/p&gt;
&lt;p&gt;My solution then was to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rmflight/knitrProgressBar/blob/master/R/progress.R#L6&#34;&gt;add the MIT license&lt;/a&gt; from &lt;code&gt;dplyr&lt;/code&gt; to the file that has the progress bar code&lt;/li&gt;
&lt;li&gt;add &lt;a href=&#34;https://github.com/rmflight/knitrProgressBar/blob/master/DESCRIPTION#L5&#34;&gt;all the authors&lt;/a&gt; of &lt;code&gt;dplyr&lt;/code&gt; as &lt;strong&gt;contributors&lt;/strong&gt; to my package, with a comment as to &lt;strong&gt;why&lt;/strong&gt; they are listed&lt;/li&gt;
&lt;li&gt;add RStudio as a &lt;strong&gt;copyright holder&lt;/strong&gt; to my package, with a comment that this only applies to the one file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the &lt;code&gt;Authors@R&lt;/code&gt; line in my &lt;code&gt;DESCRIPTION&lt;/code&gt; ended up being:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Authors@R: c(person(given = c(&amp;quot;Robert&amp;quot;, &amp;quot;M&amp;quot;), family = &amp;quot;Flight&amp;quot;, email = &amp;quot;rflight79@gmail.com&amp;quot;, role = c(&amp;quot;aut&amp;quot;, &amp;quot;cre&amp;quot;)),
            person(&amp;quot;Hadley&amp;quot;, &amp;quot;Wickham&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;Romain&amp;quot;, &amp;quot;Francois&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;Lionel&amp;quot;, &amp;quot;Henry&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;Kirill&amp;quot;, &amp;quot;Müller&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;RStudio&amp;quot;, role = &amp;quot;cph&amp;quot;, comment = &amp;quot;Copyright holder of included dplyr fragments&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>docopt &amp; Numeric Options</title>
      <link>/post/docopt-numeric-options/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/docopt-numeric-options/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you use the &lt;code&gt;docopt&lt;/code&gt; package to create command line &lt;code&gt;R&lt;/code&gt; executables that take
options, there is something to know about numeric command line options: they should
have &lt;code&gt;as.double&lt;/code&gt; before using them in your script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Lets set up a new &lt;code&gt;docopt&lt;/code&gt; string, that includes both string and
numeric arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;quot;
Usage:
  test_numeric.R [--string=&amp;lt;string_value&amp;gt;] [--numeric=&amp;lt;numeric_value&amp;gt;]
  test_numeric.R (-h | --help)
  test_numeric.R

Description: Testing how values are passed using docopt.

Options:
  --string=&amp;lt;string_value&amp;gt;  A string value [default: Hi!]
  --numeric=&amp;lt;numeric_value&amp;gt;   A numeric value [default: 10]

&amp;quot; -&amp;gt; doc&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(methods)
library(docopt)

script_options &amp;lt;- docopt(doc)

script_options&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 8
##  $ --string : chr &amp;quot;Hi!&amp;quot;
##  $ --numeric: chr &amp;quot;10&amp;quot;
##  $ -h       : logi FALSE
##  $ --help   : logi FALSE
##  $ string   : chr &amp;quot;Hi!&amp;quot;
##  $ numeric  : chr &amp;quot;10&amp;quot;
##  $ h        : logi FALSE
##  $ help     : logi FALSE
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very easy to see here, that the &lt;code&gt;numeric&lt;/code&gt; argument is indeed a string, and
if you want to use it as numeric, it should first be converted using &lt;code&gt;as.double&lt;/code&gt;,
&lt;code&gt;as.integer&lt;/code&gt;, or even &lt;code&gt;as.numeric&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cant-you-easily-tell-its-character&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can’t You Easily Tell It’s Character?&lt;/h2&gt;
&lt;p&gt;I just bring this up because I recently used &lt;code&gt;docopt&lt;/code&gt; to provide interfaces to
three executables scripts, and I spent a lot of time &lt;code&gt;printing&lt;/code&gt; the &lt;code&gt;doc&lt;/code&gt; strings,
and I somehow never noticed that the numeric values were actually character and
needed to be converted to a numeric first. Hopefully this will save someone else
some time in that regard.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Custom Deployment Script</title>
      <link>/post/custom-deployment-script/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/custom-deployment-script/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Use a short bash script to do deployment from your own computer directly to your &lt;code&gt;*.github.io&lt;/code&gt; domain.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;So Yihui recommends using Netlify, or even Travis-CI in the Blogdown book. I wasn’t willing to setup a custom domain yet, and some of my posts involve a lot of personally created packages, etc, that I don’t want to debug installation on Travis. So, I wanted a simple script I could call on my laptop that would copy the &lt;code&gt;/public&lt;/code&gt; directory to the repo for my &lt;code&gt;github.io&lt;/code&gt; site, and then push the changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Script&lt;/h2&gt;
&lt;p&gt;Here is the simple script I ended up using:&lt;/p&gt;
&lt;pre class=&#34;sh&#34;&gt;&lt;code&gt;#!/bin/bash
org_dir=`pwd`
cd path/to/github.io/repo/
#rm -rf *
cp -Rfu path/to/blogdown/public/* .

git add *
commit_time=`date`
git commit -m &amp;quot;update at $commit_time&amp;quot;
git push origin master

cd $org_dir&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It changes directories, because to push from a &lt;code&gt;git&lt;/code&gt; repo I’m pretty sure you need to be in the directory, so it also makes sure to go back there at the end. It then copies the contents of &lt;code&gt;/public&lt;/code&gt; to the repo, &lt;code&gt;add&lt;/code&gt;s all the files, and then uses the current time-stamp as the commit message, and finally pushes all the updates.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Differences in Posted Date vs sessionInfo()</title>
      <link>/post/differences-in-posted-date-vs-sessioninfo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/differences-in-posted-date-vs-sessioninfo/</guid>
      <description>&lt;p&gt;If you are a newcomer to my weblog, you may notice that some posts that are &lt;code&gt;R&lt;/code&gt; tutorials generally include the output of &lt;code&gt;Sys.time()&lt;/code&gt; at the end. If you look closeley at that time and the &lt;strong&gt;Posted on&lt;/strong&gt; date, you may notice that some posts show disagreement between them. This is because I decided to move &lt;em&gt;all&lt;/em&gt; of my old blog posts from &lt;em&gt;blogspot&lt;/em&gt; to here, and keep the original posted dates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linking to Manually Inserted Images in Blogdown / Hugo</title>
      <link>/post/linking-to-manually-inserted-images-in-hugo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linking-to-manually-inserted-images-in-hugo/</guid>
      <description>&lt;div id=&#34;manual-linking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manual Linking?&lt;/h2&gt;
&lt;p&gt;Using &lt;code&gt;blogdown&lt;/code&gt; for generating websites and blog-posts from &lt;code&gt;Rmarkdown&lt;/code&gt; files with lots of inserted code and figures seems pretty awesome, but sometimes you want to include a figure manually, either because you want to generate something manually and convert it (say for going from SVG of lots of points to hi-res PNG), or because it is a figure from something else (&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;like this figure from wikipedia&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-to&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where to??&lt;/h2&gt;
&lt;p&gt;To do this, you want the text of your &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag to your image to be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src = &amp;quot;/img/image_file.png&amp;quot;&amp;gt;&amp;lt;/img&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then put the image itself in the directory &lt;code&gt;/static/img/image_file.png&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/Standard_deviation_diagram.svg&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By M. W. Toews, &lt;a href=&#34;http://creativecommons.org/licenses/by/2.5&#34;&gt;CC BY 2.5&lt;/a&gt;, via Wikimedia Commons, &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This information is also mentioned in &lt;a href=&#34;https://bookdown.org/yihui/blogdown/static-files.html&#34;&gt;section 2.7 of the Blogdown book&lt;/a&gt;. Obviously I need to do more reading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I was Part of the Problem</title>
      <link>/post/i-was-part-of-the-problem/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/i-was-part-of-the-problem/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;With the recent charges of sexual harassment against some high-profile individuals, and so many women coming forward with #metoo (and the understanding that this is really something almost &lt;em&gt;all&lt;/em&gt; women have faced), I realized that my younger self was #partoftheproblem. I think many other men are part of the problem, &lt;strong&gt;even though they might not think so&lt;/strong&gt;. I didn’t think I was part of the problem either. I hope that other men might read this and critically evaluate if they are #partoftheproblem. I also hope and pray that my own sons will do better at this if I teach them right.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-could-i-be&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Could I Be?&lt;/h2&gt;
&lt;p&gt;Let me be up front. I have never &lt;strong&gt;sexually assaulted&lt;/strong&gt; anyone, let alone considered such a thing. But that’s not really the problem, because the way I acted towards women, I think they may have been scared that I might, as I have put tons of &lt;strong&gt;unwanted attention&lt;/strong&gt; on several women over the years, starting with when I was 12 years old, in the sixth grade.&lt;/p&gt;
&lt;p&gt;I also want to be clear, I was a horrible guy friend to women (even if they didn’t think so). If I knew a girl had a boyfriend, well then, I would &lt;strong&gt;not&lt;/strong&gt; even consider trying to hit on or express interest in that girl, and I was “friends” with plenty of women over the course of my school years who had boyfriends. But in &lt;strong&gt;most&lt;/strong&gt; cases I secretly hoped they might dump their boyfriends and go out with me instead. Also, if I knew they didn’t have a boyfriend, and I found them remotely attractive, then I would do all I could to try to become friends with them in the &lt;strong&gt;hope to eventually become their boyfriend&lt;/strong&gt;. So, my sole reason for being friends with women, really, was to eventually become romantically involved. That was my primary motivation. Looking back on it now, it makes me sick.&lt;/p&gt;
&lt;p&gt;I’ve never had a woman tell me she was assaulted by anyone either, but given my past behavior, even if someone I knew had, I don’t think my actions made me someone that a woman would trust to tell.&lt;/p&gt;
&lt;p&gt;Let me give you some examples of my behavior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grade-school&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grade School&lt;/h2&gt;
&lt;p&gt;In 6th grade, I decided that I wanted a girlfriend, and I picked out one girl in my class who I wanted to be my girlfriend. I am very sure I never asked her out, to be my girlfriend, but I made sure to spend tons of time with her, and if I recall correctly, she eventually got the gist of my interest, and told me &lt;strong&gt;very clearly she wasn’t interested&lt;/strong&gt;. But &lt;strong&gt;her telling me no did not stop my unwanted advances&lt;/strong&gt; or attention. I am sure that I made her very uncomfortable the rest of that grade.&lt;/p&gt;
&lt;p&gt;In middle school (7-9 at the time), I pretty much continued this process unabated. I would latch onto a woman that I found attractive, and make her the target of my affections, and pour out my unwanted attention upon her, &lt;strong&gt;not taking no for an answer&lt;/strong&gt;. I only stopped after long periods of continued rejection, or when that person acquired a significant other. Although not an excuse for my actions, my tactics and hopes were largely fueled by rampaging hormones, way too many romantic comedies where the nice guy always got the girl by virtue of sheer persistence (this was the 90’s), and nascent exposure to pornography.&lt;/p&gt;
&lt;p&gt;I would find out girls numbers and call them without being asked. I would know where these girls were at all times through the day, even during lunch and between classes. I would find any excuse to be near them. Every sock-hop (weekly lunch time dance on gym floor) I would ask these girls to dance with me. I would give them valentines cards, Christmas cards, etc, in &lt;strong&gt;the hopes that they would realize what a great guy I was and go out with me.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Just so we are clear, none of this got me any dates in grade school.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;undergraduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Undergraduate&lt;/h2&gt;
&lt;p&gt;Now I’ve graduated high-school, I’m heading off to a local university, with lots of girls. I made lots of friends with girls who had boyfriends, in fact I think my circle of friends had way more girls in it than guys. But, I was always finding one girl who I wanted to date, and would make sure to spend extra time around them, helping them whenever possible, etc, and dropping subtle and not so subtle hints that I wanted to be their boyfriend. And there was always the hope that someone would break-up with their current boyfriend and find me, the faithful friend, waiting to comfort them.&lt;/p&gt;
&lt;p&gt;Over the course of this time, I had three women agree to be my date. Two of those did not result in an actual date, because I started acting like a stalker after they said yes, and they wisely stayed away. In the third case, we went out twice, but me calling at random hours, and showing up at her house un-announced because I thought she was really sad freaked her out, and she stopped talking to my creepy, stalkerish, clingy self.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-graduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-Graduate&lt;/h2&gt;
&lt;p&gt;Somehow, it seems, by the time I got to my PhD, I had &lt;strong&gt;mostly&lt;/strong&gt; given up on finding a girlfriend, settling down and getting married (really, that was my goal). I say mostly. I don’t know if I hadn’t met my now spouse in the first couple of months of my PhD that I would not have continued making unwanted advances on the women in my PhD program. (By the way, I met my spouse outside of work, at a Church actually, and was introduced by a mutual friend. In the 13 years I’ve known her, there are only a handful of days we haven’t talked to each other since we went on our first date).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-real-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Real Problem&lt;/h2&gt;
&lt;p&gt;And this is the &lt;strong&gt;real&lt;/strong&gt; problem. Too many men, my past self included, think women owe them something for being their friend, for being a &lt;strong&gt;nice guy&lt;/strong&gt;. For giving them any kind of attention, or any kind of help. Too many men believe these things, and then use their power and prestige, to demand things of women. Guys, &lt;strong&gt;women don’t owe you anything.&lt;/strong&gt; They definitely don’t owe you sex or reciprocated romantic interest because of something you did for them. They are another person worthy of respect, simply because they are a person.&lt;/p&gt;
&lt;p&gt;In addition, real life is not a romantic comedy. Non-romantic friendships are a good thing, because we need other peoples perspectives in our lives. So, if a woman tells you &lt;strong&gt;no, she doesn’t want to date you&lt;/strong&gt;, accept it, and move on. Don’t make it awkward, especially if you are in the same work environment. &lt;strong&gt;Don’t assume that a woman is romantically interested just because she is friendly.&lt;/strong&gt; I know, radical thought. Maybe try being friends, colleagues, whatever with no romantic intentions, and no expectations of them either. Don’t be #partoftheproblem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solutions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solutions&lt;/h2&gt;
&lt;p&gt;Teach your children that they can be friends with people of the opposite sex without being romantically involved, especially as they hit puberty. Teach them that &lt;strong&gt;no means no&lt;/strong&gt;, not &lt;strong&gt;no means maybe in 3 weeks&lt;/strong&gt;, or &lt;strong&gt;no means maybe if I try hard enough&lt;/strong&gt;. And if you see other men engaging in putting unwanted attention on women, call them out on it, &lt;strong&gt;whatever form it may take&lt;/strong&gt;. I wish someone had said something to me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;I realize that our general culture is really #partoftheproblem, when we have highly sexualized advertising (especially of women to men), and the idea that &lt;strong&gt;boys will be boys&lt;/strong&gt;, tell jokes about sexual assault, and propagate the idea that &lt;strong&gt;women want it&lt;/strong&gt;, based on how they act or dress. Those are all wrong too, and our culture needs to change.&lt;/p&gt;
&lt;p&gt;I also realize that some of what I describe about myself is rather mild in comparison to much of what gets reported, but that’s not the point. It is still unwanted attention, and I didn’t know how to take no for an answer. Those women &lt;strong&gt;didn’t want my attention&lt;/strong&gt;, and I couldn’t accept that. If I had a different temperament, I don’t know what I would have done. Enough people realized it that some friends in Undergrad stopped being around me, but no one ever told me that what I was doing was wrong, and my parents weren’t involved enough in my so-called love life to know what was going on. If they had, I think they would have told me to knock it off and stop being an idiot.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Criticizing a Publication, and Lying About It</title>
      <link>/post/criticizing-a-publication-and-lying-about-it/</link>
      <pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/criticizing-a-publication-and-lying-about-it/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Other researchers &lt;a href=&#34;https://dx.doi/org/10.1002/prot.25024&#34;&gt;directly criticized&lt;/a&gt; a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;recent publication of ours&lt;/a&gt; in a “research article”. Although they raised valid points, they &lt;strong&gt;outright lied&lt;/strong&gt; about the availability of our results. In addition, they did not provide access to their own results. We have published &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;new work&lt;/a&gt; supporting our original results, and a direct rebuttal of their critique in a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;perspective article&lt;/a&gt;. The peer reviewers of their “research article” must have been asleep at the wheel to allow the major point, lack of access to our results, to stand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;original-publication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Original Publication&lt;/h2&gt;
&lt;p&gt;Back in the summer of 2015, I was second author on a publication (&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.24834/full&#34;&gt;Yao et al., 2015&lt;/a&gt;, hereafter YS2015) describing an automated method to characterize zinc ion coordination geometries (CGs). Applying our automated method to all zinc sites in the worldwide Protein Data Bank (wwPDB), we found &lt;em&gt;abberrant&lt;/em&gt; zinc CGs that don’t fit the canonical CGs. We were pretty sure that these aberrant CGs are real, and they have always existed, but had not been previously characterized because methods assumed that only the &lt;em&gt;canonical&lt;/em&gt; geometries should be observed in biological systems, and were excluding the &lt;em&gt;abberrant&lt;/em&gt; ones because they didn’t have good methods to detect and characterize them.&lt;/p&gt;
&lt;p&gt;Also of note, the proteins with aberrant zinc geometries showed enrichment for different types of enzyme classifications than those with canonical zinc geometries.&lt;/p&gt;
&lt;p&gt;For this publication, we made &lt;strong&gt;all&lt;/strong&gt; of our code and results available in a tarball that could be downloaded from our &lt;a href=&#34;http://bioinformatics.cesb.uky.edu/bin/view/Main/SoftwareDevelopment#Metal_ion_coordination_analysis_software&#34;&gt;website&lt;/a&gt;. This data went up while the paper was in review, on Dec 7, 2015 (with a correction on Dec 15). Recently, we’ve also put a copy of the tarball on &lt;a href=&#34;https://figshare.com/articles/Zn_metalloprotein_paper/4229333&#34;&gt;FigShare&lt;/a&gt;. Every draft of the publication, from initial submission through to accepted publication, included the link to the tarball on the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Critique&lt;/h2&gt;
&lt;p&gt;Less than a year later, &lt;a href=&#34;http://sci-hub.cc/doi/10.1002/prot.25024&#34;&gt;Raczynska, Wlodawer, and Jaskolski&lt;/a&gt; (RJW2016) published a &lt;em&gt;critique&lt;/em&gt; of YS2015 as a “research article”. In their publication, they questioned the existence of the &lt;em&gt;abberrant&lt;/em&gt; sites completely, based on the examination and remodeling of four aberrant structures highlighted in YS2015. To be fair, they did have some valid criticisms of the methods, and Sen Yao did a lot of work in our latest paper to address them.&lt;/p&gt;
&lt;p&gt;As part of the critique, however, they claimed that they could only evaluate the four structures listed in two figures &lt;strong&gt;because we didn’t provide all of our results&lt;/strong&gt;. However, we had previously made our full results available as a tarball from our website. As you can see in the below figure, &lt;strong&gt;all&lt;/strong&gt; of the results were really available in that tarball.&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/ys2017_figure1.png&#34;, width = &#34;600&#34;&gt;&lt;/p&gt;
&lt;p&gt;In addition, although RWJ2016 went to all the trouble to actually remodel those four structures by going back to the original X-ray density, they &lt;strong&gt;didn’t make any of their models available&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, no one from RWJ2016 ever contacted our research group to see if the results might be available.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;response&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Response&lt;/h2&gt;
&lt;div id=&#34;follow-up-paper-on-5-metals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Follow-Up Paper on 5 Metals&lt;/h3&gt;
&lt;p&gt;By the time the critiques appeared in RJW2016, Sen was already hard at work showing that the previously developed methods could be modified and then applied to other metal ion CGs, and that they also contained aberrant CGs (see &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;YS2017-1&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique-direct-response&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Critique Direct Response&lt;/h3&gt;
&lt;p&gt;In addition to YS2017-1, we felt that the critique deserved separate response (&lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;YS2017-2&lt;/a&gt;). To that end, we began drafting a response, wherein we pointed out some of the problems with RJW2016, the first being that we did indeed provide the &lt;strong&gt;full&lt;/strong&gt; set of results from YS2015, and therefore it was possible to evaluate our full work. We also addressed each of their other criticisms of YS2015, in many cases going beyond the original criticism, and explaining how it was being addressed in YS2017-1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-results-and-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open Results and Code&lt;/h3&gt;
&lt;p&gt;A major part of the conclusions in YS2017-2 was also devoted to the idea that code and results in science need to be shared, highlighting the fact that RJW2016 &lt;strong&gt;did not share their models&lt;/strong&gt; they used to try and discredit our work, lied about the fact that we did not share our own results, and pointing out some other projects in this research area that have shared well and others that have shared badly, and that the previous attitude of competition among research groups does not move science forward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;peer-review&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer Review&lt;/h2&gt;
&lt;p&gt;Let’s just say that the &lt;em&gt;peer-review&lt;/em&gt; of both of the papers was &lt;strong&gt;interesting&lt;/strong&gt;. Both manuscripts had the same set of reviewers. YS2017-1, the five metal paper, had some rather rigorous peer review, and was definitely improved by the reviewer’s comments. YS2017-2, our perspective, in contrast, was attacked by one peer reviewer right from submission, and was questioned almost continually as to whether it should even be published. I am thankful that one reviewer saw the need for it to be published, and that the Editor ultimately decided that it should be published, and that we were able to rebut each of the reviewer’s criticisms.&lt;/p&gt;
&lt;p&gt;Finally, I really don’t know what happened in the peer review of RWJ2016. The first major claim was that our data wasn’t available, it should have taken a reviewer 10 minutes to verify and debunk that claim. I would have expected a much different critique from the authors had they actually examined our full data set. But, because of traditional closed peer review, that record is closed to us.&lt;/p&gt;
&lt;p&gt;Overall though, I’m very happy both of our publications are now out, and we can move on to new stages of our analyses. Looking forward to continuing to work with my co-authors to move the work forward.&lt;/p&gt;
&lt;div id=&#34;papers-discussed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Papers Discussed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Original Zinc CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;Yao et al 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Critique of Zinc CGs: Raczynska, Wlodawer &amp;amp; Jaskolski 2016, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25024&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://sci-hub.cc/10.1002/prot.25024&#34;&gt;sci-hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5 Metal CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;Yao et al 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Response to critique: Yao et al 2017, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;copy on figshare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Authentication of Key Resources for Data Analysis</title>
      <link>/post/authentication-of-key-resources-for-data-analysis/</link>
      <pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/authentication-of-key-resources-for-data-analysis/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;NIH recently introduced a reproducibility initiative, extending to including the
“Authentication of Key Resources” page in grant applications from Jan 25, 2016.
Seems to be intended for grants involving biological reagents, but we included
it in our recent R03 grant developing new data analysis methods. We believe that
this type of thing should become common for all grants, not just those that use
biological/chemical resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nih-and-reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NIH and Reproducibility&lt;/h2&gt;
&lt;p&gt;There has been a lot of things published recently about the &lt;em&gt;reproducibility
crisis&lt;/em&gt; in science (see refs). The federal funding agencies are starting to
respond to this, and beginning with grants submitted after January 25, 2016,
grants are &lt;a href=&#34;http://grants.nih.gov/reproducibility/index.htm&#34;&gt;supposed to address the
reproducibility&lt;/a&gt; of the work
proposed, including the presence of various confounding factors (i.e. sex of
animals, the source of cell lines, etc). In addition to this, there is a new
document that can be added to grants, the &lt;a href=&#34;http://nexus.od.nih.gov/all/2016/01/29/authentication-of-key-biological-andor-chemical-resources-in-nih-grant-applications/&#34;&gt;&lt;strong&gt;Authentication
Plan&lt;/strong&gt;&lt;/a&gt;,
which as far as I can tell is intended specifically for:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;key biological and/or chemical resources used in the proposed studies&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, this makes sense. Some sources of irreproducibility include, but are not
limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unvalidated antibodies&lt;/li&gt;
&lt;li&gt;cell lines that are not what was thought&lt;/li&gt;
&lt;li&gt;impure chemicals&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think this is a &lt;strong&gt;good thing&lt;/strong&gt;. What does it have to do with data analysis?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-code-authentication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data / Code Authentication&lt;/h2&gt;
&lt;p&gt;When we were submitting a recent R03 proposal for developing novel data analysis
methods and statistical tools, the grant management office asked us about the
&lt;strong&gt;Authentication of Key Resources&lt;/strong&gt; attachment, which we completely missed. Upon
review of the guidelines, we initially determined that this document did not
apply. However, we decided to go ahead and take some initiative.&lt;/p&gt;
&lt;div id=&#34;data-authentication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Authentication?&lt;/h3&gt;
&lt;p&gt;When dealing with multiple samples from high-throughput samples, there are
frequently a few easy ways to examine the data quality, and although it can be
hard to verify that the data &lt;strong&gt;is what the supplier says it is&lt;/strong&gt;, which would be
true &lt;strong&gt;authentication&lt;/strong&gt;, there are some ways to verify that the various samples
in the dataset are at least self-consistent within each sample class (normal and
disease, condition 1 and condition 2).&lt;/p&gt;
&lt;p&gt;My go-to for data self-consistency are principal components analysis (PCA) and
correlation heat-maps. Correlation heat-maps involve calculating
all of the pairwise sample to sample correlations using all of the non-zero
sample features (those that are non-zero in the two pairs being compared). These
heatmaps, combined with the sample class information, and clustering within each
class, are a nice visual way to eyeball samples that have potential problems. A
simple example for RNA-seq transcriptomics was shown in &lt;a href=&#34;https://dx.doi.org/10.1093/bioinformatics/btv425&#34;&gt;Gierliński et al.,
Statistical models for RNA-seq data derived from a two-condition 48-replicate
experiment Bioinformatics (2015) 31 (22): 3625-3630, Figure
1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/yeast_48rep_cor_heatmap.jpg&#34; alt=&#34;Gierlinkski et al heatmap, Figure 1&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Gierlinkski et al heatmap, Figure 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The other measures they used in this paper are also very nice, in plotting the
median correlation of a sample against all other samples, and the fraction of
outlier features in a given sample (see figure 2 of Gierlinkski et al). The
final measure they propose is not generally applicable to all -omics data
however.&lt;/p&gt;
&lt;p&gt;PCA on the data, followed by visualizing the scores on the first few principal
components, and colored by sample class (or experimental condition) is similar
in spirit to the correlation heat-map. In fact, it is very similar, because PCA
is actually decomposing on the covariance of the samples, which is very related
to the correlations (an early algorithm actually used the correlation matrix).&lt;/p&gt;
&lt;p&gt;Both of these methods can highlight possible problems with individual samples,
and make sure that the set of data going into the analysis is at least
self-consistent, which is important when doing classification or differential
abundance analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code-authentication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code Authentication&lt;/h3&gt;
&lt;p&gt;The other thing we highlighted in the document was &lt;strong&gt;code&lt;/strong&gt; authentication. In
this case, we highlighted the use of unit-testing in the R packages that we are
planning to develop. Even though this is software coming out of a research lab,
we need to have confidence that the functions we write return the correct values
given various inputs. In addition, code testing coverage helps evaluate that we
are testing &lt;em&gt;all&lt;/em&gt; of the functionality by checking that all of the lines in our
code are run by the tests. Finally, we are also planning to write tests for core
functions provided by others (i.e. functions in other R packages), in that they
work as we expect, by returning correct values given specific inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Going forward, I think it would be a good thing if people writing research grants
for data analysis methods would discuss how they are going to look at the data
to assess it’s quality, and how they are going to do unit testing, and will have
to start saying that they are going to do unit testing of their analysis method.&lt;/p&gt;
&lt;p&gt;I’d be interested in others’ thoughts on this as well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
