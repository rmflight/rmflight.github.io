<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Deciphering Life: One Bit at a Time</title>
    <link>/post/</link>
    <description>Recent content in Posts on Deciphering Life: One Bit at a Time</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Robert M Flight</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Comments enabled via utterances</title>
      <link>/post/comments-enabled-via-utterances/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/comments-enabled-via-utterances/</guid>
      <description>TL;DR Utterances is a lightweight commenting platform built on GitHub issues. So you have to have a GitHub account, but I expect most people who comment on this blog already have one.
 Why Utterances When I switched to blogdown, I lost my disqus comments. I had considered migrating them over, but never got around to it. I also thought that there had to be a way to link GitHub issues to blog posts, but didn’t investigate it much.</description>
    </item>
    
    <item>
      <title>Comparisons using for loops vs split</title>
      <link>/post/for-loops-vs-split/</link>
      <pubDate>Wed, 13 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/for-loops-vs-split/</guid>
      <description>TL;DR Sometimes for loops are useful, and sometimes they shouldn’t really be used, because they don’t really help you understand your data, and even if you try, they might still be slow(er) than other ways of doing things.
 Comparing Groups I have some code where I am trying to determine duplicates of a group of things. This data looks something like this:
create_random_sets = function(n_sets = 1000){ set.seed(1234) sets = purrr::map(seq(5, n_sets), ~ sample(seq(1, .</description>
    </item>
    
    <item>
      <title>Nicer PNG Graphics</title>
      <link>/post/nicer-png-graphics/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/nicer-png-graphics/</guid>
      <description>TL;DR If you are getting crappy looking png images from rmarkdown html or word documents, try using type=&#39;cairo&#39; or dev=&#39;CairoPNG&#39; in your chunk options.
 PNG Graphics?? So, I write a lot of reports using rmarkdown and knitr, and have been using knitr for quite a while. My job involves doing analyses for collaborators and communicating results. Most of the time, I will generate a pdf report, and I get beautiful graphics, thanks to the eps graphics device.</description>
    </item>
    
    <item>
      <title>Don&#39;t do PCA After Statistical Testing!</title>
      <link>/post/don-t-do-pca-after-statistical-testing/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/don-t-do-pca-after-statistical-testing/</guid>
      <description>TL;DR If you do a statistical test before a dimensional reduction method like PCA, the highest source of variance is likely to be whatever you tested statistically.
 Wait, Why?? Let me describe the situation. You’ve done an -omics level analysis on your system of interest. You run a t-test (or ANOVA, etc) on each of the features in your data (gene, protein, metabolite, etc). Filter down to those things that were statistically significant, and then finally, you decide to look at the data using a dimensionality reduction method such as principal components analysis (PCA) so you can see what is going on.</description>
    </item>
    
    <item>
      <title>Finding Modes Using Kernel Density Estimates</title>
      <link>/post/finding-modes-using-kernel-density-estimates/</link>
      <pubDate>Thu, 19 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/finding-modes-using-kernel-density-estimates/</guid>
      <description>TL; DR If you have a unimodal distribution of values, you can use R’s density or Scipy’s gaussian_kde to create density estimates of the data, and then take the maxima of the density estimate to get the mode. See below for actual examples in R and Python.
 Mode in R First, lets do this in R. Need some values to work with.
library(ggplot2) set.seed(1234) n_point &amp;lt;- 1000 data_df &amp;lt;- data.</description>
    </item>
    
    <item>
      <title>Split - Unsplit Anti-Pattern</title>
      <link>/post/split-unsplit-anti-pattern/</link>
      <pubDate>Tue, 17 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/split-unsplit-anti-pattern/</guid>
      <description>TL;DR If you notice yourself using split -&amp;gt; unsplit / rbind on two object to match items up, maybe you should be using dplyr::join_ instead. Read below for concrete examples.
 Motivation I have had a lot of calculations lately that involve some sort of normalization or scaling a group of related values, each group by a different factor.
Lets setup an example where we will have 1e5 values in 10 groups, each group of values being normalized by their own value.</description>
    </item>
    
    <item>
      <title>Using IRanges for Non-Integer Overlaps</title>
      <link>/post/iranges-for-non-integer-overlaps/</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/iranges-for-non-integer-overlaps/</guid>
      <description>TL;DR The IRanges package implements interval algebra, and is very fast for finding overlaps of two ranges. If you have non-integer data, multiply values by a large constant factor and round them. The constant depends on how much accuracy you need.
 IRanges?? IRanges is a bioconductor package for interval algebra of integer ranges. It is used extensively in the GenomicRanges package for finding overlaps between various genomic features. For genomic features, integers make sense, because one cannot have fractional base locations.</description>
    </item>
    
    <item>
      <title>Turn Robert&#39;s Beard Purple!</title>
      <link>/post/turn-roberts-beard-purple/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/turn-roberts-beard-purple/</guid>
      <description>TL;DR If I raise $100 by August 25ths The Walk to End Alzheimer’s, I will have my beard dyed purple in support of Alzheimer’s awareness.
If you are in another country, donate to your local Alzheimer’s charity and email me with the subject walk so I count it towards my total.
Links:
 My donation page (Charity report on Alzheimer’s Association)
 Facebook Fundraising Page (if you want to share it!</description>
    </item>
    
    <item>
      <title>knitrProgressBar Package</title>
      <link>/post/knitrprogressbar/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/knitrprogressbar/</guid>
      <description>TL;DR If you like dplyr progress bars, and wished you could use them everywhere, including from within Rmd documents, non-interactive shells, etc, then you should check out knitrProgressBar (cran github).
 Why Yet Another Progress Bar?? I didn’t set out to create another progress bar package. But I really liked dplyrs style of progress bar, and how they worked under the hood (thanks to the examples from Bob Rudis).</description>
    </item>
    
    <item>
      <title>Licensing R Packages that Include Others Code</title>
      <link>/post/licensing-r-packages-that-include-others-code/</link>
      <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/licensing-r-packages-that-include-others-code/</guid>
      <description>TL;DR If you include others code in your own R package, list them as contributors with comments about what they contributed, and add a license statement in the file that includes their code.
 Motivation I recently created the knitrProgressBar package. It is a really simple package, that takes the dplyr progress bars and makes it possible for them to write progress to a supplied file connection. The dplyr package itself is licensed under MIT, so I felt fine taking the code directly from dplyr itself.</description>
    </item>
    
    <item>
      <title>docopt &amp; Numeric Options</title>
      <link>/post/docopt-numeric-options/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/docopt-numeric-options/</guid>
      <description>TL;DR If you use the docopt package to create command line R executables that take options, there is something to know about numeric command line options: they should have as.double before using them in your script.
 Setup Lets set up a new docopt string, that includes both string and numeric arguments.
&amp;quot; Usage: test_numeric.R [--string=&amp;lt;string_value&amp;gt;] [--numeric=&amp;lt;numeric_value&amp;gt;] test_numeric.R (-h | --help) test_numeric.R Description: Testing how values are passed using docopt.</description>
    </item>
    
    <item>
      <title>Custom Deployment Script</title>
      <link>/post/custom-deployment-script/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/custom-deployment-script/</guid>
      <description>TL;DR Use a short bash script to do deployment from your own computer directly to your *.github.io domain.
 Why? So Yihui recommends using Netlify, or even Travis-CI in the Blogdown book. I wasn’t willing to setup a custom domain yet, and some of my posts involve a lot of personally created packages, etc, that I don’t want to debug installation on Travis. So, I wanted a simple script I could call on my laptop that would copy the /public directory to the repo for my github.</description>
    </item>
    
    <item>
      <title>Differences in Posted Date vs sessionInfo()</title>
      <link>/post/differences-in-posted-date-vs-sessioninfo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/differences-in-posted-date-vs-sessioninfo/</guid>
      <description>If you are a newcomer to my weblog, you may notice that some posts that are R tutorials generally include the output of Sys.time() at the end. If you look closeley at that time and the Posted on date, you may notice that some posts show disagreement between them. This is because I decided to move all of my old blog posts from blogspot to here, and keep the original posted dates.</description>
    </item>
    
    <item>
      <title>Linking to Manually Inserted Images in Blogdown / Hugo</title>
      <link>/post/linking-to-manually-inserted-images-in-hugo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linking-to-manually-inserted-images-in-hugo/</guid>
      <description>Manual Linking? Using blogdown for generating websites and blog-posts from Rmarkdown files with lots of inserted code and figures seems pretty awesome, but sometimes you want to include a figure manually, either because you want to generate something manually and convert it (say for going from SVG of lots of points to hi-res PNG), or because it is a figure from something else (like this figure from wikipedia).
 Where to?</description>
    </item>
    
    <item>
      <title>I was Part of the Problem</title>
      <link>/post/i-was-part-of-the-problem/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/i-was-part-of-the-problem/</guid>
      <description>TL;DR With the recent charges of sexual harassment against some high-profile individuals, and so many women coming forward with #metoo (and the understanding that this is really something almost all women have faced), I realized that my younger self was #partoftheproblem. I think many other men are part of the problem, even though they might not think so. I didn’t think I was part of the problem either. I hope that other men might read this and critically evaluate if they are #partoftheproblem.</description>
    </item>
    
    <item>
      <title>Criticizing a Publication, and Lying About It</title>
      <link>/post/criticizing-a-publication-and-lying-about-it/</link>
      <pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/criticizing-a-publication-and-lying-about-it/</guid>
      <description>TL;DR Other researchers directly criticized a recent publication of ours in a “research article”. Although they raised valid points, they outright lied about the availability of our results. In addition, they did not provide access to their own results. We have published new work supporting our original results, and a direct rebuttal of their critique in a perspective article. The peer reviewers of their “research article” must have been asleep at the wheel to allow the major point, lack of access to our results, to stand.</description>
    </item>
    
    <item>
      <title>Authentication of Key Resources for Data Analysis</title>
      <link>/post/authentication-of-key-resources-for-data-analysis/</link>
      <pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/authentication-of-key-resources-for-data-analysis/</guid>
      <description>TL;DR NIH recently introduced a reproducibility initiative, extending to including the “Authentication of Key Resources” page in grant applications from Jan 25, 2016. Seems to be intended for grants involving biological reagents, but we included it in our recent R03 grant developing new data analysis methods. We believe that this type of thing should become common for all grants, not just those that use biological/chemical resources.
 NIH and Reproducibility There has been a lot of things published recently about the reproducibility crisis in science (see refs).</description>
    </item>
    
    <item>
      <title>Random Forest vs PLS on Random Data</title>
      <link>/post/random-forest-vs-pls-on-random-data/</link>
      <pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/random-forest-vs-pls-on-random-data/</guid>
      <description>TL;DR Partial least squares (PLS) discriminant-analysis (DA) can ridiculously over fit even on completely random data. The quality of the PLS-DA model can be assessed using cross-validation, but cross-validation is not typically performed in many metabolomics publications. Random forest, in contrast, because of the forest of decision tree learners, and the out-of-bag (OOB) samples used for testing each tree, automatically provides an indication of the quality of the model.</description>
    </item>
    
    <item>
      <title>Novel Zinc Coordination Geometries</title>
      <link>/post/novel-zinc-coordination-geometries/</link>
      <pubDate>Wed, 17 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/novel-zinc-coordination-geometries/</guid>
      <description>TL;DR Currently available methods to discover metal geometries make too many assumptions. We were able to discover novel zinc coordination geometries using a less-biased method that makes fewer assumptions. These novel geometries seem to also have specific functionality. This work was recently published under an #openaccess license in Proteins Journal: Yao, S., Flight, R. M., Rouchka, E. C. and Moseley, H. N. B. (2015), A less-biased analysis of metalloproteins reveals novel zinc coordination geometries.</description>
    </item>
    
    <item>
      <title>Mouse / Human Transcriptomics and Batch Effects</title>
      <link>/post/mouse-human-transcriptomics-and-batch-effects/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/mouse-human-transcriptomics-and-batch-effects/</guid>
      <description>TL;DR This 2014 PNAS paper by S. Lin et al (Lin et al., PNAS, 2014) that compares transcription of tissues between species has a flawed experimental design, where species is almost perfectly confounded with machine / lane on which the sequencing was done. Y. Golad and O. Mizrahi-Man have published a manuscript describing the confounding and the results of removing it. This was possible because the original authors supplied the information about which publically available files were used in the original analysis.</description>
    </item>
    
    <item>
      <title>First Open Post-Publication Peer Review, with Credit!</title>
      <link>/post/first-open-post-publication-peer-review-with-credit/</link>
      <pubDate>Wed, 25 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/first-open-post-publication-peer-review-with-credit/</guid>
      <description>TL;DR Reviewed Jason McDermott’s MDRPred paper on F1000Research!, where my review is posted along side the paper, with a DOI, completely in the open with my name attached. Was a pleasant experience, aided by the fact that Jason wrote a good paper.
 F1000Research! F1000Research! is a new publishing startup from F1000 that has a model of post-publication peer review, whereby upon submission the manuscript undergoes basic quality checks (no real editorial control), and then is published.</description>
    </item>
    
    <item>
      <title>Being a PhD Student and Post-Doc with Migraines</title>
      <link>/post/being-a-phd-student-and-post-doc-with-migraines/</link>
      <pubDate>Wed, 31 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/being-a-phd-student-and-post-doc-with-migraines/</guid>
      <description>A blog post on the Weecology group blog by Elita Baldridge on being a PhD student with fibromyalgia, and how they are working through that, caused me to pause and reflect on my experience as a PhD student and PostDoc with migraines. For those who haven’t read my blog, I do research in bioinformatics, specifically in transcriptomics and metabolomics. I spend almost all of my research hours in front of a computer writing code, generating plots, and trying to make sense of -omics level data.</description>
    </item>
    
    <item>
      <title>Travis-CI to GitHub Pages</title>
      <link>/post/travis-ci-to-github-pages/</link>
      <pubDate>Wed, 05 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/travis-ci-to-github-pages/</guid>
      <description>I don’t remember how I got on this, but I believe I had a recent twitter exchange with some persons (or saw it fly by) about pushing R package vignettes to the web after building and checking on travis-ci. Hadley Wickham pointed to using such a scheme to push the web version of his book after each update and the S3 deploy hooks on travis-ci. Deploying your html content to S3 is great, but given the availability of the gh-pages branch on GitHub, I thought it would be neat to work out how to deploy the html output from an R package vignette to the gh-pages branch on GitHub.</description>
    </item>
    
    <item>
      <title>My Career Goals</title>
      <link>/post/my-career-goals/</link>
      <pubDate>Fri, 08 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/my-career-goals/</guid>
      <description>TL;DR I don’t want to be a PI because I enjoy spending time with my family, and don’t think I can handle the stress of juggling multiple grants, people, and deadlines. I want to be a staff member in a group that affords relative autonomy, while providing some security. If I’m lucky enough, my current position will enable that.
 The Bad News If you keep up with the news in academia, this is a horrible time to be a postdoc (post-doctoral) or PI (principal investigator).</description>
    </item>
    
    <item>
      <title>Analyses as Packages</title>
      <link>/post/analyses-as-packages/</link>
      <pubDate>Mon, 28 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/analyses-as-packages/</guid>
      <description>TL;DR Instead of writing an analysis as a single or set of R scripts, use a package and include the analysis as a vignette of the package. Read below for the why, the how is in the next post.
 Analyses and Reports As data science or statistical researchers, we tend to do a lot of analyses, whether for our own research or as part of a collaboration, or even for supervisors depending on where we work.</description>
    </item>
    
    <item>
      <title>Creating an Analysis as a Package and Vignette</title>
      <link>/post/vignette-analysis/</link>
      <pubDate>Mon, 28 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/vignette-analysis/</guid>
      <description>Following from my last post, I am going to go step by step through the process I use to generate an analysis as a package vignette. This will be an analysis of the tweets from the 2012 and 2014 ISMB conference (thanks to Neil and Stephen for compiling the data).
I will link to individual commits so that you can see how things change as we go along.
Setup Initialization To start, we will initialize the package.</description>
    </item>
    
    <item>
      <title>Packages vs ProjectTemplate</title>
      <link>/post/packages-vs-projecttemplate/</link>
      <pubDate>Mon, 28 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/packages-vs-projecttemplate/</guid>
      <description>tl;dr Imposing a different structure than R packages for distributing R code is a bad idea, especially now that R package tools have gotten to the point where managing a package has become much easier.
 ProjectTemplate ?? My last two posts (1, 2) provided an argument and an example of why one should use R packages to contain analyses. They were partly motivated by trends I had seen in other areas, including the appearance of the package ProjectTemplate.</description>
    </item>
    
    <item>
      <title>R Job Notifications Using Twitter</title>
      <link>/post/r-job-notifications-using-twitter/</link>
      <pubDate>Mon, 30 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/r-job-notifications-using-twitter/</guid>
      <description>There has been some interesting activity about getting R to send a notification somehow when a long running job is completed. The most notable entries I have seen in this category are RPushBullet for web notifications and pingr for audio notifications.
Although RPushBullet looks really cool (and Dirk does great work), I wondered if there was a way to do this using a free service that I already had access to, namely twitter.</description>
    </item>
    
    <item>
      <title>Researcher Discoverability</title>
      <link>/post/researcher-discoverability/</link>
      <pubDate>Thu, 19 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/researcher-discoverability/</guid>
      <description>University of Kentucky (UK) recently partnered with the discovery portal KNODE, for helping others to discover potential collaborators at UK. KNODE looks like a large corporate venture, that is probably costing a large amount of capital to the university (and other places that use it). I wonder if the universities money would be better spent on encouraging submission of preprints, a Github Enterprise/Education package and teaching researchers and faculty how to use social media like twitter.</description>
    </item>
    
    <item>
      <title>Bioinformatics Presentations that Lack Results (or Biological Relevance)</title>
      <link>/post/bioinformatics-presentations-that-lack-results-or-biological-relevance/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/bioinformatics-presentations-that-lack-results-or-biological-relevance/</guid>
      <description>TL;DR In bioinformatics research we need to show validated results (if doing classification or discovery of new things), or show biological relevance. If you do neither of those things in a paper or presentation, then I’m not going to believe your method is worth anything.
 Seminar Without Results I attended a seminar yesterday (I’m not going to comment on who gave the seminar or what it was about, so please don’t ask) where the presenter had a distinct lack of any useful results.</description>
    </item>
    
    <item>
      <title>categoryCompare Paper Finally Out!</title>
      <link>/post/categorycompare-paper-finally-out/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/categorycompare-paper-finally-out/</guid>
      <description>I can finally say that the publication on my Bioconductor package categoryCompare is finally published in the Bioinformatics and Computational Biology section of Frontiers in Genetics. This has been a long time coming, and I wanted to give some background on the inspiration and development of the method and software.
TL;DR The software package has been in development in one form or another since 2010, released to Bioconductor in summer 2012, and the publication has bounced around and been revised since spring of 2013, and it is finally available to you.</description>
    </item>
    
    <item>
      <title>Self-Written Function Help</title>
      <link>/post/self-written-function-help/</link>
      <pubDate>Thu, 20 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/self-written-function-help/</guid>
      <description>I have noted at least one instance (and there are probably others) about how Python’s docStrings are so great, and wouldn’t it be nice to have a similar system in R. Especially when you can have your new function tab completion available depending on your development environment.
This is a false statement, however. If you set up your R development environment properly, you can have these features available in R.</description>
    </item>
    
    <item>
      <title>Installing MatLab vs Installing R</title>
      <link>/post/installing-matlab-vs-installing-r/</link>
      <pubDate>Mon, 10 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/installing-matlab-vs-installing-r/</guid>
      <description>I retweeted this a few days ago:
1. Open MATLAB for first time in a few years after using #rstats. 2. Site license doesn’t work right. 3. F*** MATLAB, I’ll try to do it in R
And as I have started the process of installing MatLab on my own machine because I want to translate a published MatLab package into R, I am reminded of how painful the process can be.</description>
    </item>
    
    <item>
      <title>Package Version Increment Pre- and Post-Commit Hooks</title>
      <link>/post/package-version-increment-pre-and-post-commit-hooks/</link>
      <pubDate>Fri, 07 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/package-version-increment-pre-and-post-commit-hooks/</guid>
      <description>If you just want the hook scripts, check this gist. If you want to know some of the motivation behind writing them, and about the internals, then read on.
Package Version Incrementing A good practice to get into is incrementing the minor version number (i.e. going from 0.0.1 to 0.0.2) after each git commit when developing packages (this is recommended by the Bioconductor devs as well ). This makes it very easy to know what changes are in your currently installed version, and if you remembered to actually install the most recent version for testing.</description>
    </item>
    
    <item>
      <title>Motivated Learning</title>
      <link>/post/motivated-learning/</link>
      <pubDate>Tue, 28 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/motivated-learning/</guid>
      <description>As part of the instructor training of Software-Carpentry, we were asked to write a blog post about two things:
 A story about a time you were motivated/demotivated to learn A story that will help motivate our learners drawn from personal experience  Here are mine. I thought others who read my ramblings might find them useful. Note that these are cross-posted on the Software-Carpentry teaching blog.
Being Motivated to Learn Calculus As part of my undergraduate degree, I was required to take higher level mathematics, either calculus or linear algebra.</description>
    </item>
    
    <item>
      <title>PubmedCommons API</title>
      <link>/post/pubmedcommons-api/</link>
      <pubDate>Tue, 22 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/pubmedcommons-api/</guid>
      <description>The announcements are out, Pubmed is introducing a commenting system pubmedcommons, theoretically providing a single location for true post-publication peer review. This is a really good idea, as NCBI is likely to be around for a lot longer than a given publisher, and the requirement for all NIH funded research to be deposited into Pubmed.
There are some detractors, and they may have some valid points link. However, the alternative, pubpeer, I had not heard about.</description>
    </item>
    
    <item>
      <title>Open vs Closed Analysis Languages</title>
      <link>/post/open-vs-closed-analysis-languages/</link>
      <pubDate>Mon, 21 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/open-vs-closed-analysis-languages/</guid>
      <description>TL;DR I think data scientists should choose to learn open languages such as R and python because they are open in the sense that anyone can obtain them, use them and modify them for free, and this has lead to large, robust groups of users, making it more likely that packages exist that you can use, and others can easily build on your own work.
 Why the debate? This was sparked by a comment on twitter suggesting that data scientists and analysts need to be polyglots, that they should know more than one programming language or analysis framework (the full conversation of tweets can be found here)</description>
    </item>
    
    <item>
      <title>Pre-Calculating Large Tables of Values</title>
      <link>/post/pre-calculating-large-tables-of-values/</link>
      <pubDate>Thu, 17 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/pre-calculating-large-tables-of-values/</guid>
      <description>I’m currently working on a project where we want to know, based on a euclidian distance measure, what is the probability that the value is a match to the another value. i.e. given an actual value, and a theoretical value from calculation, what is the probability that they are the same? This can be calculated using a chi-square distribution with one degree-of-freedom, easily enough by considering how much of the chi-cdf we are taking up.</description>
    </item>
    
    <item>
      <title>Portable, Peronal Packages</title>
      <link>/post/portable-peronal-packages/</link>
      <pubDate>Mon, 23 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/portable-peronal-packages/</guid>
      <description>ProgrammingR had an interesting post recently about keeping a set of R functions that are used often as a gist on Github, and sourceing that file at the beginning of R analysis scripts. There is nothing inherently wrong with this, but it does end up cluttering the user workspace, and there is no real documentation on the functions, and no good way to implement unit testing.
However, the best way to have sets of R functions is as a package, that can then be installed and loaded by anyone.</description>
    </item>
    
    <item>
      <title>K-12 Wants Scientists!!</title>
      <link>/post/k-12-wants-scientists/</link>
      <pubDate>Thu, 19 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/k-12-wants-scientists/</guid>
      <description>It seems that the PostDoc committee here at UK has an interest in providing some information on alternative careers for PostDocs outside of academia. We all know that there are not enough PI slots at universities for all of the PhDs who are going on to do PostDocs, and many will end up in alternative (I use that term loosely) careers.
Yesterday (2013-09-18), Scott Diamond gave a seminar to PostDocs in the Medical Center at UK on getting involved in teaching science at the K-12 and college level.</description>
    </item>
    
    <item>
      <title>R, RStudio, and Release and Dev Bioconductor</title>
      <link>/post/r-rstudio-and-release-and-dev-bioconductor/</link>
      <pubDate>Tue, 17 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/r-rstudio-and-release-and-dev-bioconductor/</guid>
      <description>I have one Bioconductor package that I am currently responsible for. Each bi-annual release of Bioconductor requires testing and squashing errors, warnings and bugs in a given package. Doing this means being able to work with multiple versions of R and multiple versions of Bioconductor libraries on a single system (assuming that you do production work and development on the same machine, right?).
I really, really like RStudio as my working R environment, as some of you have read before.</description>
    </item>
    
    <item>
      <title>Reproducible Methods</title>
      <link>/post/reproducible-methods/</link>
      <pubDate>Fri, 16 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/reproducible-methods/</guid>
      <description>Science is built on the whole idea of being able to reproduce results, i.e. if I publish something, it should be possible for someone else to reproduce it, using the description of the methods used in the publication. As biological sciences have become increasingly reliant on computational methods, this has become a bigger and bigger issue, especially as the results of experiments become dependent on independently developed computational code, or use rather sophisticated computer packages that have a variety of settings that can affect output, and multiple versions.</description>
    </item>
    
    <item>
      <title>R Interface for Teaching</title>
      <link>/post/r-interface-for-teaching/</link>
      <pubDate>Thu, 11 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/r-interface-for-teaching/</guid>
      <description>Kaitlin Thaney asked on Twitter last week about using Ramnath Vaidyanathan’s new interactive R notebook 1 2 for teaching.
Now, to be clear up front, I am not trying to be mean to Ramnath, discredit his work, or the effort that went into that project. I think it is really cool, and has some rather interesting potential applications, but I don’t really think it is the right interface for teaching R.</description>
    </item>
    
    <item>
      <title>Tim Hortons Density</title>
      <link>/post/tim-hortons-density/</link>
      <pubDate>Wed, 05 Jun 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/tim-hortons-density/</guid>
      <description>Inspired by this post, I wanted to examine the locations and density of Tim Hortons restaurants in Canada. Using Stats Canada data, each census tract is queried on Foursquare for Tims locations.
Setup options(stringsAsFactors=F) require(timmysDensity) require(plyr) require(maps) require(ggplot2) require(geosphere)  Statistics Canada Census Data The actual Statistics Canada data at the dissemination block level can be downloaded from here. You will want to download the Excel format, read it, and then save it as either tab-delimited or CSV using a non-standard delimiter, I used a semi-colon (;).</description>
    </item>
    
    <item>
      <title>Storing Package Data in Custom Environments</title>
      <link>/post/storing-package-data-in-custom-environments/</link>
      <pubDate>Thu, 30 May 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/storing-package-data-in-custom-environments/</guid>
      <description>If you do R package development, sometimes you want to be able to store variables specific to your package, without cluttering up the users workspace. One way to do this is by modifying the global options. This is done by packages grDevices and parallel. Sometimes this doesn’t seem to work quite right (see this issue for example.
Another way to do this is to create an environment within your package, that only package functions will be able to see, and therefore read from and modify.</description>
    </item>
    
    <item>
      <title>Writing Up Scientific Results and Literate Programming</title>
      <link>/post/writing-up-scientific-results-and-literate-programming/</link>
      <pubDate>Fri, 10 May 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/writing-up-scientific-results-and-literate-programming/</guid>
      <description>As an academic researcher, my primary purpose is to find some new insight, and subsequently communicate this insight to the general public. The process of doing this is traditionally thought to be:
 from observations of the world, generate a hypothesis design experiments to test hypothesis analyse results of the experiments to determine if hypothesis correct write report to communicate results to others (academics and / or general public)  And then repeat.</description>
    </item>
    
    <item>
      <title>Writing Papers Using R Markdown</title>
      <link>/post/writing-papers-using-r-markdown/</link>
      <pubDate>Tue, 09 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/writing-papers-using-r-markdown/</guid>
      <description>I have been watching the activity in RStudio and knitr for a while, and have even been using Rmd (R markdown) files in my own work as a way to easily provide commentary on an actual dataset analysis. Yihui has proposed writing papers in markdown and posting them to a blog as a way to host a statistics journal, and lots of people are now using knitr as a way to create reproducible blog posts that include code (including yours truly).</description>
    </item>
    
    <item>
      <title>Journal Club 2012-08-15</title>
      <link>/post/journal-club-2012-08-15/</link>
      <pubDate>Wed, 15 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/journal-club-2012-08-15/</guid>
      <description>I just came back from our Bioinformatic group (a rather loose association of various researchers at UofL interested in and doing bioinformatics) journal club, where we discussed this recent paper:
Google Goes Cancer: Improving Outcome Prediction for Cancer Patients by Network-Based Ranking of Marker Genes
Besides the catchy title that makes one believe that perhaps Google is getting into cancer research (maybe they are and we don’t know it yet), there were some interesting aspects to this paper.</description>
    </item>
    
    <item>
      <title>Creating Custom CDF&#39;s for Affymetrix Chips in Bioconductor</title>
      <link>/post/creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/</link>
      <pubDate>Fri, 13 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/</guid>
      <description>What? For those who don’t know, CDF files are chip definition format files that define which probes belong to which probesets, and are necessary to use any of the standard summarization methods such as RMA, and others.
 Why? Because we can, and because custom definitions have been shown to be quite useful. See the information over at Brainarray.
 Why not somewhere else? A lot of times other people create custom CDF files based on their own criteria, and make it subsequently available for others to use (see the Brainarray for an example of what some are doing, as well as PlandbAffy)</description>
    </item>
    
    <item>
      <title></title>
      <link>/post/2018-01-17-docopt-numeric-optoins/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/post/2018-01-17-docopt-numeric-optoins/</guid>
      <description>TL;DR If you use the docopt package to create command line R executables that take options, there is something to know about numeric command line options: they should have as.double before using them in your script.
Setup Lets set up a new docopt string, that includes both string and numeric arguments.
&amp;quot; Usage: test_numeric.R [--string=&amp;lt;string_value&amp;gt;] [--numeric=&amp;lt;numeric_value&amp;gt;] test_numeric.R (-h | --help) test_numeric.R Description: Testing how values are passed using docopt. Options: --string=&amp;lt;string_value&amp;gt; A string value [default: Hi!</description>
    </item>
    
  </channel>
</rss>