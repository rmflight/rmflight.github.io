<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Deciphering Life: One Bit at a Time</title>
    <link>/post/</link>
    <description>Recent content in Posts on Deciphering Life: One Bit at a Time</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Robert M Flight</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Using IRanges for Non-Integer Overlaps</title>
      <link>/post/iranges-for-non-integer-overlaps/</link>
      <pubDate>Sat, 23 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/iranges-for-non-integer-overlaps/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://bioconductor.org/packages/IRanges/&#34;&gt;&lt;code&gt;IRanges&lt;/code&gt;&lt;/a&gt; package implements interval algebra, and is very fast for finding overlaps of two ranges. If you have non-integer data, multiply values by a &lt;strong&gt;large&lt;/strong&gt; constant factor and round them. The constant depends on how much accuracy you need.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iranges&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IRanges??&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://bioconductor.org/packages/IRanges/&#34;&gt;&lt;code&gt;IRanges&lt;/code&gt;&lt;/a&gt; is a bioconductor package for interval algebra of &lt;strong&gt;i&lt;/strong&gt;nteger &lt;strong&gt;ranges&lt;/strong&gt;. It is used extensively in the &lt;code&gt;GenomicRanges&lt;/code&gt; package for finding overlaps between various genomic features. For genomic features, &lt;strong&gt;integers&lt;/strong&gt; make sense, because one cannot have fractional base locations.&lt;/p&gt;
&lt;p&gt;However, &lt;code&gt;IRanges&lt;/code&gt; uses &lt;a href=&#34;https://en.wikipedia.org/wiki/Red%E2%80%93black_tree&#34;&gt;red-black trees&lt;/a&gt; as its data structure, which provide very fast searches of overlaps. This makes it very attractive for &lt;strong&gt;any&lt;/strong&gt; problem that involves overlapping ranges.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;My motivation comes from mass-spectrometry data, where I want to count the number of raw data points and / or the number of peaks in a &lt;strong&gt;large&lt;/strong&gt; number of M/Z windows. Large here means on the order of 1,000,000 M/Z windows.&lt;/p&gt;
&lt;p&gt;Generating the windows is not hard, but searching the list of points / peaks for which ones are within the bounds of a window takes &lt;strong&gt;a really long time&lt;/strong&gt;. Long enough that I needed some other method.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iranges-to-the-rescue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;IRanges to the Rescue!&lt;/h2&gt;
&lt;p&gt;So my idea was to use &lt;code&gt;IRanges&lt;/code&gt;. But there is a problem, &lt;code&gt;IRanges&lt;/code&gt; is for integer ranges. How do we use this for non-integer data? Simple, multiply and round the fractional numbers to generate integers.&lt;/p&gt;
&lt;p&gt;It turns out that multiplying our mass-spec data by &lt;code&gt;20,000&lt;/code&gt; gives us differences down to the &lt;code&gt;0.00005&lt;/code&gt; place, which is more than enough accuracy for the size of the windows we are interested in. If needed, &lt;code&gt;IRanges&lt;/code&gt; can handle &lt;code&gt;1600 * 1e6&lt;/code&gt;, but currently will crash at &lt;code&gt;1600 * 1e7&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-fast-is-it&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Fast Is It?&lt;/h2&gt;
&lt;p&gt;Lets actually test differences in speed by counting how many overlapping points there are.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(IRanges)
library(ggplot2)
load(&amp;quot;../../data/iranges_example_data.rda&amp;quot;)

head(mz_points)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IRanges object with 6 ranges and 1 metadata column:
##           start       end     width |               mz
##       &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; |        &amp;lt;numeric&amp;gt;
##   [1]   2970182   2970182         1 | 148.509100524992
##   [2]   2970183   2970183         1 | 148.509161249802
##   [3]   2970184   2970184         1 |  148.50922197465
##   [4]   2970186   2970186         1 | 148.509282699535
##   [5]   3000526   3000526         1 | 150.026298638453
##   [6]   3000527   3000527         1 | 150.026360296201&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(mz_windows)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## IRanges object with 6 ranges and 2 metadata columns:
##           start       end     width |         mz_start           mz_end
##       &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; &amp;lt;integer&amp;gt; |        &amp;lt;numeric&amp;gt;        &amp;lt;numeric&amp;gt;
##   [1]   2960000   2960011        12 |              148 148.000554529159
##   [2]   2960001   2960012        12 | 148.000055452916 148.000609982075
##   [3]   2960002   2960013        12 | 148.000110905832 148.000665434991
##   [4]   2960003   2960014        12 | 148.000166358748 148.000720887907
##   [5]   2960004   2960016        13 | 148.000221811664 148.000776340823
##   [6]   2960006   2960017        12 |  148.00027726458 148.000831793739&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have some &lt;a href=&#34;https://github.com/rmflight/researchBlog_blogdown/blob/master/data/iranges_example_data.rda&#34;&gt;example data&lt;/a&gt; with 3447542 windows, and 991816 points. We will count how many point there are in each window using the below functions, with differing number of windows.&lt;/p&gt;
&lt;div id=&#34;functions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_overlaps_naive &amp;lt;- function(mz_start, mz_end, points){
  sum((points &amp;gt;= mz_start) &amp;amp; (points &amp;lt;= mz_end))
}

iterate_windows &amp;lt;- function(windows, points){
  purrr::pmap_int(windows, count_overlaps_naive, points)
}

run_times_iterating &amp;lt;- function(windows, points){
  t &amp;lt;- Sys.time()
  window_counts &amp;lt;- iterate_windows(windows, points)
  t2 &amp;lt;- Sys.time()
  run_time &amp;lt;- difftime(t2, t, units = &amp;quot;secs&amp;quot;)
  run_time
}

run_times_countoverlaps &amp;lt;- function(windows, points){
  t &amp;lt;- Sys.time()
  window_counts &amp;lt;- countOverlaps(points, windows)
  t2 &amp;lt;- Sys.time()
  run_time &amp;lt;- difftime(t2, t, units = &amp;quot;secs&amp;quot;)
  run_time
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;define-samples-of-different-sizes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Define Samples of Different Sizes&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

sample_sizes &amp;lt;- c(10, 100, 1000, 10000, 50000, 100000)

window_samples &amp;lt;- purrr::map(sample_sizes, function(x){sample(length(mz_windows), size = x)})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;run-it&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Run It&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iranges_times &amp;lt;- purrr::map_dbl(window_samples, function(x){
  run_times_countoverlaps(mz_windows[x], mz_points)
})

window_frame &amp;lt;- as.data.frame(mcols(mz_windows))

naive_times &amp;lt;- purrr::map_dbl(window_samples, function(x){
  run_times_iterating(window_frame[x, ], mz_points)
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-them&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Plot Them&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_times &amp;lt;- data.frame(size = rep(sample_sizes, 2),
                        time = c(iranges_times, naive_times),
                        method = rep(c(&amp;quot;iranges&amp;quot;, &amp;quot;naive&amp;quot;), each = 6))

p &amp;lt;- ggplot(all_times, aes(x = log10(size), y = time, color = method)) + geom_point() + geom_line() + labs(y = &amp;quot;time (s)&amp;quot;, x = &amp;quot;log10(# of windows)&amp;quot;, title = &amp;quot;Naive &amp;amp; IRanges Timings&amp;quot;) + theme(legend.position = c(0.2, 0.8))
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-23-iranges-for-non-integer-overlaps_files/figure-html/difference_times-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p + ylim(c(0, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-06-23-iranges-for-non-integer-overlaps_files/figure-html/difference_times-2.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As the two figures show, the naive solution, while a little faster under 1000 regions, is quickly outperformed by &lt;code&gt;IRanges&lt;/code&gt;, whose time increases much more slowly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Turn Robert&#39;s Beard Purple!</title>
      <link>/post/turn-roberts-beard-purple/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/turn-roberts-beard-purple/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If I &lt;a href=&#34;http://act.alz.org/site/TR?pg=personal&amp;amp;px=14694554&amp;amp;fr_id=11244&#34;&gt;raise $100 by August 25ths The Walk to End Alzheimer’s&lt;/a&gt;, I will have my beard dyed purple in support of Alzheimer’s awareness.&lt;/p&gt;
&lt;p&gt;If you are in another country, donate to your local Alzheimer’s charity and &lt;a href=&#34;http://rmflight.github.io/#contact&#34;&gt;email me&lt;/a&gt; with the subject &lt;em&gt;walk&lt;/em&gt; so I count it towards my total.&lt;/p&gt;
&lt;p&gt;Links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://act.alz.org/site/TR?pg=personal&amp;amp;px=14694554&amp;amp;fr_id=11244&#34;&gt;My donation page&lt;/a&gt; (&lt;a href=&#34;https://www.charitywatch.org/ratings-and-metrics/alzheimers-association-national-office/461&#34;&gt;Charity report&lt;/a&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://secure2.convio.net/alz/site/Donation2;jsessionid=00000000.app202b?df_id=1683&amp;amp;mfc_pref=T&amp;amp;1683.donation=form1&amp;amp;s_locale=en_CA&amp;amp;NONCE_TOKEN=D46EF58FAD6FA1276A4C99FBED9E9155&#34;&gt;Alzheimer Society Canada&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://secure.alzheimers.org.uk/?gclid=CjwKCAjw9qfZBRA5EiwAiq0AbScJN_IE7JW3arALIYmd2F2gzMk3ZV32sT9FGyR3yx_spMlgUzqMkhoCgUEQAvD_BwE&#34;&gt;Alzheimer’s Society UK&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;video&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Video&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/5cmMtfDnqbM?rel=0&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;why-a-purple-beard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why a Purple Beard??&lt;/h2&gt;
&lt;p&gt;I decided to participate in the Walk To End Alzheimer’s this year, coming up on August 25th here in Lexington. I will be walking the 2 miles in support of my Mom, who was diagnosed with early onset Alzheimer’s some time ago (she will turn 68 this summer).&lt;/p&gt;
&lt;p&gt;Why am I walking?? Because I’ve seen a little bit of what Alzheimer’s does, and although my Mom won’t benefit from new treatments, we need to find effective treatments for this devastating disease.&lt;/p&gt;
&lt;p&gt;As an incentive to donate, if I reach my &lt;a href=&#34;http://act.alz.org/site/TR?pg=personal&amp;amp;px=14694554&amp;amp;fr_id=11244&#34;&gt;fundraising goal of $100&lt;/a&gt;, I will have my beard dyed Alzheimer’s Association purple! The wonderful folks at &lt;a href=&#34;http://bak4morestudio.com/&#34;&gt;Bak 4 More studios&lt;/a&gt; have agreed to help me dye my beard purple before the Walk date. I will make sure to take lots of video and pictures of the process, which I am told will probably involve two trips to the studio, one to lighten my beard and a second to actually apply the purple color.&lt;/p&gt;
&lt;p&gt;Using the magic of computers, I’ve tried to show here what that might look like. I’m sure it will look 100X better than these, this is just to give a possible idea.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/selfie_walk_lores.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;I also realize that not everyone may be able to donate to the American Alzheimer’s Association, so if you want to support my fundraising for the Walk (and turn my beard purple) by donating to your local Alzheimer’s charity, just &lt;a href=&#34;http://rmflight.github.io/#contact&#34;&gt;send me an email&lt;/a&gt; with the subject &lt;em&gt;walk&lt;/em&gt; with the amount you donated and to which charity, and I will count your donation towards your local charity towards my goal. I will also respond to your email so that you know I’ve counted it.&lt;/p&gt;
&lt;p&gt;I will NOT be trimming my beard between now and the Walk date, so there will be even &lt;strong&gt;more purple beard&lt;/strong&gt; to go around, with 9 weeks between now and then.&lt;/p&gt;
&lt;p&gt;Thank you for helping me raise funds for the Walk to End Alzheimer’s, and feel free to spread this post far and wide.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>knitrProgressBar Package</title>
      <link>/post/knitrprogressbar/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/knitrprogressbar/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you like &lt;code&gt;dplyr&lt;/code&gt; progress bars, and wished you could use them everywhere, including from within Rmd documents, non-interactive shells, etc, then you should check out &lt;code&gt;knitrProgressBar&lt;/code&gt; (&lt;a href=&#34;https://cran.r-project.org/package=knitrProgressBar&#34;&gt;cran&lt;/a&gt; &lt;a href=&#34;https://github.com/rmflight/knitrProgressBar&#34;&gt;github&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-yet-another-progress-bar&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why Yet Another Progress Bar??&lt;/h2&gt;
&lt;p&gt;I didn’t set out to create &lt;strong&gt;another&lt;/strong&gt; progress bar package. But I really liked &lt;code&gt;dplyr&lt;/code&gt;s style of progress bar, and how they worked under the hood (thanks to the &lt;a href=&#34;https://rud.is/b/2017/03/27/all-in-on-r%E2%81%B4-progress-bars-on-first-post/&#34;&gt;examples&lt;/a&gt; from Bob Rudis).&lt;/p&gt;
&lt;p&gt;As I used them, I noticed that no progress was displayed if you did &lt;code&gt;rmarkdown::render()&lt;/code&gt; or &lt;code&gt;knitr::knit()&lt;/code&gt;. That just didn’t seem right to me, as that means you get no progress indicator if you want to use caching facilities of &lt;code&gt;knitr&lt;/code&gt;. So this package was born.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How??&lt;/h2&gt;
&lt;p&gt;These are pretty easy to setup and use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitrProgressBar)

# borrowed from example by @hrbrmstr
arduously_long_nchar &amp;lt;- function(input_var, .pb=NULL) {
  
  update_progress(.pb) # function from knitrProgressBar
  
  Sys.sleep(0.01)
  
  nchar(input_var)
  
}

# using stdout() here so progress is part of document
pb &amp;lt;- progress_estimated(26, progress_location = stdout())

purrr::map(letters, arduously_long_nchar, .pb = pb)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
|===                                              |  8% ~1 s remaining     
|=============                                    | 27% ~0 s remaining     
|======================                           | 46% ~0 s remaining     
|================================                 | 65% ~0 s remaining     
|=========================================        | 85% ~0 s remaining     
Completed after 0 s&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 1
## 
## [[2]]
## [1] 1
## 
## [[3]]
## [1] 1
## 
## [[4]]
## [1] 1
## 
## [[5]]
## [1] 1
## 
## [[6]]
## [1] 1
## 
## [[7]]
## [1] 1
## 
## [[8]]
## [1] 1
## 
## [[9]]
## [1] 1
## 
## [[10]]
## [1] 1
## 
## [[11]]
## [1] 1
## 
## [[12]]
## [1] 1
## 
## [[13]]
## [1] 1
## 
## [[14]]
## [1] 1
## 
## [[15]]
## [1] 1
## 
## [[16]]
## [1] 1
## 
## [[17]]
## [1] 1
## 
## [[18]]
## [1] 1
## 
## [[19]]
## [1] 1
## 
## [[20]]
## [1] 1
## 
## [[21]]
## [1] 1
## 
## [[22]]
## [1] 1
## 
## [[23]]
## [1] 1
## 
## [[24]]
## [1] 1
## 
## [[25]]
## [1] 1
## 
## [[26]]
## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main difference to &lt;code&gt;dplyr&lt;/code&gt;s progress bars is that here you have the option to set &lt;strong&gt;where&lt;/strong&gt; the progress gets written to, either automatically using the built-in &lt;code&gt;make_kpb_output_decisions()&lt;/code&gt;, or directly. Also, I have provided the &lt;code&gt;update_progress&lt;/code&gt; function to actually do the updating or finalizing properly.&lt;/p&gt;
&lt;p&gt;There are also package specific options to control &lt;strong&gt;how&lt;/strong&gt; the decisions are made.&lt;/p&gt;
&lt;p&gt;See the &lt;a href=&#34;https://rmflight.github.io/knitrProgressBar/&#34;&gt;main&lt;/a&gt; documentation, as well as the &lt;a href=&#34;https://rmflight.github.io/knitrProgressBar/articles/example_progress_bars.html&#34;&gt;included vignette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-processing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multi-Processing&lt;/h2&gt;
&lt;p&gt;As of V1.1.0 (should be on CRAN soon), the package also supports indicating progress on multi-processed jobs. See the included &lt;a href=&#34;https://rmflight.github.io/knitrProgressBar/articles/multiprocessing.html&#34;&gt;vignette&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;By the way, I know this method is not ideal, but I could not get the combination of &lt;code&gt;later&lt;/code&gt; and &lt;code&gt;processx&lt;/code&gt; to work in my case. If anyone is willing to help out, that would be great.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Licensing R Packages that Include Others Code</title>
      <link>/post/licensing-r-packages-that-include-others-code/</link>
      <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/licensing-r-packages-that-include-others-code/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you include others code in your own R package, list them as contributors with comments about what they contributed, and add a license statement in the file that includes their code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;I recently created the &lt;a href=&#34;https://CRAN.R-project.org/package=knitrProgressBar&#34;&gt;&lt;code&gt;knitrProgressBar&lt;/code&gt;&lt;/a&gt; package. It is a really simple package, that takes the &lt;code&gt;dplyr&lt;/code&gt; progress bars and makes it possible for them to write progress to a supplied file connection. The &lt;code&gt;dplyr&lt;/code&gt; package itself is licensed under MIT, so I felt fine taking the code directly from &lt;code&gt;dplyr&lt;/code&gt; itself. In addition, I didn’t want my package to depend on &lt;code&gt;dplyr&lt;/code&gt;, so I wanted that code self-contained in my own package, and I wanted to be able to modify underlying mechanics that might have been more complicated if I had just made a new class of progress bar that inherited from &lt;code&gt;dplyr&lt;/code&gt;’s.&lt;/p&gt;
&lt;p&gt;I also wanted to be able to release my code on CRAN, not just on GitHub. I knew to accomplish that I would have to have all the license stuff correct. However, I had not seen any guide on how to &lt;strong&gt;license&lt;/strong&gt; a package and give proper attribution in the &lt;code&gt;Authors@R&lt;/code&gt; field.&lt;/p&gt;
&lt;p&gt;Note that I did ask this question on &lt;a href=&#34;https://stackoverflow.com/questions/48525023/properly-license-r-package-that-includes-other-mit-code&#34;&gt;StackOverflow&lt;/a&gt; and &lt;a href=&#34;https://discuss.ropensci.org/t/licensing-a-new-package-that-uses-code-from-another-source/1046&#34;&gt;ROpenSci&lt;/a&gt; forums as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;information-from-cran&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Information from CRAN&lt;/h2&gt;
&lt;p&gt;I should note here, that the CRAN author guidelines do provide a &lt;strong&gt;small&lt;/strong&gt; hint in this regard in the &lt;a href=&#34;https://cran.r-project.org/doc/manuals/r-release/R-exts.html&#34;&gt;Writing R Extensions&lt;/a&gt; guide:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that all significant contributors must be included: if you wrote an R wrapper for the work of others included in the src directory, you are not the sole (and maybe not even the main) author.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;However, there is not any guidance provided on &lt;strong&gt;how&lt;/strong&gt; these should ideally be listed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-answer&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Full Answer&lt;/h2&gt;
&lt;p&gt;I am the main author and maintainer of the new package, that is easy. The original code is MIT licensed, authored by several persons, and has copyright held by RStudio.&lt;/p&gt;
&lt;p&gt;My solution then was to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/rmflight/knitrProgressBar/blob/master/R/progress.R#L6&#34;&gt;add the MIT license&lt;/a&gt; from &lt;code&gt;dplyr&lt;/code&gt; to the file that has the progress bar code&lt;/li&gt;
&lt;li&gt;add &lt;a href=&#34;https://github.com/rmflight/knitrProgressBar/blob/master/DESCRIPTION#L5&#34;&gt;all the authors&lt;/a&gt; of &lt;code&gt;dplyr&lt;/code&gt; as &lt;strong&gt;contributors&lt;/strong&gt; to my package, with a comment as to &lt;strong&gt;why&lt;/strong&gt; they are listed&lt;/li&gt;
&lt;li&gt;add RStudio as a &lt;strong&gt;copyright holder&lt;/strong&gt; to my package, with a comment that this only applies to the one file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So the &lt;code&gt;Authors@R&lt;/code&gt; line in my &lt;code&gt;DESCRIPTION&lt;/code&gt; ended up being:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Authors@R: c(person(given = c(&amp;quot;Robert&amp;quot;, &amp;quot;M&amp;quot;), family = &amp;quot;Flight&amp;quot;, email = &amp;quot;rflight79@gmail.com&amp;quot;, role = c(&amp;quot;aut&amp;quot;, &amp;quot;cre&amp;quot;)),
            person(&amp;quot;Hadley&amp;quot;, &amp;quot;Wickham&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;Romain&amp;quot;, &amp;quot;Francois&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;Lionel&amp;quot;, &amp;quot;Henry&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;Kirill&amp;quot;, &amp;quot;Müller&amp;quot;, role = &amp;quot;ctb&amp;quot;, comment = &amp;quot;Author of included dplyr fragments&amp;quot;),
            person(&amp;quot;RStudio&amp;quot;, role = &amp;quot;cph&amp;quot;, comment = &amp;quot;Copyright holder of included dplyr fragments&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>docopt &amp; Numeric Options</title>
      <link>/post/docopt-numeric-options/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/docopt-numeric-options/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;If you use the &lt;code&gt;docopt&lt;/code&gt; package to create command line &lt;code&gt;R&lt;/code&gt; executables that take
options, there is something to know about numeric command line options: they should
have &lt;code&gt;as.double&lt;/code&gt; before using them in your script.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Lets set up a new &lt;code&gt;docopt&lt;/code&gt; string, that includes both string and
numeric arguments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;&amp;quot;
Usage:
  test_numeric.R [--string=&amp;lt;string_value&amp;gt;] [--numeric=&amp;lt;numeric_value&amp;gt;]
  test_numeric.R (-h | --help)
  test_numeric.R

Description: Testing how values are passed using docopt.

Options:
  --string=&amp;lt;string_value&amp;gt;  A string value [default: Hi!]
  --numeric=&amp;lt;numeric_value&amp;gt;   A numeric value [default: 10]

&amp;quot; -&amp;gt; doc&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(methods)
library(docopt)

script_options &amp;lt;- docopt(doc)

script_options&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 8
##  $ --string : chr &amp;quot;Hi!&amp;quot;
##  $ --numeric: chr &amp;quot;10&amp;quot;
##  $ -h       : logi FALSE
##  $ --help   : logi FALSE
##  $ string   : chr &amp;quot;Hi!&amp;quot;
##  $ numeric  : chr &amp;quot;10&amp;quot;
##  $ h        : logi FALSE
##  $ help     : logi FALSE
## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very easy to see here, that the &lt;code&gt;numeric&lt;/code&gt; argument is indeed a string, and
if you want to use it as numeric, it should first be converted using &lt;code&gt;as.double&lt;/code&gt;,
&lt;code&gt;as.integer&lt;/code&gt;, or even &lt;code&gt;as.numeric&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cant-you-easily-tell-its-character&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can’t You Easily Tell It’s Character?&lt;/h2&gt;
&lt;p&gt;I just bring this up because I recently used &lt;code&gt;docopt&lt;/code&gt; to provide interfaces to
three executables scripts, and I spent a lot of time &lt;code&gt;printing&lt;/code&gt; the &lt;code&gt;doc&lt;/code&gt; strings,
and I somehow never noticed that the numeric values were actually character and
needed to be converted to a numeric first. Hopefully this will save someone else
some time in that regard.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Custom Deployment Script</title>
      <link>/post/custom-deployment-script/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/custom-deployment-script/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Use a short bash script to do deployment from your own computer directly to your &lt;code&gt;*.github.io&lt;/code&gt; domain.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;So Yihui recommends using Netlify, or even Travis-CI in the Blogdown book. I wasn’t willing to setup a custom domain yet, and some of my posts involve a lot of personally created packages, etc, that I don’t want to debug installation on Travis. So, I wanted a simple script I could call on my laptop that would copy the &lt;code&gt;/public&lt;/code&gt; directory to the repo for my &lt;code&gt;github.io&lt;/code&gt; site, and then push the changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Script&lt;/h2&gt;
&lt;p&gt;Here is the simple script I ended up using:&lt;/p&gt;
&lt;pre class=&#34;sh&#34;&gt;&lt;code&gt;#!/bin/bash
org_dir=`pwd`
cd path/to/github.io/repo/
#rm -rf *
cp -Rfu path/to/blogdown/public/* .

git add *
commit_time=`date`
git commit -m &amp;quot;update at $commit_time&amp;quot;
git push origin master

cd $org_dir&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It changes directories, because to push from a &lt;code&gt;git&lt;/code&gt; repo I’m pretty sure you need to be in the directory, so it also makes sure to go back there at the end. It then copies the contents of &lt;code&gt;/public&lt;/code&gt; to the repo, &lt;code&gt;add&lt;/code&gt;s all the files, and then uses the current time-stamp as the commit message, and finally pushes all the updates.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Differences in Posted Date vs sessionInfo()</title>
      <link>/post/differences-in-posted-date-vs-sessioninfo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/differences-in-posted-date-vs-sessioninfo/</guid>
      <description>&lt;p&gt;If you are a newcomer to my weblog, you may notice that some posts that are &lt;code&gt;R&lt;/code&gt; tutorials generally include the output of &lt;code&gt;Sys.time()&lt;/code&gt; at the end. If you look closeley at that time and the &lt;strong&gt;Posted on&lt;/strong&gt; date, you may notice that some posts show disagreement between them. This is because I decided to move &lt;em&gt;all&lt;/em&gt; of my old blog posts from &lt;em&gt;blogspot&lt;/em&gt; to here, and keep the original posted dates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linking to Manually Inserted Images in Blogdown / Hugo</title>
      <link>/post/linking-to-manually-inserted-images-in-hugo/</link>
      <pubDate>Wed, 27 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/linking-to-manually-inserted-images-in-hugo/</guid>
      <description>&lt;div id=&#34;manual-linking&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Manual Linking?&lt;/h2&gt;
&lt;p&gt;Using &lt;code&gt;blogdown&lt;/code&gt; for generating websites and blog-posts from &lt;code&gt;Rmarkdown&lt;/code&gt; files with lots of inserted code and figures seems pretty awesome, but sometimes you want to include a figure manually, either because you want to generate something manually and convert it (say for going from SVG of lots of points to hi-res PNG), or because it is a figure from something else (&lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;like this figure from wikipedia&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-to&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where to??&lt;/h2&gt;
&lt;p&gt;To do this, you want the text of your &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tag to your image to be:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;img src = &amp;quot;/img/image_file.png&amp;quot;&amp;gt;&amp;lt;/img&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then put the image itself in the directory &lt;code&gt;/static/img/image_file.png&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/Standard_deviation_diagram.svg&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;By M. W. Toews, &lt;a href=&#34;http://creativecommons.org/licenses/by/2.5&#34;&gt;CC BY 2.5&lt;/a&gt;, via Wikimedia Commons, &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg&#34;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This information is also mentioned in &lt;a href=&#34;https://bookdown.org/yihui/blogdown/static-files.html&#34;&gt;section 2.7 of the Blogdown book&lt;/a&gt;. Obviously I need to do more reading.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I was Part of the Problem</title>
      <link>/post/i-was-part-of-the-problem/</link>
      <pubDate>Wed, 18 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/i-was-part-of-the-problem/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;With the recent charges of sexual harassment against some high-profile individuals, and so many women coming forward with #metoo (and the understanding that this is really something almost &lt;em&gt;all&lt;/em&gt; women have faced), I realized that my younger self was #partoftheproblem. I think many other men are part of the problem, &lt;strong&gt;even though they might not think so&lt;/strong&gt;. I didn’t think I was part of the problem either. I hope that other men might read this and critically evaluate if they are #partoftheproblem. I also hope and pray that my own sons will do better at this if I teach them right.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-could-i-be&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Could I Be?&lt;/h2&gt;
&lt;p&gt;Let me be up front. I have never &lt;strong&gt;sexually assaulted&lt;/strong&gt; anyone, let alone considered such a thing. But that’s not really the problem, because the way I acted towards women, I think they may have been scared that I might, as I have put tons of &lt;strong&gt;unwanted attention&lt;/strong&gt; on several women over the years, starting with when I was 12 years old, in the sixth grade.&lt;/p&gt;
&lt;p&gt;I also want to be clear, I was a horrible guy friend to women (even if they didn’t think so). If I knew a girl had a boyfriend, well then, I would &lt;strong&gt;not&lt;/strong&gt; even consider trying to hit on or express interest in that girl, and I was “friends” with plenty of women over the course of my school years who had boyfriends. But in &lt;strong&gt;most&lt;/strong&gt; cases I secretly hoped they might dump their boyfriends and go out with me instead. Also, if I knew they didn’t have a boyfriend, and I found them remotely attractive, then I would do all I could to try to become friends with them in the &lt;strong&gt;hope to eventually become their boyfriend&lt;/strong&gt;. So, my sole reason for being friends with women, really, was to eventually become romantically involved. That was my primary motivation. Looking back on it now, it makes me sick.&lt;/p&gt;
&lt;p&gt;I’ve never had a woman tell me she was assaulted by anyone either, but given my past behavior, even if someone I knew had, I don’t think my actions made me someone that a woman would trust to tell.&lt;/p&gt;
&lt;p&gt;Let me give you some examples of my behavior.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grade-school&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grade School&lt;/h2&gt;
&lt;p&gt;In 6th grade, I decided that I wanted a girlfriend, and I picked out one girl in my class who I wanted to be my girlfriend. I am very sure I never asked her out, to be my girlfriend, but I made sure to spend tons of time with her, and if I recall correctly, she eventually got the gist of my interest, and told me &lt;strong&gt;very clearly she wasn’t interested&lt;/strong&gt;. But &lt;strong&gt;her telling me no did not stop my unwanted advances&lt;/strong&gt; or attention. I am sure that I made her very uncomfortable the rest of that grade.&lt;/p&gt;
&lt;p&gt;In middle school (7-9 at the time), I pretty much continued this process unabated. I would latch onto a woman that I found attractive, and make her the target of my affections, and pour out my unwanted attention upon her, &lt;strong&gt;not taking no for an answer&lt;/strong&gt;. I only stopped after long periods of continued rejection, or when that person acquired a significant other. Although not an excuse for my actions, my tactics and hopes were largely fueled by rampaging hormones, way too many romantic comedies where the nice guy always got the girl by virtue of sheer persistence (this was the 90’s), and nascent exposure to pornography.&lt;/p&gt;
&lt;p&gt;I would find out girls numbers and call them without being asked. I would know where these girls were at all times through the day, even during lunch and between classes. I would find any excuse to be near them. Every sock-hop (weekly lunch time dance on gym floor) I would ask these girls to dance with me. I would give them valentines cards, Christmas cards, etc, in &lt;strong&gt;the hopes that they would realize what a great guy I was and go out with me.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Just so we are clear, none of this got me any dates in grade school.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;undergraduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Undergraduate&lt;/h2&gt;
&lt;p&gt;Now I’ve graduated high-school, I’m heading off to a local university, with lots of girls. I made lots of friends with girls who had boyfriends, in fact I think my circle of friends had way more girls in it than guys. But, I was always finding one girl who I wanted to date, and would make sure to spend extra time around them, helping them whenever possible, etc, and dropping subtle and not so subtle hints that I wanted to be their boyfriend. And there was always the hope that someone would break-up with their current boyfriend and find me, the faithful friend, waiting to comfort them.&lt;/p&gt;
&lt;p&gt;Over the course of this time, I had three women agree to be my date. Two of those did not result in an actual date, because I started acting like a stalker after they said yes, and they wisely stayed away. In the third case, we went out twice, but me calling at random hours, and showing up at her house un-announced because I thought she was really sad freaked her out, and she stopped talking to my creepy, stalkerish, clingy self.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;post-graduate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Post-Graduate&lt;/h2&gt;
&lt;p&gt;Somehow, it seems, by the time I got to my PhD, I had &lt;strong&gt;mostly&lt;/strong&gt; given up on finding a girlfriend, settling down and getting married (really, that was my goal). I say mostly. I don’t know if I hadn’t met my now spouse in the first couple of months of my PhD that I would not have continued making unwanted advances on the women in my PhD program. (By the way, I met my spouse outside of work, at a Church actually, and was introduced by a mutual friend. In the 13 years I’ve known her, there are only a handful of days we haven’t talked to each other since we went on our first date).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-real-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Real Problem&lt;/h2&gt;
&lt;p&gt;And this is the &lt;strong&gt;real&lt;/strong&gt; problem. Too many men, my past self included, think women owe them something for being their friend, for being a &lt;strong&gt;nice guy&lt;/strong&gt;. For giving them any kind of attention, or any kind of help. Too many men believe these things, and then use their power and prestige, to demand things of women. Guys, &lt;strong&gt;women don’t owe you anything.&lt;/strong&gt; They definitely don’t owe you sex or reciprocated romantic interest because of something you did for them. They are another person worthy of respect, simply because they are a person.&lt;/p&gt;
&lt;p&gt;In addition, real life is not a romantic comedy. Non-romantic friendships are a good thing, because we need other peoples perspectives in our lives. So, if a woman tells you &lt;strong&gt;no, she doesn’t want to date you&lt;/strong&gt;, accept it, and move on. Don’t make it awkward, especially if you are in the same work environment. &lt;strong&gt;Don’t assume that a woman is romantically interested just because she is friendly.&lt;/strong&gt; I know, radical thought. Maybe try being friends, colleagues, whatever with no romantic intentions, and no expectations of them either. Don’t be #partoftheproblem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solutions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solutions&lt;/h2&gt;
&lt;p&gt;Teach your children that they can be friends with people of the opposite sex without being romantically involved, especially as they hit puberty. Teach them that &lt;strong&gt;no means no&lt;/strong&gt;, not &lt;strong&gt;no means maybe in 3 weeks&lt;/strong&gt;, or &lt;strong&gt;no means maybe if I try hard enough&lt;/strong&gt;. And if you see other men engaging in putting unwanted attention on women, call them out on it, &lt;strong&gt;whatever form it may take&lt;/strong&gt;. I wish someone had said something to me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;I realize that our general culture is really #partoftheproblem, when we have highly sexualized advertising (especially of women to men), and the idea that &lt;strong&gt;boys will be boys&lt;/strong&gt;, tell jokes about sexual assault, and propagate the idea that &lt;strong&gt;women want it&lt;/strong&gt;, based on how they act or dress. Those are all wrong too, and our culture needs to change.&lt;/p&gt;
&lt;p&gt;I also realize that some of what I describe about myself is rather mild in comparison to much of what gets reported, but that’s not the point. It is still unwanted attention, and I didn’t know how to take no for an answer. Those women &lt;strong&gt;didn’t want my attention&lt;/strong&gt;, and I couldn’t accept that. If I had a different temperament, I don’t know what I would have done. Enough people realized it that some friends in Undergrad stopped being around me, but no one ever told me that what I was doing was wrong, and my parents weren’t involved enough in my so-called love life to know what was going on. If they had, I think they would have told me to knock it off and stop being an idiot.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Criticizing a Publication, and Lying About It</title>
      <link>/post/criticizing-a-publication-and-lying-about-it/</link>
      <pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/criticizing-a-publication-and-lying-about-it/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Other researchers &lt;a href=&#34;https://dx.doi/org/10.1002/prot.25024&#34;&gt;directly criticized&lt;/a&gt; a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;recent publication of ours&lt;/a&gt; in a “research article”. Although they raised valid points, they &lt;strong&gt;outright lied&lt;/strong&gt; about the availability of our results. In addition, they did not provide access to their own results. We have published &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;new work&lt;/a&gt; supporting our original results, and a direct rebuttal of their critique in a &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;perspective article&lt;/a&gt;. The peer reviewers of their “research article” must have been asleep at the wheel to allow the major point, lack of access to our results, to stand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;original-publication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Original Publication&lt;/h2&gt;
&lt;p&gt;Back in the summer of 2015, I was second author on a publication (&lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.24834/full&#34;&gt;Yao et al., 2015&lt;/a&gt;, hereafter YS2015) describing an automated method to characterize zinc ion coordination geometries (CGs). Applying our automated method to all zinc sites in the worldwide Protein Data Bank (wwPDB), we found &lt;em&gt;abberrant&lt;/em&gt; zinc CGs that don’t fit the canonical CGs. We were pretty sure that these aberrant CGs are real, and they have always existed, but had not been previously characterized because methods assumed that only the &lt;em&gt;canonical&lt;/em&gt; geometries should be observed in biological systems, and were excluding the &lt;em&gt;abberrant&lt;/em&gt; ones because they didn’t have good methods to detect and characterize them.&lt;/p&gt;
&lt;p&gt;Also of note, the proteins with aberrant zinc geometries showed enrichment for different types of enzyme classifications than those with canonical zinc geometries.&lt;/p&gt;
&lt;p&gt;For this publication, we made &lt;strong&gt;all&lt;/strong&gt; of our code and results available in a tarball that could be downloaded from our &lt;a href=&#34;http://bioinformatics.cesb.uky.edu/bin/view/Main/SoftwareDevelopment#Metal_ion_coordination_analysis_software&#34;&gt;website&lt;/a&gt;. This data went up while the paper was in review, on Dec 7, 2015 (with a correction on Dec 15). Recently, we’ve also put a copy of the tarball on &lt;a href=&#34;https://figshare.com/articles/Zn_metalloprotein_paper/4229333&#34;&gt;FigShare&lt;/a&gt;. Every draft of the publication, from initial submission through to accepted publication, included the link to the tarball on the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Critique&lt;/h2&gt;
&lt;p&gt;Less than a year later, &lt;a href=&#34;http://sci-hub.cc/doi/10.1002/prot.25024&#34;&gt;Raczynska, Wlodawer, and Jaskolski&lt;/a&gt; (RJW2016) published a &lt;em&gt;critique&lt;/em&gt; of YS2015 as a “research article”. In their publication, they questioned the existence of the &lt;em&gt;abberrant&lt;/em&gt; sites completely, based on the examination and remodeling of four aberrant structures highlighted in YS2015. To be fair, they did have some valid criticisms of the methods, and Sen Yao did a lot of work in our latest paper to address them.&lt;/p&gt;
&lt;p&gt;As part of the critique, however, they claimed that they could only evaluate the four structures listed in two figures &lt;strong&gt;because we didn’t provide all of our results&lt;/strong&gt;. However, we had previously made our full results available as a tarball from our website. As you can see in the below figure, &lt;strong&gt;all&lt;/strong&gt; of the results were really available in that tarball.&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/ys2017_figure1.png&#34;, width = &#34;600&#34;&gt;&lt;/p&gt;
&lt;p&gt;In addition, although RWJ2016 went to all the trouble to actually remodel those four structures by going back to the original X-ray density, they &lt;strong&gt;didn’t make any of their models available&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Finally, no one from RWJ2016 ever contacted our research group to see if the results might be available.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;response&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Response&lt;/h2&gt;
&lt;div id=&#34;follow-up-paper-on-5-metals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Follow-Up Paper on 5 Metals&lt;/h3&gt;
&lt;p&gt;By the time the critiques appeared in RJW2016, Sen was already hard at work showing that the previously developed methods could be modified and then applied to other metal ion CGs, and that they also contained aberrant CGs (see &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;YS2017-1&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;critique-direct-response&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Critique Direct Response&lt;/h3&gt;
&lt;p&gt;In addition to YS2017-1, we felt that the critique deserved separate response (&lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;YS2017-2&lt;/a&gt;). To that end, we began drafting a response, wherein we pointed out some of the problems with RJW2016, the first being that we did indeed provide the &lt;strong&gt;full&lt;/strong&gt; set of results from YS2015, and therefore it was possible to evaluate our full work. We also addressed each of their other criticisms of YS2015, in many cases going beyond the original criticism, and explaining how it was being addressed in YS2017-1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-results-and-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Open Results and Code&lt;/h3&gt;
&lt;p&gt;A major part of the conclusions in YS2017-2 was also devoted to the idea that code and results in science need to be shared, highlighting the fact that RJW2016 &lt;strong&gt;did not share their models&lt;/strong&gt; they used to try and discredit our work, lied about the fact that we did not share our own results, and pointing out some other projects in this research area that have shared well and others that have shared badly, and that the previous attitude of competition among research groups does not move science forward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;peer-review&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Peer Review&lt;/h2&gt;
&lt;p&gt;Let’s just say that the &lt;em&gt;peer-review&lt;/em&gt; of both of the papers was &lt;strong&gt;interesting&lt;/strong&gt;. Both manuscripts had the same set of reviewers. YS2017-1, the five metal paper, had some rather rigorous peer review, and was definitely improved by the reviewer’s comments. YS2017-2, our perspective, in contrast, was attacked by one peer reviewer right from submission, and was questioned almost continually as to whether it should even be published. I am thankful that one reviewer saw the need for it to be published, and that the Editor ultimately decided that it should be published, and that we were able to rebut each of the reviewer’s criticisms.&lt;/p&gt;
&lt;p&gt;Finally, I really don’t know what happened in the peer review of RWJ2016. The first major claim was that our data wasn’t available, it should have taken a reviewer 10 minutes to verify and debunk that claim. I would have expected a much different critique from the authors had they actually examined our full data set. But, because of traditional closed peer review, that record is closed to us.&lt;/p&gt;
&lt;p&gt;Overall though, I’m very happy both of our publications are now out, and we can move on to new stages of our analyses. Looking forward to continuing to work with my co-authors to move the work forward.&lt;/p&gt;
&lt;div id=&#34;papers-discussed&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Papers Discussed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Original Zinc CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.24834&#34;&gt;Yao et al 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Critique of Zinc CGs: Raczynska, Wlodawer &amp;amp; Jaskolski 2016, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25024&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://sci-hub.cc/10.1002/prot.25024&#34;&gt;sci-hub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;5 Metal CGs: &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25257&#34;&gt;Yao et al 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Response to critique: Yao et al 2017, &lt;a href=&#34;https://dx.doi.org/10.1002/prot.25263&#34;&gt;publisher&lt;/a&gt;, &lt;a href=&#34;https://doi.org/10.6084/m9.figshare.4754263.v1&#34;&gt;copy on figshare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Authentication of Key Resources for Data Analysis</title>
      <link>/post/authentication-of-key-resources-for-data-analysis/</link>
      <pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/authentication-of-key-resources-for-data-analysis/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;NIH recently introduced a reproducibility initiative, extending to including the
“Authentication of Key Resources” page in grant applications from Jan 25, 2016.
Seems to be intended for grants involving biological reagents, but we included
it in our recent R03 grant developing new data analysis methods. We believe that
this type of thing should become common for all grants, not just those that use
biological/chemical resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nih-and-reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;NIH and Reproducibility&lt;/h2&gt;
&lt;p&gt;There has been a lot of things published recently about the &lt;em&gt;reproducibility
crisis&lt;/em&gt; in science (see refs). The federal funding agencies are starting to
respond to this, and beginning with grants submitted after January 25, 2016,
grants are &lt;a href=&#34;http://grants.nih.gov/reproducibility/index.htm&#34;&gt;supposed to address the
reproducibility&lt;/a&gt; of the work
proposed, including the presence of various confounding factors (i.e. sex of
animals, the source of cell lines, etc). In addition to this, there is a new
document that can be added to grants, the &lt;a href=&#34;http://nexus.od.nih.gov/all/2016/01/29/authentication-of-key-biological-andor-chemical-resources-in-nih-grant-applications/&#34;&gt;&lt;strong&gt;Authentication
Plan&lt;/strong&gt;&lt;/a&gt;,
which as far as I can tell is intended specifically for:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;key biological and/or chemical resources used in the proposed studies&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, this makes sense. Some sources of irreproducibility include, but are not
limited to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unvalidated antibodies&lt;/li&gt;
&lt;li&gt;cell lines that are not what was thought&lt;/li&gt;
&lt;li&gt;impure chemicals&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think this is a &lt;strong&gt;good thing&lt;/strong&gt;. What does it have to do with data analysis?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-code-authentication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data / Code Authentication&lt;/h2&gt;
&lt;p&gt;When we were submitting a recent R03 proposal for developing novel data analysis
methods and statistical tools, the grant management office asked us about the
&lt;strong&gt;Authentication of Key Resources&lt;/strong&gt; attachment, which we completely missed. Upon
review of the guidelines, we initially determined that this document did not
apply. However, we decided to go ahead and take some initiative.&lt;/p&gt;
&lt;div id=&#34;data-authentication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data Authentication?&lt;/h3&gt;
&lt;p&gt;When dealing with multiple samples from high-throughput samples, there are
frequently a few easy ways to examine the data quality, and although it can be
hard to verify that the data &lt;strong&gt;is what the supplier says it is&lt;/strong&gt;, which would be
true &lt;strong&gt;authentication&lt;/strong&gt;, there are some ways to verify that the various samples
in the dataset are at least self-consistent within each sample class (normal and
disease, condition 1 and condition 2).&lt;/p&gt;
&lt;p&gt;My go-to for data self-consistency are principal components analysis (PCA) and
correlation heat-maps. Correlation heat-maps involve calculating
all of the pairwise sample to sample correlations using all of the non-zero
sample features (those that are non-zero in the two pairs being compared). These
heatmaps, combined with the sample class information, and clustering within each
class, are a nice visual way to eyeball samples that have potential problems. A
simple example for RNA-seq transcriptomics was shown in &lt;a href=&#34;https://dx.doi.org/10.1093/bioinformatics/btv425&#34;&gt;Gierliński et al.,
Statistical models for RNA-seq data derived from a two-condition 48-replicate
experiment Bioinformatics (2015) 31 (22): 3625-3630, Figure
1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/yeast_48rep_cor_heatmap.jpg&#34; alt=&#34;Gierlinkski et al heatmap, Figure 1&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Gierlinkski et al heatmap, Figure 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The other measures they used in this paper are also very nice, in plotting the
median correlation of a sample against all other samples, and the fraction of
outlier features in a given sample (see figure 2 of Gierlinkski et al). The
final measure they propose is not generally applicable to all -omics data
however.&lt;/p&gt;
&lt;p&gt;PCA on the data, followed by visualizing the scores on the first few principal
components, and colored by sample class (or experimental condition) is similar
in spirit to the correlation heat-map. In fact, it is very similar, because PCA
is actually decomposing on the covariance of the samples, which is very related
to the correlations (an early algorithm actually used the correlation matrix).&lt;/p&gt;
&lt;p&gt;Both of these methods can highlight possible problems with individual samples,
and make sure that the set of data going into the analysis is at least
self-consistent, which is important when doing classification or differential
abundance analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;code-authentication&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Code Authentication&lt;/h3&gt;
&lt;p&gt;The other thing we highlighted in the document was &lt;strong&gt;code&lt;/strong&gt; authentication. In
this case, we highlighted the use of unit-testing in the R packages that we are
planning to develop. Even though this is software coming out of a research lab,
we need to have confidence that the functions we write return the correct values
given various inputs. In addition, code testing coverage helps evaluate that we
are testing &lt;em&gt;all&lt;/em&gt; of the functionality by checking that all of the lines in our
code are run by the tests. Finally, we are also planning to write tests for core
functions provided by others (i.e. functions in other R packages), in that they
work as we expect, by returning correct values given specific inputs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Going forward, I think it would be a good thing if people writing research grants
for data analysis methods would discuss how they are going to look at the data
to assess it’s quality, and how they are going to do unit testing, and will have
to start saying that they are going to do unit testing of their analysis method.&lt;/p&gt;
&lt;p&gt;I’d be interested in others’ thoughts on this as well.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Random Forest vs PLS on Random Data</title>
      <link>/post/random-forest-vs-pls-on-random-data/</link>
      <pubDate>Sat, 12 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/random-forest-vs-pls-on-random-data/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Partial least squares (PLS) discriminant-analysis (DA) can ridiculously over fit
even on completely random data. The quality of the PLS-DA model can be assessed
using cross-validation, but cross-validation is not typically performed in many
metabolomics publications. Random forest, in contrast, because of the &lt;em&gt;forest&lt;/em&gt; of
decision tree learners, and the out-of-bag (OOB) samples used for testing each tree,
automatically provides an indication of the quality of the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;I’ve recently been working on some machine learning work using &lt;strong&gt;random forests&lt;/strong&gt;
(RF) &lt;a href=&#34;https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf&#34;&gt;Breimann, 2001&lt;/a&gt; on metabolomics data. This has been relatively successful,
with decent sensitivity and specificity, and hopefully I’ll be able to post more
soon. However, PLS (Wold, 1975) is a standard technique used in metabolomics
due to the prevalence of analytical chemists in metabolomics and a long familiarity
with the method. Importantly, my collaborators frequently use PLS-DA to generate
plots to show that the various classes of samples are separable.&lt;/p&gt;
&lt;p&gt;However, it has long been known that PLS (and all of it’s variants, PLS-DA, OPLS,
OPLS-DA, etc) can easily generate models that over fit the data, and that over fitting
of the model needs to be assessed if the model is going to be used in subsequent
analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random Data&lt;/h2&gt;
&lt;p&gt;To illustrate the behavior of both RF and PLS-DA, we will generate some random data
where each of the samples are randomly assigned to one of two classes.&lt;/p&gt;
&lt;div id=&#34;feature-intensities&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Feature Intensities&lt;/h3&gt;
&lt;p&gt;We will generate a data set with 1000 features, where each feature’s mean value
is from a uniform distribution with a range of 1-10000.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;cowplot&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     ggsave&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fakeDataWithError)
set.seed(1234)
n_point &amp;lt;- 1000
max_value &amp;lt;- 10000
init_values &amp;lt;- runif(n_point, 0, max_value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init_data &amp;lt;- data.frame(data = init_values)
ggplot(init_data, aes(x = data)) + geom_histogram() + ggtitle(&amp;quot;Initial Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/plot_initial-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For each of these features, their distribution across samples will be based on
a random normal distribution where the mean is the initial feature value and a
standard deviation of 200. The number of samples is 100.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_sample &amp;lt;- 100
error_values &amp;lt;- add_uniform_noise(n_sample, init_values, 200)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just for information, the &lt;code&gt;add_uniform_noise&lt;/code&gt; function is this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;add_uniform_noise&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (n_rep, value, sd, use_zero = FALSE) 
## {
##     n_value &amp;lt;- length(value)
##     n_sd &amp;lt;- n_rep * n_value
##     out_sd &amp;lt;- rnorm(n_sd, 0, sd)
##     out_sd &amp;lt;- matrix(out_sd, nrow = n_value, ncol = n_rep)
##     if (!use_zero) {
##         tmp_value &amp;lt;- matrix(value, nrow = n_value, ncol = n_rep, 
##             byrow = FALSE)
##         out_value &amp;lt;- tmp_value + out_sd
##     }
##     else {
##         out_value &amp;lt;- out_sd
##     }
##     return(out_value)
## }
## &amp;lt;bytecode: 0x5d3e3a0&amp;gt;
## &amp;lt;environment: namespace:fakeDataWithError&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I created it as part of a package that is able to add different kinds of noise
to data.&lt;/p&gt;
&lt;p&gt;The distribution of values for a single feature looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;error_data &amp;lt;- data.frame(feature_1 = error_values[1,])
ggplot(error_data, aes(x = feature_1)) + geom_histogram() + ggtitle(&amp;quot;Error Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/plot_error-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And we will assign the first 50 samples to &lt;strong&gt;class_1&lt;/strong&gt; and the second 50 samples
to &lt;strong&gt;class_2&lt;/strong&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_class &amp;lt;- rep(c(&amp;quot;class_1&amp;quot;, &amp;quot;class_2&amp;quot;), each = 50)
sample_class&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##   [8] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [15] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [22] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [29] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [36] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [43] &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot; &amp;quot;class_1&amp;quot;
##  [50] &amp;quot;class_1&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [57] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [64] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [71] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [78] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [85] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [92] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;
##  [99] &amp;quot;class_2&amp;quot; &amp;quot;class_2&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PCA&lt;/h2&gt;
&lt;p&gt;Just to show that the data is pretty random, lets use principal components
analysis (PCA) to do a decomposition, and plot the first two components:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp_pca &amp;lt;- prcomp(t(error_values), center = TRUE, scale. = TRUE)
pca_data &amp;lt;- as.data.frame(tmp_pca$x[, 1:2])
pca_data$class &amp;lt;- as.factor(sample_class)
ggplot(pca_data, aes(x = PC1, y = PC2, color = class)) + geom_point(size = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/pca_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random Forest&lt;/h2&gt;
&lt;p&gt;Let’s use RF first, and see how things look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(randomForest)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## randomForest 4.6-14&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;randomForest&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_model &amp;lt;- randomForest(t(error_values), y = as.factor(sample_class))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The confusion matrix comparing actual &lt;em&gt;vs&lt;/em&gt; predicted classes based on the
out of bag (OOB) samples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(rf_model$confusion)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;class_1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;class_2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;class.error&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;class_1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;class_2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.56&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And an overall error of 0.5760364.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pls-da&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PLS-DA&lt;/h2&gt;
&lt;p&gt;So PLS-DA is really just PLS with &lt;strong&gt;y&lt;/strong&gt; variable that is binary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pls_model &amp;lt;- plsda(t(error_values), as.factor(sample_class), ncomp = 2)
pls_scores &amp;lt;- data.frame(comp1 = pls_model$scores[,1], comp2 = pls_model$scores[,2], class = sample_class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And plot the PLS scores:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(pls_scores, aes(x = comp1, y = comp2, color = class)) + geom_point(size = 4) + ggtitle(&amp;quot;PLS-DA of Random Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/plot_plsda-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And voila! Perfectly separated data! If I didn’t tell you that it was random, would
you suspect it?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cross-validated-pls-da&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Cross-validated PLS-DA&lt;/h2&gt;
&lt;p&gt;Of course, one way to truly assess the worth of the model would be to use
cross-validation, where a fraction of data is held back, and the model trained
on the rest. Predictions are then made on the held back fraction, and because we
know the truth, we will then calculate the &lt;strong&gt;area under the reciever operator curve&lt;/strong&gt;
(AUROC) or area under the curve (AUC) created by plotting true positives &lt;em&gt;vs&lt;/em&gt;
false positives.&lt;/p&gt;
&lt;p&gt;To do this we will need two functions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Generates all of the CV folds&lt;/li&gt;
&lt;li&gt;Generates PLS-DA model, does prediction on hold out, calculates AUC&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cvTools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: robustbase&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ROCR)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: gplots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gplots&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     lowess&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_cv &amp;lt;- function(xdata, ydata, nrep, kfold){
  n_sample &amp;lt;- length(ydata)
  all_index &amp;lt;- seq(1, n_sample)
  cv_data &amp;lt;- cvFolds(n_sample, K = kfold, R = nrep, type = &amp;quot;random&amp;quot;)
  
  rep_values &amp;lt;- vapply(seq(1, nrep), function(in_rep){
    use_rep &amp;lt;- cv_data$subsets[, in_rep]
    cv_values &amp;lt;- vapply(seq(1, kfold), function(in_fold){
      test_index &amp;lt;- use_rep[cv_data$which == in_fold]
      train_index &amp;lt;- all_index[-test_index]
      
      plsda_cv(xdata[train_index, ], ydata[train_index], xdata[test_index, ],
               ydata[test_index])
    }, numeric(1))
  }, numeric(kfold))
}

plsda_cv &amp;lt;- function(xtrain, ytrain, xtest, ytest){
  pls_model &amp;lt;- plsda(xtrain, ytrain, ncomp = 2)
  pls_pred &amp;lt;- predict(pls_model, xtest, type = &amp;quot;prob&amp;quot;)
  
  use_pred &amp;lt;- pls_pred[, 2, 1]
  
  pred_perf &amp;lt;- ROCR::prediction(use_pred, ytest)
  pred_auc &amp;lt;- ROCR::performance(pred_perf, &amp;quot;auc&amp;quot;)@y.values[[1]]
  return(pred_auc)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now lets do a bunch of replicates (100).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_vals &amp;lt;- gen_cv(t(error_values), factor(sample_class), nrep = 100, kfold = 5)

mean(cv_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.4198336&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(cv_vals)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1134857&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv_frame &amp;lt;- data.frame(auc = as.vector(cv_vals))
ggplot(cv_frame, aes(x = auc)) + geom_histogram(binwidth = 0.01)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2015-12-12-random-forest-vs-pls-on-random-data_files/figure-html/rep_plsda-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we get an average AUC of 0.4198336, which is pretty awful. This implies
that even though there was good separation on the scores, maybe the model is
not actually that good, and we should be cautious of any predictions being made.&lt;/p&gt;
&lt;p&gt;Of course, the PCA at the beginning of the analysis shows that there is no &lt;em&gt;real&lt;/em&gt;
separation in the data in the first place.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::session_info()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Session info -------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  setting  value                       
##  version  R version 3.5.0 (2018-04-23)
##  system   x86_64, linux-gnu           
##  ui       X11                         
##  language en_US                       
##  collate  en_US.UTF-8                 
##  tz       America/New_York            
##  date     2018-06-25&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Packages -----------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  package           * version    date      
##  assertthat          0.2.0      2017-04-11
##  backports           1.1.2      2017-12-13
##  base              * 3.5.0      2018-04-30
##  bindr               0.1.1      2018-03-13
##  bindrcpp            0.2.2      2018-03-29
##  bitops              1.0-6      2013-08-17
##  blogdown            0.5        2018-01-24
##  bookdown            0.7        2018-02-18
##  broom               0.4.4      2018-03-29
##  caret             * 6.0-79     2018-03-29
##  caTools             1.17.1     2014-09-10
##  class               7.3-14     2015-08-30
##  codetools           0.2-15     2016-10-05
##  colorspace          1.3-2      2016-12-14
##  compiler            3.5.0      2018-04-30
##  cowplot           * 0.9.2      2017-12-17
##  CVST                0.2-1      2013-12-10
##  cvTools           * 0.3.2      2012-05-14
##  datasets          * 3.5.0      2018-04-30
##  ddalpha             1.3.1.1    2018-02-02
##  DEoptimR            1.0-8      2016-11-19
##  devtools            1.13.5     2018-02-18
##  digest              0.6.15     2018-01-28
##  dimRed              0.1.0      2017-05-04
##  dplyr               0.7.5      2018-05-19
##  DRR                 0.0.3      2018-01-06
##  evaluate            0.10.1     2017-06-24
##  fakeDataWithError * 0.0.1      2018-04-02
##  foreach             1.4.4      2017-12-12
##  foreign             0.8-70     2017-11-28
##  gdata               2.18.0     2017-06-06
##  ggplot2           * 2.2.1      2016-12-30
##  glue                1.2.0      2017-10-29
##  gower               0.1.2      2017-02-23
##  gplots            * 3.0.1      2016-03-30
##  graphics          * 3.5.0      2018-04-30
##  grDevices         * 3.5.0      2018-04-30
##  grid                3.5.0      2018-04-30
##  gtable              0.2.0      2016-02-26
##  gtools              3.5.0      2015-05-29
##  highr               0.6        2016-05-09
##  htmltools           0.3.6      2017-04-28
##  ipred               0.9-6      2017-03-01
##  iterators           1.0.9      2017-12-12
##  kernlab             0.9-25     2016-10-03
##  KernSmooth          2.23-15    2015-06-29
##  knitr               1.20       2018-02-20
##  labeling            0.3        2014-08-23
##  lattice           * 0.20-35    2017-03-25
##  lava                1.6.1      2018-03-28
##  lazyeval            0.2.1      2017-10-29
##  lubridate           1.7.3      2018-02-27
##  magrittr            1.5        2014-11-22
##  MASS                7.3-49     2018-02-23
##  Matrix              1.2-13     2018-04-02
##  memoise             1.1.0      2017-04-21
##  methods           * 3.5.0      2018-04-30
##  mnormt              1.5-5      2016-10-15
##  ModelMetrics        1.1.0      2016-08-26
##  munsell             0.4.3      2016-02-13
##  nlme                3.1-136    2018-03-09
##  nnet                7.3-12     2016-02-02
##  parallel            3.5.0      2018-04-30
##  pillar              1.2.1      2018-02-27
##  pkgconfig           2.0.1      2017-03-21
##  pls                 2.6-0      2016-12-18
##  plyr                1.8.4      2016-06-08
##  prodlim             1.6.1      2017-03-06
##  psych               1.8.3.3    2018-03-30
##  purrr               0.2.5      2018-05-29
##  R6                  2.2.2      2017-06-17
##  randomForest      * 4.6-14     2018-03-25
##  Rcpp                0.12.17    2018-05-18
##  RcppRoll            0.2.2      2015-04-05
##  recipes             0.1.2      2018-01-11
##  reshape2            1.4.3      2017-12-11
##  rlang               0.2.0.9001 2018-06-07
##  rmarkdown           1.9        2018-03-01
##  robustbase        * 0.92-8     2017-11-01
##  ROCR              * 1.0-7      2015-03-26
##  rpart               4.1-13     2018-02-23
##  rprojroot           1.3-2      2018-01-03
##  scales              0.5.0.9000 2018-04-02
##  sfsmisc             1.1-2      2018-03-05
##  splines             3.5.0      2018-04-30
##  stats             * 3.5.0      2018-04-30
##  stats4              3.5.0      2018-04-30
##  stringi             1.2.2      2018-05-02
##  stringr             1.3.1      2018-05-10
##  survival            2.41-3     2017-04-04
##  tibble              1.4.2      2018-01-22
##  tidyr               0.8.0      2018-01-29
##  tidyselect          0.2.4      2018-02-26
##  timeDate            3043.102   2018-02-21
##  tools               3.5.0      2018-04-30
##  utils             * 3.5.0      2018-04-30
##  withr               2.1.2      2018-04-02
##  xfun                0.1        2018-01-22
##  yaml                2.1.18     2018-03-08
##  source                                     
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  local                                      
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  local                                      
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  local                                      
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  cran (@0.7.5)                              
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  Github (rmflight/fakeDataWithError@ccd8714)
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  local                                      
##  local                                      
##  local                                      
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  local                                      
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  local                                      
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  cran (@0.2.5)                              
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  cran (@0.12.17)                            
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  Github (r-lib/rlang@ba4fb06)               
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  Github (hadley/scales@d767915)             
##  CRAN (R 3.5.0)                             
##  local                                      
##  local                                      
##  local                                      
##  cran (@1.2.2)                              
##  cran (@1.3.1)                              
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)                             
##  local                                      
##  local                                      
##  Github (jimhester/withr@79d7b0d)           
##  CRAN (R 3.5.0)                             
##  CRAN (R 3.5.0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Novel Zinc Coordination Geometries</title>
      <link>/post/novel-zinc-coordination-geometries/</link>
      <pubDate>Wed, 17 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/novel-zinc-coordination-geometries/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Currently available methods to discover metal geometries make too many assumptions. We were able to discover novel zinc coordination geometries using a &lt;strong&gt;less-biased&lt;/strong&gt; method that makes fewer assumptions. These novel geometries seem to also have specific functionality. This work was recently published under an #openaccess license in Proteins Journal: Yao, S., Flight, R. M., Rouchka, E. C. and Moseley, H. N. B. (2015), A less-biased analysis of metalloproteins reveals novel zinc coordination geometries. Proteins. &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/prot.24834/full&#34;&gt;doi: 10.1002/prot.24834&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;zinc-coordination&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Zinc Coordination&lt;/h2&gt;
&lt;p&gt;As I’m sure many people know, zinc is a very important metal for biology. It is one of the most numerous metal ions, and plays a role in many different types of proteins. Zinc ions in protein structures can have anywhere from 4 to 6 ligands, in many different coordination geometries.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/prot24834-fig-0001.png&#34; alt=&#34;zinc coordination geometries&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;zinc coordination geometries&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Figure 1 from Yao et al., 2015&lt;/p&gt;
&lt;p&gt;How the zinc ion is coordinated is going to depend on the protein sequence of amino acids that surround it, and knowledge of the protein sequence should enable knowledge of how the zinc ions are coordinated. Therefore, being able to characterize zinc ion coordination geometries (CGs) from structural data (such as protein structures in the world-wide protein data bank) and associate them with protein sequences is very important.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;previous-attempts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Previous Attempts&lt;/h2&gt;
&lt;p&gt;Other groups had done this previously, and we now have hidden-markov models for determining zinc binding thanks to this work. However, in all the cases that we could find, the determination of CG was made by comparison to &lt;strong&gt;previously known&lt;/strong&gt; geometries that have been described from zinc-compound crystal structures.&lt;/p&gt;
&lt;p&gt;When the biologically based CGs are compared to previously known, there will be a bunch of CGs that are &lt;strong&gt;outliers&lt;/strong&gt; or will remain &lt;strong&gt;unclassified&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;our-initial-attempt&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Our Initial Attempt&lt;/h2&gt;
&lt;p&gt;When Sen (the lead author, currently a third year PhD student in the University of Louisville Bioinformatics program) first tried to use bootstrapping and a expectation-maximization algorithm to automatically classify the zinc CGs, she started getting rather &lt;strong&gt;funny&lt;/strong&gt; results, as in, why is the algorithm not converging and we are getting these &lt;strong&gt;huge variances&lt;/strong&gt; in the various measures that characterize the CGs.&lt;/p&gt;
&lt;p&gt;A single visualization was key in unlocking what was going on. Figure 2 in the paper is a histogram of the minimum angle (where angle is ligand-zinc-ligand) in degrees.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/prot24834-fig-0002.png&#34; alt=&#34;minimum angle histogram&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;minimum angle histogram&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Based on Figure 1, we would expect the minimum angle to be &lt;strong&gt;90&lt;/strong&gt;, but as you can see in Figure 2, there are an awful lot of angles less than &lt;strong&gt;90&lt;/strong&gt;. We are very sure that they are real given the definition of zinc-ligand bond-length that was used to define potential ligands, and the statistical decision used to define &lt;strong&gt;how many&lt;/strong&gt; ligands a given zinc ion has.&lt;/p&gt;
&lt;p&gt;So we have a bunch of zinc-ions that make ligand-zinc-ligand bond angles at 60, and even 30 degrees! It turns out that these small angles are largely (but not all) due to bidentate ligands, where for example the zinc ion forms bonds with two oxygen’s from an aspartate or glutamate amino acid.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;separate-out-compressed-and-cluster&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Separate Out Compressed and Cluster!&lt;/h2&gt;
&lt;p&gt;To make the problem tractable, we first have to separate out the compressed angle zinc sites, and then we can do clustering on the set of ligand-zinc-ligand angles to determine in a mostly un-biased fashion what CGs are present and which zinc-site belongs to which CG.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;p&gt;The use of k-means clustering on the angles also required developing a way to order the ligand-zinc-ligand angles in such a way that they are comparable across all of the different CGs. The final method used in the paper was to order them using &lt;em&gt;largest-sortedmiddle-opposite&lt;/em&gt;, which is the largest angle, the middle angles sorted in order, and lastly the opposite of the largest angle. This keeps them from being scrambled from site to site, and allows them to be comparable across zinc sites.&lt;/p&gt;
&lt;p&gt;Also, because the number of true clusters was unknown as we are trying to discover in an unbiased way all CGs, the number of clusters was varied, and for each &lt;em&gt;k&lt;/em&gt;-clusters, replicate clusterings done, and the stability of the clusters was assessed by cluster membership and locations across replicates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;novel-cgs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Novel CGs&lt;/h2&gt;
&lt;p&gt;Based on all this work, we were able to compare generated clusters from both the &lt;em&gt;normal&lt;/em&gt; and &lt;em&gt;compressed&lt;/em&gt; groups with canonical CGs to determine if a CG from clustering corresponds to a &lt;strong&gt;known&lt;/strong&gt; CG or a &lt;strong&gt;novel&lt;/strong&gt; CG. The &lt;strong&gt;normal&lt;/strong&gt; clusters mostly corresponded to &lt;strong&gt;known&lt;/strong&gt; CGs or unsurprising variants thereof. What was really interesting is that there were multiple clusters corresponding to a tetrahedral CG. To determine if the &lt;em&gt;compressed&lt;/em&gt; CGs were merely a &lt;em&gt;compressed&lt;/em&gt; variant of the canonical, the compressed angle was removed from the comparison to generate a probability of assignment. At least one of the &lt;em&gt;compressed&lt;/em&gt; groups appear to be novel, in that it has not been described before, either structurally or functionally.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;functional-characterization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Functional Characterization&lt;/h2&gt;
&lt;p&gt;We also wanted to determine if there was a functional component to the CGs, &lt;em&gt;i.e.&lt;/em&gt; different CGs have different functionality (this is my primary but not only contribution to this manuscript). To do this we annotated all of the PDB sequences using &lt;a href=&#34;http://www.ebi.ac.uk/Tools/pfa/iprscan5/&#34;&gt;&lt;em&gt;InterProScan&lt;/em&gt;&lt;/a&gt;, and kept annotations that intersected the range of amino-acids that potentially interact with the zinc ion. This bit is tricky, because many different CGs can have many different functionality, merely doing hypergeometric-enrichment of annotations in each cluster doesn’t lead to a convincing picture; as we quickly found out (it was the first thing I tried). However, if we generate a measure of functional similarity between clusters (based on a faux covariance, really, read the paper, it is rather neat) and compare this functional cluster similarity measure with a cluster distance based on the angles, there is an extremely high Spearman correlation of 0.88 for the &lt;em&gt;normal&lt;/em&gt; and 0.66 for the &lt;em&gt;compressed&lt;/em&gt; CGs, implying that CG and function are intimately related.&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/prot24834-fig-0009.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src = &#34;/img/prot24834-fig-0010.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Finally, we considered &lt;strong&gt;all&lt;/strong&gt; &lt;em&gt;normal&lt;/em&gt; and &lt;em&gt;compressed&lt;/em&gt; clusters as separate groups and performed hypergeometric-enrichment on both the &lt;em&gt;InterProScan&lt;/em&gt; and &lt;em&gt;EC&lt;/em&gt; number annotations, finding enriched annotations specific to each group of clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Making too many assumptions about biological structure can be a bad thing. Given that, however, if you are using canonical structures for assignment and you get a ton of &lt;strong&gt;outliers&lt;/strong&gt;, maybe you need to re-examine your data and methods.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Machine learning methods such as &lt;em&gt;random-forest&lt;/em&gt;, &lt;em&gt;k-means clustering&lt;/em&gt;, and statistical classification can be readily used to discover and assign metal-ion CG. If you are careful, and separate out the &lt;em&gt;compressed&lt;/em&gt; from &lt;em&gt;normal&lt;/em&gt; first.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is a rather tight relationship between a zinc-ion’s CG and the functionality of the protein it is embedded within.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;reproducibility&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reproducibility&lt;/h2&gt;
&lt;p&gt;Although we have admittedly dropped the ball on this, there will be a tarball of all of the scripts used to generate the results including a README explaining how to run them available soon.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Mouse / Human Transcriptomics and Batch Effects</title>
      <link>/post/mouse-human-transcriptomics-and-batch-effects/</link>
      <pubDate>Mon, 01 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/mouse-human-transcriptomics-and-batch-effects/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;This &lt;a href=&#34;http://dx.doi.org/10.1073/pnas.1413624111&#34;&gt;2014 PNAS paper by S. Lin et al (Lin et al., PNAS, 2014)&lt;/a&gt; that compares transcription of tissues between species has a flawed experimental design, where species is almost perfectly confounded with machine / lane on which the sequencing was done. Y. Golad and O. Mizrahi-Man have &lt;a href=&#34;http://f1000research.com/articles/4-121/v1&#34;&gt;published a manuscript&lt;/a&gt; describing the confounding and the results of removing it. This was possible because the original authors supplied the information about which publically available files were used in the original analysis. The data from this experiment is probably only suitable as an example of what not to do in high-throughput biology experimental design, and that there &lt;em&gt;may&lt;/em&gt; be similarities in human and mouse transcriptional programs.&lt;/p&gt;
&lt;p&gt;We discussed these papers in the Systems Biology and Omics Integration University of Kentucky Journal Club on June 1, 2015. I lead that discussion.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mouse-human-transcriptomic-differences&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mouse / Human Transcriptomic Differences&lt;/h2&gt;
&lt;p&gt;In 2014, two papers were published by members of the ENCODE project purporting that tissue gene expression clustered more by species than by tissue. In the first (&lt;a href=&#34;http://www.nature.com/nature/journal/v515/n7527/full/nature13992.html&#34;&gt;ENCODE Consortium, Nature, 2014, doi:10.1038/nature13992 2014&lt;/a&gt;), a large number of experiments were combined and compared. &lt;a href=&#34;http://www.nature.com/nature/journal/v515/n7527/fig_tab/nature13992_F2.html&#34;&gt;Figure 2a&lt;/a&gt; shows a PCA plot of expression in across 10 tissues in human and mouse.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://www.nature.com/nature/journal/v515/n7527/images/nature13992-f2.jpg&#34; alt=&#34;ENCODE paper figuer 2&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ENCODE paper figuer 2&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interestingly enough, if you collapse PC1 (definitely species), then the tissues start to look quite similar. Which does not seem that unexpected, there are species specific differences, but the tissues are doing something similar in each species.&lt;/p&gt;
&lt;p&gt;This was not the end of the story, however. A subsequent publication (&lt;a href=&#34;http://www.pnas.org/content/111/48/17224&#34;&gt;S Lin et al., PNAS, 2014, doi: 10.1073/pnas.1413624111&lt;/a&gt;) went further in doing fresh sequencing of 13 tissues in both species, still showing bigger differences between species than between tissues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-everything-as-it-seems&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is Everything As it Seems?&lt;/h2&gt;
&lt;p&gt;On April 28, Y. Gilad sent out &lt;a href=&#34;https://twitter.com/Y_Gilad/status/593088451462963202&#34;&gt;this tweet&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We reanalyzed the data from &lt;a href=&#34;http://t.co/Fv7z9WwLJ4&#34;&gt;http://t.co/Fv7z9WwLJ4&lt;/a&gt; and found the following: &lt;a href=&#34;http://t.co/37eVs8Kln9&#34;&gt;pic.twitter.com/37eVs8Kln9&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The left figure shows the original data, clustering by species, and on the right, reprocessed data, clustering by tissue. Now, I have to admit when he posted this I was kind of ticked off. Where was the blog-post or manuscript showing what exactly was done? If you look at the comments to Yoav on that tweet, it seems others were wondering the same thing. Thankfully, on May 19, the manuscript hit &lt;a href=&#34;http://f1000research.com/articles/4-121/v1&#34;&gt;F1000Research (Gilad Y and Mizrahi-Man O. A reanalysis of mouse ENCODE comparative gene expression data [v1; ref status: approved with reservations 1, http://f1000r.es/5ez] F1000Research 2015, 4:121 (doi: 10.12688/f1000research.6536.1))&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;batch-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Batch Effects&lt;/h2&gt;
&lt;p&gt;Yoav asked for the list of data files that were used in the PNAS paper, and then examined the read id line to extract the experimental design. This experimental design is captured in Figure 1:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://f1000researchdata.s3.amazonaws.com/manuscripts/7019/9f5f4330-d81d-46b8-9a3f-d8cb7aaf577e_figure1.gif&#34; alt=&#34;Y Golad Fig 1&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Y Golad Fig 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Do you notice a problem with this design?&lt;/p&gt;
&lt;p&gt;.
.
.
.
.&lt;/p&gt;
&lt;p&gt;Hopefully you noticed that species (one of the main effects to investigate) is not randomly or even semi-randomly distributed across sequencers and / or lanes, but is almost perfectly confounded with sequencer / lane. It doesn’t matter if one technician handled all the samples, this is potentially a large batch effect / confounding variable.&lt;/p&gt;
&lt;p&gt;And Yoav shows that ignoring the batch effect produces data much like that reported in the Lin et al PNAS pub, while removing the batch effect using &lt;a href=&#34;http://bioinformatics.oxfordjournals.org/content/28/6/882&#34;&gt;ComBat&lt;/a&gt; results in the tissues clustering together.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;is-the-data-still-useful&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Is the Data Still Useful?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1_yA8RHdfodOV-5IoR-yb317KwaWgXFAvJntaIm8HYaU/edit?usp=sharing&#34;&gt;I presented&lt;/a&gt; these papers and the discussion around them at our weekly Systems Biology and Omics Integration (&lt;a href=&#34;http://sboi.bioinformatics.uky.edu&#34;&gt;SBOI&lt;/a&gt;) Journal Club on June 1, 2015. There were a couple of concerns with the overall study:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is ENCODE data still useful?
&lt;ul&gt;
&lt;li&gt;some people seemed to be concerned that this brought the general ENCODE project into question. I think by the end we agreed that in general it is probably ok to be using ENCODE data, but this particular study was questionable.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What does the data tell us?
&lt;ul&gt;
&lt;li&gt;based on the correction that Yoav did and reanalysis, is there anything actually telling in the data? Unfortunately, because species is so confounded with machine, I don’t think there is much of a conclusion to draw about species differences &lt;em&gt;vs&lt;/em&gt; tissue similarity based on this data. There was some disagreement on this point.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Full experimental designs should be published, and requested by reviewers
&lt;ul&gt;
&lt;li&gt;Really, why a reviewer on the paper &lt;strong&gt;did not&lt;/strong&gt; request more information about the experimental design is beyond me, especially given the claims of the manuscript. Why it is not the norm to provide this kind of experimental design information in a manuscript is also a good question.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, I think the best use of the 2014 PNAS pub and this dataset is an example of &lt;strong&gt;how not&lt;/strong&gt; to design a biological experiment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;addendum&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;a id=&#34;addendum&#34;&gt;&lt;/a&gt; Addendum&lt;/h2&gt;
&lt;p&gt;As I looked at the comment section of the Gilad &amp;amp; Man article yesterday (June 1, 2015), I noticed that there were direct replies from S. Lin in a couple of places. In particular is a comment that Lin et al did a second set of sequencing with a new design, and reanalysed the data. Links are provided to two figures, a table of the new design and a new 3D PCA plot:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/slin_table1part2.jpg&#34; alt=&#34;new design&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;new design&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/slin_fig1part2.jpg&#34; alt=&#34;new pca&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;new pca&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The new sequencing design seems much more reasonable, and the PCA plot has many characteristics of the original one from the original comparative analysis by Mouse ENCODE (see above), in that yes, there are species specific differences, but there also appears to be a way to collapse along PC2 and PC3 where the tissues will line up with each other, which I kind of would expect.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>First Open Post-Publication Peer Review, with Credit!</title>
      <link>/post/first-open-post-publication-peer-review-with-credit/</link>
      <pubDate>Wed, 25 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/first-open-post-publication-peer-review-with-credit/</guid>
      <description>&lt;div id=&#34;tldr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;Reviewed &lt;a href=&#34;https://twitter.com/@biodataganache&#34;&gt;Jason McDermott’s&lt;/a&gt; &lt;a href=&#34;http://f1000research.com/articles/4-60/v1&#34;&gt;MDRPred paper&lt;/a&gt; on F1000Research!, where my review is posted along side the paper, &lt;a href=&#34;http://f1000research.com/articles/4-60/v1#referee-response-7889&#34;&gt;with a DOI&lt;/a&gt;, completely in the open with my name attached. Was a pleasant experience, aided by the fact that Jason wrote a good paper.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;f1000research&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;F1000Research!&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://f1000research.com/&#34;&gt;F1000Research!&lt;/a&gt; is a new publishing startup from F1000 that has a model of post-publication peer review, whereby upon submission the manuscript undergoes basic quality checks (no real editorial control), and then is published. Once the article is published, reviewers are invited to review, and they have 10 days to submit their review. Reviews are signed, and given a DOI. Reviewers are asked to assign one of &lt;strong&gt;Approved&lt;/strong&gt;, &lt;strong&gt;Approved with Reservations&lt;/strong&gt;, &lt;strong&gt;Not Approved&lt;/strong&gt; status to the article.&lt;/p&gt;
&lt;p&gt;When the article gets two revieweres giving &lt;strong&gt;Approved&lt;/strong&gt; status (likely after at least one round of review and resubmission), then it will be submitted to PubMed for indexing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-did-i-end-up-reviewing-this&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How Did I End Up Reviewing This??&lt;/h2&gt;
&lt;p&gt;Long story short, I’ve been following Jason on twitter for a while, and I happened to see him tweet and blog about trying to get an F1000Research article up as a citeable supporting publication for a grant going in. I thought it was a nifty idea, and although I actually at the time did not have much of an idea of what Jason’s research actually involved, communicated with him that when the article went live I would be willing to be recommended as a reviewer. Imagine my surprise that I actually had enough domain knowledge that I could actually review the article.&lt;/p&gt;
&lt;p&gt;That is saying something, given that the paper has a neat combination of genetic algorithms, regular expressions, and protein function prediction (really, you should go give it a read). By the way, this is also the first paper I’ve reviewed in a long time that did not have serious methodological problems, or where claims are made with no substantiation, and I could not identify any serious statistical issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-open-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Open Part&lt;/h2&gt;
&lt;p&gt;Although I don’t do a lot of reviewing, I had to admit that this was one of the best reviewing assignments I’ve had in a while. Although I believe that peer review is essential to science, and it needs to be more open (I’ve started signing my reviews for other journals), this was the first time I knew my review was open (my name would be known to the authors’ by default) and public. I have to say that this likely improved the care and thoroughness in doing the actual review as I went through the article, given that both the authors and anyone else who comes across the article and my review can see if the issues I raise are real, or if I’m trying to make myself look good. And my name will be publicly associated with it!&lt;/p&gt;
&lt;p&gt;Note that this &lt;strong&gt;openness&lt;/strong&gt; did not keep me from criticizing particular aspects of the paper. There are lots of things that need to be changed in the article before I will give it an &lt;strong&gt;Approved&lt;/strong&gt; status, and I laid those out in my review.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postpublication&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PostPublication&lt;/h2&gt;
&lt;p&gt;I really appreciated that I get to review a &lt;strong&gt;published&lt;/strong&gt; article, because it means that the whole thing is typeset, figures and tables are in their logical place (not at the end of the document!!!), the line spacing is readable, etc. Note to other publishers, at least let authors put the figures in-line for review if you are not going to type-set the article before it goes to reviewers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;disclaimers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Disclaimers&lt;/h2&gt;
&lt;p&gt;I was given an F1000Research! t-shirt as a reward for commenting on a &lt;a href=&#34;http://blog.f1000research.com/2013/08/23/peer-review-credit-where-credits-due/#comment-1020559658&#34;&gt;blog-post&lt;/a&gt; regarding incentives for peer-review. I like the shirt. Having done my review, I am also eligible to get a 50% discount on the article processing charges if I submit an article to F1000Research in the next 12 months.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
