[
  {
    "path": "posts/2022-01-06-cairo-and-xquartz-in-mac-github-actions/",
    "title": "Cairo and XQuartz in Mac GitHub Actions",
    "description": "Including cairo and xquartz in MacOS github actions",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2022-01-06",
    "categories": [
      "random-code-snippets",
      "cairo",
      "xquartz",
      "development",
      "github",
      "R"
    ],
    "contents": "\nI’ve been using GitHub actions to check and test some of my R packages on multiple architectures, which is nice because I work primarily on Linux.\nOne recent package uses the {Cairo} package to generate images that are subsequently used for visualization (Urbanek and Horner 2020).\nInterestingly, {Cairo} will install fine on MacOS, and then fail as soon as you do library(Cairo), complaining about not being able to load the {cairo.so} file.\nThankfully, it seems that the GitHub actions MacOS VMs have homebrew (“Homebrew” 2022) installed, which means we can use the xquartz brew (“Xquartz Homebrew Formulae” 2022) to install it, and have {cairo.so} available.\nTo add this to your GitHub actions yml, you should add these lines, and double check the brew syntax with the official docs.\n    steps:\n      - name: Install X11 dependencies on MacOS\n        if: runner.os == 'macOS'\n        run: |\n          brew install --cask xquartz\n\n\n\n“Homebrew.” 2022. https://brew.sh/.\n\n\nUrbanek, Simon, and Jeffrey Horner. 2020. Cairo: R Graphics Device Using Cairo Graphics Library for Creating High-Quality Bitmap (PNG, JPEG, TIFF), Vector (PDF, SVG, PostScript) and Display (X11 and Win32) Output. https://CRAN.R-project.org/package=Cairo.\n\n\n“Xquartz Homebrew Formulae.” 2022. https://formulae.brew.sh/cask/xquartz#default.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-06T09:33:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-06-dplyr-in-packages-and-global-variables/",
    "title": "Dplyr in Packages and Global Variables",
    "description": "How to include dplyr in a package, and avoid warnings around global variables.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2022-01-06",
    "categories": [
      "random-code-snippets",
      "packages",
      "dplyr",
      "rlang",
      "R"
    ],
    "contents": "\nI was recently cleaning up a development package so it would actually pass checks so it could be hosted on our labs new r-universe (“The ’Moseleybioinformaticslab’ Universe,” n.d.), and was getting warnings about global variables due to using {dplyr} operations, without {enquoting} variables.\nThere are a few approaches to handling this.\nUsing {utils::globalvariables} in an R file somewhere.\nUsing {rlang::.data} and importing it into the package.\nI went with option 2, see (“How to Solve ’No Visible Binding for Global Variables’,” n.d.).\nIn the original version of {dplyr} and other packages like {ggplot2}, there was an option to use “string” arguments, normally by calling the {*_str} or {*_} version of a function name.\nThat has gone out of fashion, and the way to do it now is to use the .data pronoun (“Programming with Dplyr,” n.d.).\nSo now the code looks like this:\n# an example filtering operation\ndplyr::filter(.data$varname == \"value\")\nThe easiest way to include this correctly in your package, is by importing the {rlang::.data}.\n#' @importFrom rlang .data\nAnd all the warnings should go away during package checking.\n\n\n\n“How to Solve ’No Visible Binding for Global Variables’.” n.d. https://community.rstudio.com/t/how-to-solve-no-visible-binding-for-global-variable-note/28887/3.\n\n\n“Programming with Dplyr.” n.d. https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html.\n\n\n“The ’Moseleybioinformaticslab’ Universe.” n.d. https://moseleybioinformaticslab.r-universe.dev/ui#builds.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-06T09:37:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-06-pie-charts-in-rcy3/",
    "title": "Pie Charts in RCy3",
    "description": "How to represent nodes as pie charts using Cytoscape and RCy3",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2022-01-06",
    "categories": [
      "random-code-snippets",
      "rcy3",
      "cyoscape",
      "R"
    ],
    "contents": "\nI have a package, categoryCompare2 (Flight 2022) that I’ve been working on for a while, and recently wanted to make available on our labs r-universe (“The ’Moseleybioinformaticslab’ Universe,” n.d.).\nFor some of the current visualization, we use Cytoscape to examine annotation similarity graphs, coupling the R together with Cytoscape via RCy3 (Gustavsen et al. 2019). In the previous iteration, we had used actual piechart images generated by R, and then used {setNodeImageDirect} to point the node images to local image files.\nHowever, the latest iteration of RCy3 has essentially lost that functionality. However, there is a new visualization plugin that can do similar things, enhancedGraphics (Morris JH 2014).\nAlexander Pico, one of the primary RCy3 developers, provided me with the guidance for a code solution (Pico 2022), which I’ve adapted below.\n\n\ngrp_matrix = matrix(c(1, 0,\n                      0, 1), nrow = 2, ncol = 2)\nn_grp <- nrow(grp_matrix)\nuse_color <- rainbow_hcl(ncol(grp_matrix), c = 100)\nn_color <- length(use_color)\nuse_color = color\n# defines how many pie segments are needed, common to all the pie-charts\npie_area <- rep(1 / n_color, n_color)\nnames(pie_area) <- rep(\"\", n_color)\n\n\ndesat_color <- desaturate(use_color)\nnames(desat_color) <- names(use_color)\npiechart_strings <- purrr::map_dfr(rownames(grp_matrix), function(i_grp){\n  tmp_logical <- grp_matrix[i_grp, ]\n  tmp_color <- use_color\n    \n  # add the proper desaturated versions of the colors\n  tmp_color[!tmp_logical] <- desat_color[!tmp_logical]\n    \n  out_str = paste0('piechart: attributelist=\"',\n                   paste(colnames(grp_matrix), collapse = ','),\n                   '\" ',\n                   'colorlist=\"',\n                   paste(tmp_color, collapse = ','),\n                   '\" ',\n                   'arcstart=-90 showlabels=false')\n  data.frame(colorlist = out_str)\n})\n\ntmp_matrix = as.data.frame(matrix(1, nrow = nrow(piechart_strings),\n                      ncol = ncol(grp_matrix)))\nnames(tmp_matrix) = colnames(grp_matrix) \npiechart_df = cbind(tmp_matrix, piechart_strings)\n\n# after merging this with the node information to\n# put the right things with the right node\n# this gives **node_vis_df** below\nRCy3::setNodeShapeDefault(\"ELLIPSE\")\nRCy3::setNodeSizeDefault(35)\nRCy3::loadTableData(node_vis_df, data.key.column = \"name\", table = \"node\", table.key.column = \"name\")\nRCy3::updateStyleMapping(\"default\",\n   RCy3::mapVisualProperty(\"NODE_CUSTOMGRAPHICS_1\", \"colorlist\", \"p\"))\n\n\n\n\n\n\nFlight, Robert M. 2022. “categoryCompare2.” https://github.com/MoseleyBioinformaticsLab/categoryCompare2.\n\n\nGustavsen, Julia A., Pai, Shraddha, Isserlin, Ruth, Demchak, Barry, Pico, and Alexander R. 2019. “RCy3: Network Biology Using Cytoscape from Within r.” F1000Research. https://doi.org/10.12688/f1000research.20887.3.\n\n\nMorris JH, Ferrin TE, Kuchinsky A. 2014. “enhancedGraphics: A Cytoscape App for Enhanced Node Graphics.” F1000Research. https://doi.org/10.12688/f1000research.4460.1.\n\n\nPico, Alexander. 2022. “RCy3 GitHub Issue: Equivalent of setNodeImageDirect.” https://github.com/cytoscape/RCy3/issues/167#issuecomment-1004467137.\n\n\n“The ’Moseleybioinformaticslab’ Universe.” n.d. https://moseleybioinformaticslab.r-universe.dev/ui#builds.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-06T09:34:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-21-reusing-ggplot2-colors/",
    "title": "Reusing ggplot2 Colors",
    "description": "When you want to reuse ggplot2 default colors across plots.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-12-21",
    "categories": [
      "random-code-snippets",
      "ggplot2",
      "visualization"
    ],
    "contents": "\nIf you are using {ggplot2} a lot in your code, and don’t want to bother with a custom color scheme (I mean, there are lots of good options), but you are also using non-ggplot2 type plots, and want to reuse the colors.\nI frequently encounter this when I’m doing principal component analysis (PCA) combined with heatmaps plotted by {ComplexHeatmap}. I’ve colored sample groups in the PCA using {ggplot2} defaults, and now I want those same colors in the {ComplexHeatmap} row or column annotations.\nThe trick to this is extracting the palette from the color definition you want. For example, very commonly (in biology at least) we might have 2 classes of samples, cases and controls. So we need 2 colors.\nWhen we generated the {ggplot2} plot, we would do something like this:\n\n\nggplot(data_frame, aes(x = var1, y = var2, color = groups))\n\n\n\nTo generate the matching colors for something else, we can do:\n\n\ng_colors = scale_color_discrete()$palette(2)\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-21T08:58:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-25-migrating-self-hosted-gitlab-to-github/",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "description": "We wanted to migrate from self-hosted GitLab projects to GitHub repos. Here is some background on how we accomplished that.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-11-25",
    "categories": [
      "version-control",
      "github",
      "gitlab"
    ],
    "contents": "\n\nContents\nMoving to GitHub\nGetting the List of Projects\nUsers to IDs\nLocal Clones\nDuplicate Projects\nEmpty Projects\nCreating GitHub Repos\n190 Repos Later\n\nMoving to GitHub\nSix years ago, we didn’t really look into what the free academic version of GitHub teams provided for private repositories, and instead decided that we should have a self-hosted GitLab instance. Outside of some weird issues around SSL certificates and reverse proxies (i.e. how our web-hosting worked), this actually worked pretty well. However, instead of making our internal projects public, we would frequently move them over to GitHub. I think this is partly because GitHub has become the de-facto location for open-source tool development. However, it seemed to cause some friction to push things between the two platforms. This may be due to our lab not using git as much as we should be, especially the collaborative aspects in issues and pull requests.\nAfter six years, our PI decided to look into the costs for GitHub teams, and discovered that academic teams get free private repos with rather generous amounts of storage and actions minutes. Therefore, we decided it was time to migrate our projects over to GitHub. The PI delegated this task to yours truly.\nGetting the List of Projects\nTo get the list of projects to migrate, I actually copied the project lists from the web interface. If I was doing this again, I would have just asked for the backup locations of our GitLab projects and used those to get the paths for each project. Some projects had changed their names over time and not updated the corresponding path in the database.\nUsers to IDs\nBecause I used lists from the web interface, I also had to convert the users to actual IDs that define the paths to each project. These weren’t too hard to look up.\nLocal Clones\nWith a full list of projects as “User Name / Project”, and users to IDs, I could easily script converting users to IDs and the full project path, and then system calls to GIT_SSL_NO_VERIFY=TRUE git clone --mirror gitlab_url to make mirror clones of each project. GIT_SSL_NO_VERIFY=TRUE deals with our labs weird SSL certificate issues, while using --mirror means all the branches are pulled, not just the default branch. This is useful to maintain a history of branches over the course of the project development.\nNote that this means you get a directory with “projectName.git”, not just “projectName”. If you look inside the “projectName.git” folder, it’s actually all of the git contents, not the usual files you would see for a normal clone operation.\nDuplicate Projects\nOne thing I didn’t think about when doing this is that users on GitLab can fork each others projects, resulting in duplicate projects. If I had been smart, I would have cloned the projects into user specific directories. Thankfully, we only had a few duplicate projects where I had to do a second clone to a user specific directory.\nEmpty Projects\nSome of the projects are actually empty, either because we thought we were going to do something and didn’t, or someone just wanted issue tracking (that would be me). These we don’t want to push back up to GitHub. However, empty projects with no commits appear to have less than two files in the objects directory, and zero files in the objects/pack directory.\nCreating GitHub Repos\nI used the GitHub command-line-interface tools (gh) to interface with our GitHub organization. After authorizing the gh tools, for each GitLab project I created a corresponding GitHub project, and then moved to the local clone, and did\ngithub_locs = \"/path/to/github/repos\"\npurrr::walk(gitlab_project_dirs, function(in_dir){\n  proj_name = basename(in_dir)\n  setwd(github_locs)\n  gh_command = paste0(\"gh repo create --confirm --private orgName/\", proj_name)\n  system(gh_command)\n  setwd(in_dir)\n  gh_loc = paste0(\"git@github.com/orgName/\", proj_name)\n  git_push = paste0(\"git push --mirror \", gh_loc)\n  system(git_push)\n})\nYou can see here I move back and forth between directories a lot, and used system to run gh and git push. It feels a little hacky, but it worked just fine.\n190 Repos Later\nI moved over 180 GitLab projects to GitHub repos. With the projects that already had GitHub repos, that means we have over 190 lab repos. That doesn’t count the stuff we left related to testing CI/CD, people learning how to work with GitLab, and 20 copies of the code we use for sys-admin. There were a subset I had to do by hand because of replication issues and not using user directories. I also discovered some missing repos due to a missed letter when copying one lab members user ID.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-06T09:30:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-25-recreating-correlation-values-from-another-manuscript/",
    "title": "Recreating Correlation Values from Another Manuscript",
    "description": "Documenting my journey trying to recreate some correlations calculated in another manuscript.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-11-25",
    "categories": [
      "reproducibility"
    ],
    "contents": "\n\nContents\nBackground\nSo What\nMy Correlations\nFinding the Data\nNew Correlation\nWhy Not Logs?\nHow Did I Miss the Original Data?\n\nBackground\nI’ve been working on a manuscript for our newish correlation method, information-content-informed Kendall-tau (ICI-Kendalltau). As part of that manuscript, we wanted to highlight how well the correlation measure detects outlier samples. Two datasets we are using for that aspect are public, one from The Cancer Genome Atlas and the other from the Barton group. The Barton group and collaborators produced a highly replicated RNAseq yeast dataset, 48 replicates in two conditions, and have used it in various analyses (Gierliński et al. 2015).\nSo What\nFor my manuscript, ideally I want to be able to comment on the outliers found in one of the original Barton group manuscripts (Gierliński et al. 2015). To do that, I need access to one of:\nthe sample-sample correlations themselves,\nthe counts from each sample\nor be able to recreate counts from each sample.\nSeveral years ago I was asking about this dataset on twitter when I was using it for another project, because the project ID in the SRA didn’t have a mapping to condition. Dr. Geoff Barton pointed me to a metadata file available on figshare (Barton, Blaxter, Cole, Gharbi, Gierliński, Owen-Hughes, et al. 2015). This allowed me to generate read counts across biological replicates. However, if I had poked around figshare a bit more, I would have likely seen both of the files with read counts for each replicate already submitted (Barton, Blaxter, Cole, Gharbi, Gierliński, Schofield, et al. 2015a, 2015b). It really sucks that I hadn’t noticed these other files years ago, it would have saved me the effort in remapping and generating gene counts myself, as well as getting really weird correlation values compared to (Gierliński et al. 2015).\nMy Correlations\nFrom the demultiplexed read data and the metadata file I found several years ago, I had run software to generate read counts for each sample in each lane, and summed them across lanes. I know how to do these kinds of things, even though I will be the first to admit it is not my personal bread and butter analysis. (I normally get involved after count generation). Even with hints from the manuscript about how (Gierliński et al. 2015) did the correlation calculation, I don’t get anything close to the range of median correlation values within each class of samples. Which I thought was very, very weird. I implemented a variety of transformation methods and inclusion of missing values for my correlation calculations:\nlogged values (log and log1p)\nraw values\nremoval and inclusion of missing (0) values\nEven with all of these variations, I could not come close to the same values I found presented in their manuscript. Here, in panel (a) of Figure 2 from (Gierliński et al. 2015), the median correlations range from 0.7 to 1.\n\n\n\nFigure 1: Figure 2(a) from (Gierliński et al. 2015). Identifying bad RNA-seq replicates in the WT (left) and Δsnf2 (right) data. The top three panels (a–c) show individual criteria for identifying ‘bad’ replicates which are combined into a quality score (d) in order to identify ‘bad’ replicates in each condition. The identified ‘bad’ replicates are shown as numbered points in each panel. The individual criteria are (a) median correlation coefficient, ri∼⁠, for each replicate i against all other replicates, (b) outlier fraction, fi⁠, calculated as a fraction of genes where the given replicate is more than five-trimmed standard deviations from the trimmed mean and (c) median reduced χ2 of pileup depth, χ∼2i⁠, as a measure of the non-uniformity of read distribution within genes (see also Fig. 3)\n\n\n\nFigure with their graph, and my values using my data.\nFinding the Data\nFinally, during the week of 2021-11-25, I happened across another manuscript on this dataset from 2016, that mentions a GitHub repo that lo and behold had copies of the preprocessed data to the level of gene counts per biological replicate. Awesome!\nNew Correlation\nUsing the preprocessed data, I was finally able to get what amounted to identical values of correlation, based on comparing the lowest correlation values with the figure. Great.\nWhy Not Logs?\nOne interesting thing about having the correct correlations is discovering that Gierlinski et al didn’t use log-transformed data in their Pearson correlation calculations. This seems unusual to me. All my career in -omics, the one thing I’ve had drilled into me is that doing a linear correlation on data that has proportional error, i.e. the variance increases with increasing values, which is definitely true of these count data.\nIf I had to guess why log-transformed values weren’t used, is because of the analysis in the replicate paper about how well the un-transformed values fit a normal distribution vs a log-normal distribution. That figure and caption are provided here for reference.\n\n\n\nFigure 2: Figure 5 from (Gierliński et al. 2015). Goodness-of-fit test results for normal (top panels), log-normal (middle panels) and negative binomial (bottom panels) distributions. Each panel shows the test P-value versus the mean count across replicates. Each dot represents equal-count normalized data from one gene. Panels on the left (a, b, e, f, i, j) show clean data with bad replicates rejected (42 and 44 replicates remaining in WT and Δsnf2, respectively). Panels on the right (c, d, g, h, k, l) show all available data (48 replicates in each condition). Due to the number of bootstraps performed, P-values for the negative-binomial test are limited to ∼10−2. Due to numerical precision of the software library used, P-values from the normal and log-normal tests are limited to ∼10−16. Below these limits data points are marked in orange (light gray in black and white) at the bottom of each panel. Horizontal lines show the Benjamini–Hochberg limit corresponding to the significance of 0.05 for the given dataset. The numbers in the right bottom corner of each panel indicate the number of genes with P-values below the significance limit and the total number of genes\n\n\n\nHow Did I Miss the Original Data?\nIt turns out, I didn’t look hard enough at the first place that Geoff Barton sent me to. When I started poking around the figshare repo’s from Geoff and others in the group and following links, it was easy to find a copy of the preprocessed data from each condition.\n\n\n\nBarton, Geoffrey, Mark Blaxter, Christian Cole, Karim Gharbi, Marek Gierliński, Tom Owen-Hughes, Pieta Schofield, et al. 2015. “Metadata for a highly replicated two-condition yeast RNAseq experiment.” July. https://doi.org/10.6084/m9.figshare.1416210.v3.\n\n\nBarton, Geoffrey, Mark Blaxter, Christian Cole, Karim Gharbi, Marek Gierliński, Pieta Schofield, Nick Schurch, Vijender Singh, and Nicola Wrobel. 2015a. “SNF2 knock-out yeast gene read counts from a 48 replicate experiment,” May. https://doi.org/10.6084/m9.figshare.1425502.v1.\n\n\n———. 2015b. “Wild-type yeast gene read counts from 48 replicate experiment,” May. https://doi.org/10.6084/m9.figshare.1425503.v1.\n\n\nGierliński, Marek, Christian Cole, Pietà Schofield, Nicholas J. Schurch, Alexander Sherstnev, Vijender Singh, Nicola Wrobel, et al. 2015. “Statistical models for RNA-seq data derived from a two-condition 48-replicate experiment.” Bioinformatics 31 (22): 3625–30. https://doi.org/10.1093/bioinformatics/btv425.\n\n\n\n\n",
    "preview": "posts/2021-11-25-recreating-correlation-values-from-another-manuscript/gierlinski_2015_fig2_cropped.png",
    "last_modified": "2021-12-31T22:15:46-05:00",
    "input_file": {},
    "preview_width": 1900,
    "preview_height": 453
  },
  {
    "path": "posts/2021-11-11-zooming-ggraph-plots/",
    "title": "Zooming GGraph Plots",
    "description": "Some code demonstrating how to zoom into portions of a ggraph.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-11-11",
    "categories": [
      "graphing",
      "ggraph",
      "visualization",
      "random-code-snippets"
    ],
    "contents": "\n\nContents\nInspiration\nSetup\nInitial View\nFind Those Nodes\nInitial Layout\nSubset Nodes and Get Range\nPlot Subset\nPlot Using Subset Ranges\n\nInspiration\nI was recently working with a largish graph that I am using the ggraph package, and I needed to zoom into a sub-region of the graph.\nSetup\n\n\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.width = 10, fig.height = 8)\nexample_df = structure(list(from = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 7L, 7L, 7L, \n                                     7L, 7L, 7L, 7L, 7L, 7L, 8L, 9L, 10L, 11L, 12L, 12L, 12L, 13L, \n                                     13L, 13L, 13L, 13L, 13L, 13L, 14L, 15L, 16L, 16L, 17L, 17L, 18L, \n                                     18L, 19L, 20L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, \n                                     21L, 21L, 22L, 22L, 6L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, \n                                     23L, 23L, 23L, 23L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, \n                                     24L, 24L, 24L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, \n                                     25L, 25L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, \n                                     26L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, \n                                     28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 29L, \n                                     29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 30L, 30L, \n                                     30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 31L, 31L, 31L, \n                                     31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 32L, 32L, 32L, 32L, \n                                     32L, 32L, 32L, 32L, 32L, 32L, 32L, 32L, 33L, 33L, 33L, 33L, 33L, \n                                     33L, 33L, 33L, 33L, 33L, 33L, 33L, 34L, 34L, 34L, 34L, 34L, 34L, \n                                     34L, 34L, 34L, 34L, 34L, 34L, 35L, 35L, 35L, 35L, 35L, 35L, 35L, \n                                     35L, 35L, 35L, 35L, 35L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, \n                                     36L, 36L, 36L, 36L, 37L, 37L, 37L, 37L, 37L, 37L, 37L, 37L, 37L, \n                                     37L, 37L, 37L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, \n                                     38L, 38L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, \n                                     39L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, \n                                     41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 42L, \n                                     22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, \n                                     22L, 22L, 22L, 22L, 43L, 43L, 44L, 44L, 45L, 45L, 46L, 46L, 47L, \n                                     47L, 48L, 48L, 49L, 49L, 50L, 50L, 51L, 52L, 52L, 53L, 53L, 54L, \n                                     54L, 55L, 55L, 56L, 57L, 57L, 58L, 58L, 59L, 60L, 61L, 61L, 62L, \n                                     16L, 17L, 18L, 63L, 64L, 65L, 66L, 67L, 68L, 69L, 70L, 71L, 72L, \n                                     73L, 74L, 75L, 76L, 20L, 77L, 77L, 78L, 79L, 79L, 80L, 81L, 82L, \n                                     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 14L, 15L, 19L, 21L, 21L, \n                                     22L, 83L, 84L, 45L, 46L, 47L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, \n                                     56L, 57L, 58L, 59L, 85L, 48L, 48L, 48L, 48L, 48L, 46L, 46L, 46L, \n                                     46L, 86L, 86L, 13L, 13L, 22L, 7L, 55L, 55L, 55L, 55L, 55L, 51L, \n                                     51L, 51L, 58L, 58L, 58L, 58L, 53L, 53L, 53L, 53L, 49L, 49L, 49L, \n                                     49L, 87L, 87L), to = c(7L, 7L, 7L, 61L, 61L, 61L, 8L, 88L, 89L, \n                                                            90L, 91L, 92L, 93L, 94L, 95L, 96L, 9L, 7L, 9L, 9L, 16L, 17L, \n                                                            18L, 14L, 15L, 16L, 17L, 18L, 19L, 10L, 10L, 10L, 10L, 42L, 10L, \n                                                            42L, 10L, 42L, 10L, 12L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, \n                                                            54L, 55L, 57L, 58L, 6L, 9L, 9L, 45L, 46L, 47L, 48L, 49L, 50L, \n                                                            52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, \n                                                            53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, \n                                                            54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, \n                                                            55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, \n                                                            57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, \n                                                            58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, \n                                                            45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, \n                                                            46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, \n                                                            47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, \n                                                            48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, \n                                                            49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, \n                                                            50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, \n                                                            52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, \n                                                            53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, \n                                                            54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, \n                                                            55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, \n                                                            57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, \n                                                            58L, 9L, 97L, 98L, 99L, 100L, 101L, 4L, 5L, 102L, 10L, 103L, \n                                                            104L, 105L, 60L, 106L, 107L, 108L, 109L, 13L, 22L, 110L, 83L, \n                                                            21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 44L, \n                                                            21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 44L, 21L, 44L, 21L, 44L, \n                                                            44L, 111L, 111L, 112L, 60L, 83L, 83L, 83L, 22L, 22L, 22L, 22L, \n                                                            22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 83L, 83L, 110L, \n                                                            83L, 83L, 110L, 83L, 83L, 22L, 22L, 113L, 114L, 115L, 116L, 117L, \n                                                            118L, 119L, 120L, 121L, 122L, 83L, 83L, 83L, 86L, 2L, 123L, 78L, \n                                                            21L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, \n                                                            84L, 84L, 10L, 79L, 124L, 86L, 125L, 84L, 79L, 124L, 86L, 125L, \n                                                            110L, 83L, 42L, 11L, 126L, 111L, 48L, 79L, 124L, 86L, 125L, 79L, \n                                                            86L, 125L, 79L, 124L, 86L, 125L, 79L, 124L, 86L, 125L, 79L, 124L, \n                                                            86L, 125L, 46L, 48L)), class = \"data.frame\", row.names = c(NA, \n                                                                                                                       -432L))\n\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(igraph)\nexample_graph = as_tbl_graph(example_df)\n\n\n\nInitial View\nLet’s generate a plot of this to start.\n\n\nset.seed(1234)\nggraph(example_graph, \"graphopt\") +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point()\n\n\n\n\nYou can see the bottom left section of this graph looks like a mess. It sure would be awesome if we could zoom into that region, and replot it.\nFind Those Nodes\nWe will use igraphs community detection to break this into community sets and select the correct group.\n\n\nwalk_membership = cluster_walktrap(example_graph)\nwalk_communities = membership(walk_membership)\nsplit_comms = split(names(walk_communities), walk_communities)\nnames(split_comms) = NULL\n\nwhich_comm = split_comms[[which(purrr::map_lgl(split_comms, ~ \"58\" %in% .x))]]\n\n\n\nInitial Layout\nOne way to make sure we can get the same layout is to create the layout, which is treated as a graph by ggraph.\n\n\n# we set the seed so its the same layout as previous\nset.seed(1234)\nexample_layout = create_layout(example_graph, \"graphopt\")\n\nggraph(example_layout) +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point()\n\n\n\n\nAwesome, looks the same as the previous.\nSubset Nodes and Get Range\n\n\nsub_layout = example_layout %>%\n  dplyr::filter(name %in% which_comm)\nsub_xlim = range(sub_layout$x)\nsub_ylim = range(sub_layout$y)\n\n\n\nPlot Subset\nWe would think we should be able to just use the sub-layout to plot the subset of nodes. Lets try that first.\n\n\nggraph(sub_layout) +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point()\n\n\n\n\nAs we can see, this isn’t quite right. We are missing some of the edges.\nPlot Using Subset Ranges\n\n\nggraph(example_layout) +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point() +\n  coord_cartesian(xlim = sub_xlim, ylim = sub_ylim)\n\n\n\n\nFinally! This is what we wanted. And we can see why it’s so overlapped in the full plot! It’s pretty much a hairball in that part of the graph.\n\n\n\n",
    "preview": "posts/2021-11-11-zooming-ggraph-plots/zooming-ggraph-plots_files/figure-html5/plot_full-1.png",
    "last_modified": "2021-11-11T21:57:12-05:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1536
  },
  {
    "path": "posts/2021-10-27-r-cmd-check-and-non-pdf-vignettes/",
    "title": "R CMD Check and Non-PDF Vignettes",
    "description": "R CMD Check complaining about missing files? Here was my solution.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-10-27",
    "categories": [
      "packages",
      "random-code-snippets"
    ],
    "contents": "\n\nContents\nR CMD Check\nMissing Files\n\nR CMD Check\nIf you’ve written an R package that you want hosted by CRAN (or even if not hosted), then you generally want to run the infamous R CMD check on your package.\nAlthough it can be a pain in the butt, it has a wide array of checks that make sense (and some that don’t, just look at posts on the R-devel list).\nMissing Files\nIf you are using rmarkdown to generate non-standard vignettes, or even using rmarkdown because of features that aren’t present directly in knitr, then the check process may fail after it re-builds the package. You may see this message:\nWarning: Files in the 'vignettes' directory but no files in 'inst/doc':\n  ‘vignette_file.Rmd’\nThe solution, as far as I can tell, is to:\nadd both knitr and rmarkdown to the Suggests field of DESCRIPTION.\nadd VignetteBuilder: knitr to the DESCRIPTION.\nadd %\\VignetteEngine{knitr::rmarkdown} to the vignette meta-data.\nuse output: … whatever actual output document type you wanted in the vignette itself.\nI think that the build command only recognizes the knitr engine properly, and that is the only one that builds the vignettes and shoves them in the right location.\nOtherwise, you should build them manually and double check the options to build and check so that vignettes don’t get rebuilt during either build or check.\nDepending on your needs, you might want to use something like Drew Schmidt’s approach (“Vignette-Less Articles with Pkgdown” 2020).\n\n\n\n“Vignette-Less Articles with Pkgdown.” 2020. https://fml-fam.github.io/blog/2020/06/23/vignette-less-articles-with-pkgdown/.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-27T10:58:30-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-20-my-vacation-in-barometric-pressure/",
    "title": "My Vacation in Barometric Pressure",
    "description": "What can we see from my phone's barometric pressure readings?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [
      "graphing",
      "maps",
      "visualization"
    ],
    "contents": "\n\nContents\nThe What?\nLoad Data\nInitial Plot\nAnnotations\nConclusions\n\nThe What?\nI recently took a proper vacation, that involved driving from Lexington, KY, USA to Digby, NS, Canada and all points in between. I also have an app, Barometer Reborn, on my phone that measures the air pressure every 15 minutes or so. This data is useful for someone who suffers from pressure induced migraines. I decided it would be interesting to examine the air pressure readings over the course of my vacation, given that we crossed an extremely wide variety of terrain (and weather) in our travels.\nSo lets see what kinds of things we can see. A big caveat with this data is that we are essentially recording local pressure, a combination of pressure changes from both elevation and weather changes, as I don’t have the correction for elevation turned on. I think it will still be interesting to examine whats in here.\nLoad Data\nI’ve previously uploaded data to Google Drive from my phone and downloaded it. Although it says “csv”, it turns out it’s actually tab-separated. I also do the conversion to a date-time format explicitly here, because it was easier than parsing the date format that the app uses.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggforce)\ntheme_set(cowplot::theme_cowplot())\nbarometer_readings = read.table(here::here(\"barometer_2021-09-17.csv\"), sep = \"\\t\", header = TRUE) %>%\n  dplyr::mutate(pressure = Pressure..mBar.,\n                datetime = lubridate::as_datetime(Timestamp..Unix.Time. / 1000, tz = \"America/New_York\")) %>%\n  dplyr::filter(datetime >= \"2021-08-21\", datetime <= \"2021-09-15\")\n\n\n\nInitial Plot\nWe can plot the whole thing, with the pressure at sea level as well.\n\n\nggplot(barometer_readings, aes(x = datetime,\n                               y = pressure)) +\n  geom_line(size = 1.5) +\n  geom_hline(yintercept = 1013.25, color = \"red\", size = 1.5, alpha = 0.5) +\n  labs(x = \"Date\", y = \"Pressure (mBar)\")\n\n\n\n\nThis is kinda cool. At both ends it’s easy to see the passage there and back through the mountains (not big ones, but still)! It’s also easy to see that outside of those, I spent a lot of time near sea level, which is expected from Bangor, ME on.\nAnnotations\nLet’s annotate it! We can come up with some simple annotations, like the driving sections, and then when we were in a location, and add those to the plot. The (1) and (2) labels are to keep from having weird plot artifacts appearing in the line plot.\n\n\nbarometer_readings = barometer_readings %>%\n  dplyr::mutate(location =\n   dplyr::case_when(\n     \n     datetime <= \"2021-08-22 08:00\" ~ \"Lexington - Bangor\",\n     datetime <= \"2021-08-23 08:00\" ~ \"Waiting on Covid Test\",\n     datetime <= \"2021-08-23 19:00\" ~ \"Bangor - Digby\",\n     datetime <= \"2021-08-28 08:00\" ~ \"Digby 1\",\n     datetime <= \"2021-08-28 14:00\" ~ \"Daytrip to Liverpool\",\n     datetime <= \"2021-09-03 10:00\" ~ \"Digby 2\",\n     datetime <= \"2021-09-03 16:30\" ~ \"Digby - Burton\",\n     datetime <= \"2021-09-08 09:00\" ~ \"Burton 1\",\n     datetime <= \"2021-09-08 20:00\" ~ \"Daytrip to Moncton\",\n     datetime <= \"2021-09-13 08:00\" ~ \"Burton 2\",\n     datetime <= \"2021-09-15\" ~ \"Burton - Lexington\"\n   ),\n   day = lubridate::as_date(datetime))\n\n\n\n\n\nbig_plot = ggplot(barometer_readings, aes(x = datetime, y = pressure, color = location)) + \n  geom_line(size = 1.5) +\n  theme(legend.position = c(0.2, 0.3)) +\n  labs(x = \"Date\", y = \"Pressure (mBar)\")\nbig_plot\n\n\n\nbig_plot2 = big_plot + theme(legend.position = \"none\")\n\n\n\nWe can plot some of these as subsets and zoom in on them.\n\n\nbig_plot2 + \n  facet_zoom(x = location %in% c(\"Lexington - Bangor\", \"Waiting on Covid Test\", \"Bangor - Digby\")) +\n  labs(caption = \"Going from Lexington to Bangor, awaiting test results, and then driving to Digby.\")\n\n\n\n\n\n\nbig_plot2 +\n  facet_zoom(x = location %in% c(\"Digby 1\", \"Daytrip to Liverpool\", \"Digby 2\")) +\n  labs(caption = \"Digby, with a trip across land to Liverpool.\")\n\n\n\n\n\n\nbig_plot2 +\n  facet_zoom(x = location %in% c(\"Digby - Burton\", \"Burton 1\", \"Daytrip to Moncton\", \"Burton 2\")) +\n  labs(caption = \"Traveling from Digby to Burton, a daytrip to Moncton, and remainder in Burton.\")\n\n\n\n\nYou can actually see towards the tail end of our stay in Burton, the big increase in local pressure from a high-front that gave me a massive migraine and kept me home from activities with the family. Thankfully that was the only one.\n\n\nbig_plot2 +\n  facet_zoom(x = as.character(day) %in% c(\"2021-09-10\", \"2021-09-11\")) +\n  labs(caption = \"Migraine day, stayed home.\")\n\n\n\n\n\n\nbig_plot2 +\n  facet_zoom(x = location %in% c(\"Burton - Lexington\")) +\n  labs(caption = \"Traveling home to Lexington from Burton over 2 days.\")\n\n\n\n\nConclusions\nI dunno, honestly. This would probably be more interesting if it was one or the other of changes due to altitude, or due to changes in the weather with an adjustment for the altitude. Many of the dips are due to changes occurring because of a change in altitude, with the biggest ones noted as we drove through the Appalachian mountains.\n\n\n\n",
    "preview": "posts/2021-09-20-my-vacation-in-barometric-pressure/my-vacation-in-barometric-pressure_files/figure-html5/plot_them-1.png",
    "last_modified": "2021-09-20T21:38:43-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-30-random-forest-classification-using-parsnip/",
    "title": "Random Forest Classification Using Parsnip",
    "description": "How to make sure you get a classification fit and not a probability fit from a random forest model using the tidymodels framework.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-08-30",
    "categories": [
      "parsnip",
      "tidymodels",
      "machine-learning",
      "random-forest",
      "random-code-snippets"
    ],
    "contents": "\nI’ve been working on a “machine learning” project, and in the process I’ve been learning to use the tidymodels framework (“Tidymodels” 2021), which helps save you from leaking information from testing to training data, as well as creating workflows in a consistent way across methods.\nHowever, I got tripped up recently by one issue. When I’ve previously used Random Forests (“Random Forest Wiki Page” 2021), I’ve found that for classification problems, the out-of-bag (OOB) error reported is a good proxy for the area-under-the-curve (AUC), or estimate of how good any other machine learning technique will do (see (Flight 2015) for an example using actual random data). Therefore, I like to put my data through a Random Forest algorithm and check the OOB error, and then maybe reach for a tuned boosted tree to squeeze every last bit of performance out.\ntidymodels default is to use a probability tree, even for classification problems. This isn’t normally a problem for most people, because you will have a train and test set, and estimate performance on the test set using AUC. However, it is a problem if you just want to see the OOB error from the random forest, because it is reported differently for probability vs classification.\nLets run an example using the tidymodels cell data set.\n\n\nlibrary(tidymodels)\nlibrary(modeldata)\nlibrary(skimr)\ndata(cells, package = \"modeldata\")\nlibrary(ranger)\ntidymodels_prefer()\n\ncells$case = NULL\nset.seed(1234)\nranger(class ~ ., data = cells, min.node.size = 10, classification = TRUE)\n\n\nRanger result\n\nCall:\n ranger(class ~ ., data = cells, min.node.size = 10, classification = TRUE) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             17.29 % \n\nHere we can see that we get an OOB error of 17%, which isn’t too shabby. Now, let’s setup a workflow to do the same thing via tidymodels parsnip.\n\n\nrf_spec = rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nrf_recipe = recipe(class ~ ., data = cells) %>%\n  step_dummy(class, -class)\n\nset.seed(1234)\nworkflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_spec) %>%\n  fit(data = cells)\n\n\n══ Workflow [trained] ════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ──────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1198456 \n\nHere we see the OOB error is 12% (0.119), which is not significantly different than the 17% above, but still different. Also, the “Type” shows “Probability estimation” instead of “Classification estimation.”\nIf we run ranger again with a “probability” instead of “classification,” do we match up with the result above?\n\n\nset.seed(1234)\nranger(class ~ ., data = cells, min.node.size = 10, probability = TRUE)\n\n\nRanger result\n\nCall:\n ranger(class ~ ., data = cells, min.node.size = 10, probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.119976 \n\nThat is much closer to the tidymodels result! Great! Except, it’s a misestimation of the true OOB error for classification. How do we get what we want while using the tidymodels framework?\nI couldn’t find the answer, and the above looked like a bug, so I filed one on the parsnip github (“Ranger \"Classification\" Mode Still Looks Like \"Probability\"” 2021). Julia Silge helpfully provided the solution to my problem.\n\n\nrf_spec_class = rand_forest() %>%\n  set_engine(\"ranger\", probability = FALSE) %>%\n  set_mode(\"classification\")\n\nset.seed(1234)\nworkflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_spec_class) %>%\n  fit(data = cells)\n\n\n══ Workflow [trained] ════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ──────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, probability = ~FALSE,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             16.54 % \n\nAha! Now we are much closer to the original value of 17%, and the “Type” is “Classification.”\nI know in this case, the differences in OOB error are honestly not that much different, but my recent project, they differed by 20%, where I had a 45% using classification, and 25% using probability. Therefore, I was being fooled by the tidymodels framework investigation, and then wondering why my final AUC on a tuned model was only hitting just > 55%.\nSo remember, this isn’t how I would run the model for final classification and estimation of AUC on a test set, but if you want the OOB errors for a quick “feel” of your data, it’s very useful.\n\n\n\nFlight, Robert M. 2015. “Deciphering Life: One Bit at a Time: Random Forest Vs PLS on Random Data.” https://rmflight.github.io/posts/2015-12-12-random-forest-vs-pls-on-random-data/.\n\n\n“Random Forest Wiki Page.” 2021. https://en.wikipedia.org/wiki/Random_forest.\n\n\n“Ranger \"Classification\" Mode Still Looks Like \"Probability\".” 2021. https://github.com/tidymodels/parsnip/issues/546.\n\n\n“Tidymodels.” 2021. https://www.tidymodels.org/.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-30T14:56:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-03-coloring-dendrogram-edges-with-ggraph/",
    "title": "Coloring Dendrogram Edges with ggraph",
    "description": "Here is how I got edges colored in a dendrogram with ggraph. Use \"node.\" in front of the node data column you want.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-08-03",
    "categories": [
      "random-code-snippets",
      "visualization",
      "graphing",
      "dendrogram"
    ],
    "contents": "\nI wanted to color the dendrogram edges according to their class in ggraph, and I was getting stuck because of something that isn’t explicitly mentioned in the documentation, but is implied. You must use “node.” to access the data from the Node Data in the call to aes(...).\nLets set it up. We will borrow from the “Edges” vignette in the ggraph package (Pederson 2021).\n\n\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(purrr)\nlibrary(rlang)\n\nset_graph_style(plot_margin = margin(1,1,1,1))\nhierarchy <- as_tbl_graph(hclust(dist(iris[, 1:4]))) %>% \n  mutate(Class = map_bfs_back_chr(node_is_root(), .f = function(node, path, ...) {\n    if (leaf[node]) {\n      as.character(iris$Species[as.integer(label[node])])\n    } else {\n      species <- unique(unlist(path$result))\n      if (length(species) == 1) {\n        species\n      } else {\n        NA_character_\n      }\n    }\n  }))\n\nhierarchy\n\n\n# A tbl_graph: 299 nodes and 298 edges\n#\n# A rooted tree\n#\n# Node Data: 299 × 5 (active)\n  height leaf  label members Class    \n   <dbl> <lgl> <chr>   <int> <chr>    \n1  0     TRUE  \"108\"       1 virginica\n2  0     TRUE  \"131\"       1 virginica\n3  0.265 FALSE \"\"          2 virginica\n4  0     TRUE  \"103\"       1 virginica\n5  0     TRUE  \"126\"       1 virginica\n6  0     TRUE  \"130\"       1 virginica\n# … with 293 more rows\n#\n# Edge Data: 298 × 2\n   from    to\n  <int> <int>\n1     3     1\n2     3     2\n3     7     5\n# … with 295 more rows\n\nAnd with this, we can create a dendrogram.\n\n\nggraph(hierarchy, \"dendrogram\", height = height) +\n  geom_edge_elbow()\n\n\n\n\nNice! But what if we want the leaves colored by which “Class” they belong to?\n\n\nggraph(hierarchy, \"dendrogram\", height = height) +\n  geom_edge_elbow2(aes(color = node.Class))\n\n\n\n\nNote the differences in this function call compared to the previous:\nUsing geom_edge_elbow2 instead of geom_edge_elbow\nUsing node.Class, not just Class.\nThe second point is really important! When you look at the hierarchy object printed above, the Class bit is part of the Node Data, which gets identified by ggraph by the prefix “node.”\nIf we don’t use node.Class, here is the error:\n\n\nggraph(hierarchy, \"dendrogram\", height = height) +\n  geom_edge_elbow2(aes(color = Class))\n\n\nError in FUN(X[[i]], ...): object 'Class' not found\n\n\n\n\n\nPederson, Thomas Lin. 2021. “Edges.” https://cran.r-project.org/web/packages/ggraph/vignettes/Edges.html.\n\n\n\n\n",
    "preview": "posts/2021-08-03-coloring-dendrogram-edges-with-ggraph/coloring-dendrogram-edges-with-ggraph_files/figure-html5/plot_dendrogram-1.png",
    "last_modified": "2021-08-03T11:04:22-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-07-22-keep-figures-in-a-word-document/",
    "title": "Keeping Figures in an Rmd -- Word Document",
    "description": "How to make your rmarkdown to word conversion also generate a directory of figures.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-07-22",
    "categories": [
      "random-code-snippets",
      "reproducibility",
      "rmarkdown"
    ],
    "contents": "\nI’ve been working with a lot of collaborators who expect Word documents lately. I don’t really like it, but it makes it a lot easier for them to edit and work with things back and forth. So I’ve really been working on generating nice output in the Word document.\nRecently, someone brought up collecting the figures for a manuscript for submission. For those in the know, when you submit a manuscript, you often submit the text with or without figures embedded, and then you also submit “high” resolution figure files separately (yes it’s annoying, but that’s the way it is for many submission systems). Now, you could plot twice in each chunk, but thats freaking annoying. Ideally, Rmarkdown or knitr should do all the work for us. And thanks to the R Markdown Cookbook (Xie 2021), there is a simple way to do this.\nIn the yaml preamble, simply set the parameter: keep_md: true, like this:\noutput:\n  word_document:\n    keep_md: true\nThis means the markdown intermediate is kept, as well as the directory of figures. The figures will be named with the name of the chunk, so you will want to name your chunks well to make it easy to find the figures.\nYou can also control where your figures get written to, as well as the default figure size, output type, and resolution.\nThis chunk sets the directory (path) the dpi to 300, to use cairo PNG output, the figure width to 8 in, height to 5 in. And then creates the directory (which I’m sure knitr does, but this is for my own sanity).\n# the trailing slash is important!\nfig_dir = here::here(\"doc\", \"figure_directory/\") \nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, dpi = 300,\n                      dev.args = list(png = list(type = \"cairo\")),\n                      fig.width = 8, fig.height = 5,\n                      fig.path = fig_dir)\nif (!dir.exists(fig_dir)) {\n  dir.create(fig_dir)\n}\nEdit: I rolled all of this functionality into a little package documentNumbering.\n\n\n\nXie, C; Riederer, Y; Dervieux. 2021. “R Markdown Cookbook.” https://bookdown.org/yihui/rmarkdown-cookbook/keep-files.html.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-03T10:38:36-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-02-animating-a-geographic-introduction/",
    "title": "My Geographic Introduction",
    "description": "Adapting Piping Hot Data's Geographic Introduction animation for myself.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-05-02",
    "categories": [
      "maps",
      "graphing",
      "visualization",
      "animation"
    ],
    "contents": "\nI thought the recent animated map at Piping Hot Data {Pileggi (2021)} was a really neat way to demonstrate where someone has lived and what their various experiences may have been (while acknowledging that we are also more than the sum of where we have lived of course), so I thought I would take a go at creating my own, which includes stints in various parts of Canada, a stint in Germany, as well as two moves within Kentucky.\nI’ll start by getting all of the locations, as well as my time at each one. I had to add the year dates so I could get the number of years correct, as my initial try I was missing 6 years. Now I’m only missing 1.5, which isn’t bad if we are using years as our unit of time.\n\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(gganimate)\nlibrary(extrafont)\nlibrary(ggtext)\nresidence = tribble(\n~city,           ~state,  ~lat,   ~long, ~years, ~description, ~dates,\n\"Brandon\",  \"MB\", 49.8485, -99.9501, 2.5, \"Born\", \"1979-1982\",\n\"Lahr\",     \"Baden-Württemberg\", 48.3392, 7.8781, 6, \"Childhood\", \"1982-1988\",\n\"Oromocto\", \"NB\", 45.8487, -66.4813, 4, \"Childhood\", \"1988-1992\",\n\"Victoria\", \"BC\", 48.4284, -123.3656, 1, \"Childhood\", \"1992-1993\",\n\"Burton\", \"NB\", 45.8752, -66.3611, 9, \"Childhood<br>Undergrad at UNB\", \"1993-2002\",\n\"Fredericton\", \"NB\", 45.9636, -66.6431, 2, \"Masters at UNB\", \"2002-2004\",\n\"Halifax\", \"NS\", 44.6488, -63.5752, 5, \"PhD at Dalhousie\", \"2004-2009\",\n\"Louisville\", \"KY\", 38.2527, -85.7585, 2, \"PostDoc at UofL\", \"2010-2012\",\n\"Lexington\", \"KY\", 38.0406, -84.5037, 9, \"PostDoc at UK<br>Research Associate at UK\", \"2012-2021\"\n)\n\n\n\nI created a function from Shannon’s original code because I end up using it twice, and I was missing all of the variables I needed to change to make it work properly.\n\n\ncreate_connections = function(residence){\n  residence_connections_prelim = residence %>% \n  mutate(\n    # need this to create transition state ----\n    city_order = row_number() + 1,\n    # where I moved to next, for curved arrows ----\n    lat_next = lead(lat),\n    long_next = lead(long),\n    # label to show in plot, styled using ggtext ---\n    label = glue::glue(\"**{city}, {state}** ({years} yrs)<br>*{description}*\"),\n    # label of next location ----\n    label_next = lead(label)\n  ) \n  n_entry = nrow(residence_connections_prelim)\nresidence_connections = residence_connections_prelim %>%\n  # get first row of residence ----\n  slice(1) %>% \n  # manually modify for plotting ----\n  mutate(\n    city_order = 1,\n    label_next = label,\n    lat_next = lat,\n    long_next = long,\n    ) %>% \n  # combine with all other residences ----\n  bind_rows(residence_connections_prelim) %>% \n  # last (9th) row irrelevant ----\n  slice_head(n = n_entry) %>% \n  # keep what we neeed ----\n  dplyr::select(city_order, lat, long, lat_next, long_next, label_next)\n  residence_connections\n}\n\n\n\n\n\nworld_data = ggplot2::map_data(\"world\")\ntrim_world = world_data %>% \n  dplyr::filter(long >= -130 & long <= 20, lat >= 35, lat <= 70)\nggplot() + geom_polygon(data = trim_world, aes(x=long, y = lat, group = group)) +\ncoord_fixed(1.3)\n\n\n\n\nOK, at least that looks like the right region that I want to use. Basically from British Columbia to Germany, and the northern part of North America.\n\n\nbase_map = ggplot() +\n  # plot states ----\n  geom_polygon(\n    data = trim_world,\n    aes(\n      x     = long, \n      y     = lat, \n      group = group\n      ),\n    fill  = \"#F2F2F2\",\n    color = \"white\"\n  ) +\n  # lines for pins ----\n  geom_segment(\n    data = residence,\n    aes(\n      x    = long,\n      xend = long,\n      y    = lat,\n      yend = lat + 0.5\n      ),\n    color = \"#181818\",\n    size = 0.3\n    ) +\n    # pin heads, a bit above actual location, color with R ladies lighter purple ----\n  geom_point(\n    data = residence,\n    aes(\n      x = long, \n      y = lat + 0.5\n      ),\n    size = 0.5,\n    color = \"#88398A\"\n  ) +\n  theme_void() +\n  coord_fixed(1.3)\nbase_map\n\n\n\n\n\n\nres_connections = create_connections(residence)\nn_res = nrow(res_connections)\nanim = base_map +\n  # show arrows connecting residences ----\n  geom_curve(\n    # do not include 1st residence in arrows as no arrow is intended ----\n    # and inclusion messes up transition ---\n    data = res_connections %>% slice(-1),\n    # add slight adjustment to arrow positioning ----\n    aes(\n      y     = lat - 0.1,\n      x     = long,\n      yend  = lat_next - 0.2,\n      xend  = long_next,\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    color = \"#181818\",\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.02, \"npc\")),\n    size  = 0.2\n  ) +\n  # add in labels for pins, with inward positioning ----\n  # show labels either top left or top right of pin ----\n  geom_richtext(\n    data = res_connections,\n    aes(\n      x     = ifelse(long_next < -100, long_next + 1, long_next - 1),\n      y     = lat_next + 5,\n      label = label_next,\n      vjust = \"top\",\n      hjust = ifelse(long_next < -100, 0, 1),\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    size = 2,\n    label.colour = \"white\",\n    # R ladies purple ----\n    color = \"#562457\",\n    # R ladies font used in xaringan theme ----\n    family = \"Lato\"\n  ) +\n  # title determined by group value in transition ----\n  ggtitle(paste0(\"Home {closest_state} of \", n_res)) +\n  # create animation ----\n  transition_states(\n    city_order,\n    transition_length = 2,\n    state_length = 5\n    ) +\n  # style title ----\n  theme(\n    plot.title = element_text(\n      color = \"#562457\",\n      family = \"Permanent Marker\",\n      size = 12\n      )\n    )\n# render and save transition ----\n# the default nframes 100 frames, 150 makes the gif a bit longer for readability ----\n# changing dimensions for output w/ height & width ----\n# increasing resolution with res ----\nanimate(anim, nframes = 150, height = 2, width = 3, units = \"in\", res = 150)\n\n\n\nanim_save(\"homes_animation.gif\")\n\n\n\nThat’s not bad! The only issue with it is that because of the crossing of the Atlantic Ocean, the travels within North America, especially the very close travels from NB to NS, and then within KY are way too crushed together.\nSo lets see what happens if we trim to the region of North America, and remove the overseas trip to Germany.\n\n\ntrim_world2 = world_data %>% \n  dplyr::filter(long >= -130 & long <= -60, lat >= 35, lat <= 70)\nggplot() + geom_polygon(data = trim_world2, aes(x=long, y = lat, group = group)) +\ncoord_fixed(1.3)\n\n\n\n\n\n\nresidence2 = residence %>%\n  dplyr::filter(!grepl(\"Lahr\", city))\nres_connections2 = create_connections(residence2)\nn_res2 = nrow(res_connections2)\nbase_map2 = ggplot() +\n  # plot states ----\n  geom_polygon(\n    data = trim_world2,\n    aes(\n      x     = long, \n      y     = lat, \n      group = group\n      ),\n    fill  = \"#F2F2F2\",\n    color = \"white\"\n  ) +\n  # lines for pins ----\n  geom_segment(\n    data = residence2,\n    aes(\n      x    = long,\n      xend = long,\n      y    = lat,\n      yend = lat + 0.5\n      ),\n    color = \"#181818\",\n    size = 0.3\n    ) +\n    # pin heads, a bit above actual location, color with R ladies lighter purple ----\n  geom_point(\n    data = residence2,\n    aes(\n      x = long, \n      y = lat + 0.5\n      ),\n    size = 0.5,\n    color = \"#88398A\"\n  ) +\n  theme_void() +\n  coord_fixed(1.3)\nbase_map2\n\n\n\n\n\n\nanim2 = base_map2 +\n  # show arrows connecting residences ----\n  geom_curve(\n    # do not include 1st residence in arrows as no arrow is intended ----\n    # and inclusion messes up transition ---\n    data = res_connections2 %>% slice(-1),\n    # add slight adjustment to arrow positioning ----\n    aes(\n      y     = lat - 0.1,\n      x     = long,\n      yend  = lat_next - 0.2,\n      xend  = long_next,\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    color = \"#181818\",\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.02, \"npc\")),\n    size  = 0.2\n  ) +\n  # add in labels for pins, with inward positioning ----\n  # show labels either top left or top right of pin ----\n  geom_richtext(\n    data = res_connections2,\n    aes(\n      x     = ifelse(long_next < -100, long_next + 1, long_next - 1),\n      y     = lat_next + 5,\n      label = label_next,\n      vjust = \"top\",\n      hjust = ifelse(long_next < -100, 0, 1),\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    size = 2,\n    label.colour = \"white\",\n    # R ladies purple ----\n    color = \"#562457\",\n    # R ladies font used in xaringan theme ----\n    family = \"Lato\"\n  ) +\n  # title determined by group value in transition ----\n  ggtitle(paste0(\"Home {closest_state} of \", n_res2)) +\n  # create animation ----\n  transition_states(\n    city_order,\n    transition_length = 2,\n    state_length = 5\n    ) +\n  # style title ----\n  theme(\n    plot.title = element_text(\n      color = \"#562457\",\n      family = \"Permanent Marker\",\n      size = 12\n      )\n    )\n# render and save transition ----\n# the default nframes 100 frames, 150 makes the gif a bit longer for readability ----\n# changing dimensions for output w/ height & width ----\n# increasing resolution with res ----\nanimate(anim2, nframes = 150, height = 2, width = 3, units = \"in\", res = 150)\n\n\n\nanim_save(\"homes_animation2.gif\")\n\n\n\n\n\n\nPileggi, Shannon. 2021. “PIPING HOT DATA: GGanimating a Geographic Introduction.” https://www.pipinghotdata.com/posts/2021-02-15-gganimating-a-geographic-introduction/.\n\n\n\n\n",
    "preview": "posts/2021-05-02-animating-a-geographic-introduction/animating-a-geographic-introduction_files/figure-html5/get_world_data_trim-1.png",
    "last_modified": "2021-05-02T21:10:33-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-09-proportional-error-in-mass-spectrometry/",
    "title": "Proportional Error in Mass Spectrometry",
    "description": "Demonstrating the existence of proportional error in mass spectrometry measurements.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-04-09",
    "categories": [
      "mass-spectrometry",
      "proportional-error",
      "omics",
      "metabolomics"
    ],
    "contents": "\n\nContents\nIntroduction\nExample Data\nWhy??\nSolution\n\nIntroduction\nThe other day, Kareem Carr asked for a statistics / data science opinion that results in the daggers being drawn on you (Carr, n.d.), and I replied (Flight, n.d.):\n\nEvery physical analytical measurement in -omics suffers from some proportional error. Either model it (neg binomial in RNA-seq for example), or transform your data appropriately (log-transform).\nThis includes DNA microarrays, RNA-seq, Mass Spec, and NMR (I need data to confirm)\n\nI wanted to give some proof of what I’m talking about, because I don’t think enough people understand or care. For example, if you get mass-spec data from Metabolon, their Surveyor tool defaults to using non-log-transformed data. Trust me, you should log-transform the values.\nExample Data\nI have some direct-injection mass spec data on a polar fraction examining ECF derivatized amino-acids, with multiple “scans” from a Thermo Fisher Fusion Tribrid Mass Spectrometer. Each scan is the product of a small number of micro-scans. However, based on the spectrometer, and the lack of chromatography, it would not be unreasonable to expect that each scan is essentially a “replicate” of the other scans, so comparing one to any other is reasonable.\nI’m going to load up the data, and plot two scans in “raw” space. The data are already log10 transformed, so we will “untransform” it back to “raw” first before plotting it.\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nscan_data = readRDS(here::here(\"mass_spec_example_data.rds\"))\nraw_data = purrr::map_dfc(scan_data, ~ 10^.x)\n\nraw_data %>%\n  dplyr::filter(scan174 <= 1e7) %>%\nggplot(aes(x = scan174, y = scan175)) + \n  geom_point() +\n  coord_equal() +\n  labs(subtitle = \"Scan-Scan Intensities, Raw Values\")\n\n\n\n\nAs you can see here, the points really start to diverge as they cross the line at 2.5 x 10^6.\nNow lets plot the log10 transformed values.\n\n\nscan_data %>%\n  dplyr::filter(scan174 <= 1e7) %>%\nggplot(aes(x = scan174, y = scan175)) + \n  geom_point() +\n  coord_equal() +\n  labs(subtitle = \"Scan-Scan Intensities, Log10 Values\")\n\n\n\n\nAnd now everything is coming to a point as the intensity increases.\nIf you’ve worked with microarray or RNASeq data, this is also commonly seen in those data.\nTo show the presence of the proportional error more generally, we can calculate the mean and variance of each point across the scans, and plot those. To make sure we are getting “good” representations of the data, we will only use points that were present in at least 20 scans.\n\n\npoint_mean = rowMeans(raw_data, na.rm = TRUE)\npoint_sd = apply(raw_data, 1, sd, na.rm = TRUE)\nn_present = apply(raw_data, 1, function(.x){sum(!is.na(.x))})\n\nmean_sd = data.frame(mean = point_mean,\n                     sd = point_sd,\n                     n = n_present)\nmean_sd %>%\n  dplyr::filter(n >= 20) %>%\n  ggplot(aes(x = mean, y = sd)) + geom_point() +\n  labs(subtitle = \"Standard Deviation vs Mean\")\n\n\n\n\nThere you have it, the standard deviation is increasing with the mean, and doing so in two different ways, one increasing slowly, and one increasing quickly. Either of these are not good. Whether increasing slowly or quickly, the point is that the error / variance / standard deviation is somehow dependent on the mean, which most standard statistical methods do not handle well. This actually drove the whole development of using the negative-binomial distribution in RNASeq!\nWhy??\nWhy does this happen? Try this thought experiment:\nI take 5 people in a room, and ask you to count how many people are in the room in 60 seconds and give me an answer.\nI ask you to do this 20 times.\nYour answer should be 5 each time, right?\nNow I put 10 people in the room, and ask again how many are there. And ask multiple times.\nNow 20 people in the same size room.\nAnd then 30 people …\nAnd then 40 people …\nAnd so on.\nIf you think about it, given an constantly sized room, it will get harder and harder to count the people in it as their number increases, and with repeated counting, your estimates are likely to have more and more variance or a higher standard deviation as the number of people goes up. Thus the “error” is proportional or depends on the actual number of things you are estimating, much like the mass spec data above.\nIn DNA microarrays and RNA-seq, the technology was measuring photons. The higher the number of photons, the more difficult it is to be sure how many there are. And that gets translated to any downstream quantity based on the number of photons.\nIn Fourier-transform mass spectrometry, the instrument is still trying to quantify “how many” of each ion there is, and the more ions, the more difficult it is to quantify them. And we end up with proportional error.\nI think in any technology that is trying to “count” how many of something there is within a finite space, these properties will manifest themselves. You just have to know how to look.\nSolution\nIn the short term, transform your data using log’s. In the long term, I think we need more biostatisticians working with more mass-spectrometry and NMR data to determine what the error structure is for more instruments and analytical measurement types.\n\n\n\nCarr, Kareem. n.d. https://twitter.com/kareem_carr/status/1380159451345346569?s=20.\n\n\nFlight, Robert M. n.d. https://twitter.com/rmflight/status/1380161512837410835?s=20.\n\n\n\n\n",
    "preview": "posts/2021-04-09-proportional-error-in-mass-spectrometry/proportional-error-in-mass-spectrometry_files/figure-html5/load_data-1.png",
    "last_modified": "2021-04-09T13:33:46-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-26-highlighting-a-row-of-a-complexheatmap/",
    "title": "Highlighting a Row of A ComplexHeatmap",
    "description": "A simple way to highlight or bring attention to a row or column in a ComplexHeatmap.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-03-26",
    "categories": [
      "random-code-snippets",
      "heatmap",
      "visualization"
    ],
    "contents": "\nThe ComplexHeatmap Bioconductor package (Gu, Eils, and Schlesner 2016; Gu 2021a, 2021b) has become my goto for visualizing sample-sample correlation heatmaps, which I use a lot. Recently, I had a report where I wanted to highlight a particular row of the heatmap. There is not an easy way that I could find to add something that wraps around a particular column. However, you can indicate that there is a grouping in the heatmap, and use that grouping to separate a sample or samples from the others.\nLets do an example:\n\n\nlibrary(ComplexHeatmap)\nset.seed(123)\nnr1 = 4; nr2 = 8; nr3 = 6; nr = nr1 + nr2 + nr3\nnc1 = 6; nc2 = 8; nc3 = 10; nc = nc1 + nc2 + nc3\nmat = cbind(rbind(matrix(rnorm(nr1*nc1, mean = 1,   sd = 0.5), nr = nr1),\n          matrix(rnorm(nr2*nc1, mean = 0,   sd = 0.5), nr = nr2),\n          matrix(rnorm(nr3*nc1, mean = 0,   sd = 0.5), nr = nr3)),\n    rbind(matrix(rnorm(nr1*nc2, mean = 0,   sd = 0.5), nr = nr1),\n          matrix(rnorm(nr2*nc2, mean = 1,   sd = 0.5), nr = nr2),\n          matrix(rnorm(nr3*nc2, mean = 0,   sd = 0.5), nr = nr3)),\n    rbind(matrix(rnorm(nr1*nc3, mean = 0.5, sd = 0.5), nr = nr1),\n          matrix(rnorm(nr2*nc3, mean = 0.5, sd = 0.5), nr = nr2),\n          matrix(rnorm(nr3*nc3, mean = 1,   sd = 0.5), nr = nr3))\n   )\nmat = mat[sample(nr, nr), sample(nc, nc)] # random shuffle rows and columns\nrownames(mat) = paste0(\"row\", seq_len(nr))\ncolnames(mat) = paste0(\"column\", seq_len(nc))\n\nHeatmap(mat, cluster_rows = FALSE, cluster_columns = FALSE)\n\n\n\n\nNow, lets suppose we just want to highlight row2.\nWe create a data.frame with a factor to represent the grouping:\n\n\nwhich_row2 = which(grepl(\"row2\", rownames(mat)))\nsplit = data.frame(x = c(rep(\"A\", which_row2 - 1), \"B\",\n                   rep(\"C\", nrow(mat) - which_row2)))\nHeatmap(mat, cluster_rows = FALSE, cluster_columns = FALSE,\n        row_split = split,\n        row_title = NULL)\n\n\n\n\nVoila! row2 is separated from the others to draw attention to it. It’s not perfect, but hopefully it’s useful to others. Note, that you can’t use clustering with this method. If you have actual dendrograms to display, this will fail, because ComplexHeatmap expects you to use a numeric argument to tell the cut height for dendrograms for splitting (Gu 2021c). Therefore, if you have dendrograms, reorder your columns and rows according to the dendrogram first and then add the splitting information and keep the clustering off.\n\n\n\nGu, Zuguang. 2021a. “ComplexHeatmap.” https://doi.org/10.18129/B9.bioc.ComplexHeatmap.\n\n\n———. 2021b. “ComplexHeatmap Complete Reference.” https://jokergoo.github.io/ComplexHeatmap-reference/book/.\n\n\n———. 2021c. “ComplexHeatmap Complete Reference.” https://jokergoo.github.io/ComplexHeatmap-reference/book/a-single-heatmap.html#heatmap-split.\n\n\nGu, Zuguang, Roland Eils, and Matthias Schlesner. 2016. “Complex Heatmaps Reveal Patterns and Correlations in Multidimensional Genomic Data.” Bioinformatics.\n\n\n\n\n",
    "preview": "posts/2021-03-26-highlighting-a-row-of-a-complexheatmap/highlighting-a-row-of-a-complexheatmap_files/figure-html5/highlight_row2-1.png",
    "last_modified": "2021-03-26T10:17:14-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/",
    "title": "Creating a Map of Routes Weighted by Travel",
    "description": "I made a map of my spouse's travel since we got Google phones for her birthday last fall. Here's how I did it.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-03-21",
    "categories": [
      "graphing",
      "maps",
      "visualization"
    ],
    "contents": "\n\nContents\nInspiration\nDecide on a Size\nFind Your Bounding Box\nFetching Map\nAdding County / State Features & Water\nLocation Data\nRoutes\nPlot It!\n\nInspiration\nWay back in October 2020, I saw a tweet cross my feed by Esteban on making personal map art, and I was struck by their map (Moro 2020). I was also looking for an idea for my spouses birthday that was coming up in November, and I decided to do one of these maps for my lovely wife.\nIf you want to create one of these, you should definitely check out Esteban’s post (Moro 2020) for how they did it. Esteban also has figures for how things look as they go together. I’m not that organized, unfortunately.\nI’m detailing my process here, because I tried to organize it in a bit of a different way, putting as much stuff into functions as I could so I can reuse code where possible.\nYou can check out the code on GitHub (Flight 2020) and see how it’s organized.\nA short list of what you need for this project:\nA region of interest with mapped roads in OpenStreetMap\nA list of starting locations and ending locations that the shortest route is easily findable.\nEsteban and I used Google Maps takeout data.\nMy spouse and I have location data from when we started using Google phones back in August of 2016.\n\nDecide on a Size\nIdeally before embarking on this project, decide what size of a print you want. If you are in the USA, WalMart’s prices are actually pretty decent, and I’ve found their quality to be good. I was very happy with the canvas I got from them back in November. The size of the print defines the ratio of the bounding box you are going to want to use and how you want it to look. I ultimately decided on a 16 in high by 20 in wide.\nFind Your Bounding Box\nThis is the part that takes some interactive work unless you just want to work within a particular city limits.\nI used the export function on OpenStreetMap to create my bounding box of the area I was interested in.\nI used {drake} to define a workflow for this project so any fetching of data from OpenStreetMap would only have to be done once.\n\n\nthe_plan <-\n  drake_plan(\n    lexington_bbx = list(min_lon = -84.7533,\n                         max_lon = -84.2143,\n                         min_lat = 37.9358,\n                         max_lat = 38.1775),\n\n    lexington_map = get_map(lexington_bbx),\n    lexington_counties = get_counties(lexington_bbx, \"KY\"),\n    lexington_water = get_us_water(lexington_bbx,\n                                   lexington_counties,\n                                   \"KY\"),\n    lexington_counties_water = combine_counties_uswater(lexington_counties, lexington_water),\n    sarah_locations = get_takeout_locations(\"saraheflight/saraheflight_takeout\"),\n    sarah_routes = get_sarah_routes(sarah_locations)\n)\n\n\n\nThis plan has the bounding box defined, fetches the map data, counties data, and any water data, merges it together, and then grabs the locations, and determines the routes.\nFetching Map\nWe fetch the map data for the bounding box. Note that we fetch a ton of the road data, because that is what is likely to make it look nice. We also classify the highways and streets so that they can have a different weight in the final map.\n\n\nget_map = function(bbx_list){\n  bbx = rbind(x=c(bbx_list$min_lon, bbx_list$max_lon),y=c(bbx_list$min_lat, bbx_list$max_lat))\n  colnames(bbx) = c(\"min\",\"max\")\n\n  highways = bbx %>%\n    opq() %>%\n    add_osm_feature(key = \"highway\",\n                    value=c(\"motorway\", \"trunk\",\n                            \"primary\",\"secondary\",\n                            \"tertiary\",\"motorway_link\",\n                            \"trunk_link\",\"primary_link\",\n                            \"secondary_link\",\n                            \"tertiary_link\")) %>%\n    osmdata_sf()\n  streets = bbx %>%\n    opq() %>%\n    add_osm_feature(key = \"highway\",\n                    value = c(\"residential\", \"living_street\",\n                              \"service\",\"unclassified\",\n                              \"pedestrian\", \"footway\",\n                              \"track\",\"path\")) %>%\n    osmdata_sf()\n\n  list(highways = highways,\n       streets = streets)\n}\n\n\n\nAdding County / State Features & Water\nWe also want to have the data for the county and waterways (there are probably lakes and rivers or coastlines near you).\n\n\nget_counties = function(bbx_list, state = \"KY\"){\n\n  counties_state = counties(state=state, cb=T, class=\"sf\")\n  counties_state = st_crop(counties_state,\n                         xmin = bbx_list$min_lon, xmax = bbx_list$max_lon,\n                         ymin = bbx_list$min_lat, ymax = bbx_list$max_lat)\n  counties_state\n}\n\nget_us_water = function(bbx_list, counties_list, state){\n  get_water = function(county_GEOID, state = state){\n    area_water(state, county_GEOID, class = \"sf\")\n  }\n  water = do.call(rbind,\n                   lapply(counties_list$COUNTYFP, get_water, state))\n  water = st_crop(water,\n                   xmin = bbx_list$min_lon, xmax = bbx_list$max_lon,\n                   ymin = bbx_list$min_lat, ymax = bbx_list$max_lat)\n  water\n}\n\ncombine_counties_uswater = function(counties_state, counties_water){\n  st_difference(counties_state, st_union(counties_water))\n}\n\n\n\nThis gives us a decent image with the highways, streets, county level features, and the waterways.\nLocation Data\nAs I said previously, I used the Google Maps location data from Google Takeout. I asked my spouse for the data. Be advised, it can take Google a little bit to prepare this data depending on how much there is.\nIf you didn’t have an automated source of data, you could probably set up a set of destinations replicated by how often you think you traveled there to get relative weights.\nFor this, we will parse through the takeout data and get all of the destinations.\nThe function below goes through all of the files (they are organized by year and month) and grabs the locations, and puts them into a data.frame to iterate through.\n\n\nget_takeout_locations = function(takeout_dir){\n\n  file2 = file.path(takeout_dir, \"Takeout\", \"Location History\", \"Semantic Location History\")\n  files = list.files(file2, pattern = \"*.json\", recursive = TRUE, full.names = TRUE)\n  get_locations = function(file, .progress = NULL){\n    knitrProgressBar::update_progress(.progress)\n    data = jsonlite::fromJSON(file)\n    tl_obj = data$timelineObjects$placeVisit\n    loc = cbind(tl_obj$location, tl_obj$duration)\n    tt = as.numeric(loc$startTimestampMs)/1000\n    loc$time=as.POSIXct(tt,origin = \"1970-01-01\")\n    #conver longitude & latitude from E7 to GPS\n    loc$lat = loc$latitudeE7 / 1e7\n    loc$lon = loc$longitudeE7 / 1e7\n    loc = data.frame(loc)\n    loc = loc[, c(\"placeId\", \"time\", \"lat\", \"lon\")]\n    loc = dplyr::filter(loc, !is.na(lon))\n    loc\n  }\n  locs_df = purrr::map_df(files, get_locations)\n  locs_df\n}\n\n\n\nRoutes\nThen we have to work out the routes. For this project, it was complicated by the fact that we’ve lived in two different locations since we moved here. So this function sets two different home locations, and switches between them depending on the date of the trip.\nWe also assume that every trip is a trip between home and the destination. The locations are organized by day, so we have to do some transformations to make every trip start at home and end at the destination. Obviously that’s not how we actually travel, but otherwise I’d have to try and extract the route level data from the takeout, and that would be more of a pain. And for the kind of map we are trying to generate, this works well enough.\n\n\nget_sarah_routes = function(locs_df){\n  old_home = list(lat = 37.9898308, lon = -84.5054868)\n  new_home = list(lat = 37.982469, lon = -84.506552)\n  locs_df$day = lubridate::floor_date(locs_df$time, unit = \"day\")\n  locs_df = tibble::as_tibble(locs_df)\n  locs_df = dplyr::mutate(locs_df, home = dplyr::case_when(\n    day <= as.POSIXct(\"2018-03-14\") ~ list(old_home),\n    TRUE ~ list(new_home)\n  ))\n\n  split_day = split(locs_df, locs_df$day)\n\n  day_routes = purrr::map(split_day, daily_routes)\n  day_routes = do.call(rbind, day_routes)\n}\n\ndaily_routes = function(day_locations){\n  home_loc = day_locations$home[[1]]\n  use_locs = day_locations[, c(\"lat\", \"lon\")]\n  use_locs2 = rbind(data.frame(lat = home_loc$lat, lon = home_loc$lon),\n                    use_locs,\n                    data.frame(lat = home_loc$lat, lon = home_loc$lon))\n  route = NULL\n  for(irow in 2:nrow(use_locs2)){\n    p1 = c(use_locs2$lon[irow - 1], use_locs2$lat[irow - 1])\n    p2 = c(use_locs2$lon[irow], use_locs2$lat[irow])\n    oo = osrmRoute(src = p1, dst = p2, returnclass = \"sf\",\n                    overview = \"full\")\n    route <- rbind(route, oo)\n  }\n  route\n}\n\n\n\nPlot It!\nFinally, we put everything together into an image that can be plotted!\nThis ended up in a script because I was doing a lot of playing around with it, and when I finally got the image, I just saved the final script. You can also see here that after the fact I was trying to mess with the bounding box to get the correct aspect ratio. Don’t be like me, do it up front and figure it out.\n\n\nsource(\"packages.R\")\nloadd(lexington_bbx)\nloadd(lexington_map)\nnames(lexington_map)\nloadd(lexington_counties_water)\nloadd(sarah_routes)\n\nlexington_bbx = list(min_lon = -84.7533,\n                     max_lon = -84.355,\n                     min_lat = 37.9358,\n                     max_lat = 38.1775)\n\n(lexington_bbx$max_lat - lexington_bbx$min_lat) / (lexington_bbx$max_lon - lexington_bbx$min_lon)\n\ncolor_roads <- rgb(0.42,0.449,0.488)\nfinal_map = ggplot() +\n  geom_sf(data = lexington_counties_water,\n          inherit.aes = FALSE,\n          lwd= 0.0, fill = rgb(0.203,0.234,0.277)) +\n  geom_sf(data = lexington_map$streets$osm_lines,\n          inherit.aes = FALSE,\n          color=color_roads,\n          size = .4,\n          alpha = .65) +\n  geom_sf(data = lexington_map$highways$osm_lines,\n          inherit.aes = FALSE,\n          color=color_roads,\n          size = .6,\n          alpha = .65) +\n  geom_sf(data = st_geometry(sarah_routes),\n          inherit.aes = FALSE, col = \"orange\", alpha = 0.2) +\n  coord_sf(xlim = c(lexington_bbx$min_lon, lexington_bbx$max_lon),\n           ylim = c(lexington_bbx$min_lat, lexington_bbx$max_lat),\n           expand = FALSE) +\n  theme(legend.position = \"none\") + theme_void() +\n  theme(panel.background=\n          element_rect(fill = \"white\"))\n\nggsave(final_map,\n       filename = \"sarah_lexington.png\",\n       scale = 1,\n       width = 20,\n       height = 16,\n       units = \"in\",\n       bg = rgb(0.203,0.234,0.277),\n       dpi = 500)\n\n\n\nAnd what I got was this:\n\n\n\nAnd now it hangs on our wall as a canvas print:\n\n\n\n\n\n\nFlight, Robert M. 2020. “Mapart.” https://github.com/rmflight/mapart.\n\n\nMoro, Esteban. 2020. “Personal Art Map with r.” http://estebanmoro.org/post/2020-10-19-personal-art-map-with-r/.\n\n\n\n\n",
    "preview": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/sarah_lexington.png",
    "last_modified": "2021-03-21T22:05:28-04:00",
    "input_file": {},
    "preview_width": 10000,
    "preview_height": 8000
  },
  {
    "path": "posts/2021-03-05-randomcodesnippets/",
    "title": "Random Code Snippets",
    "description": "Introducing random-code-snippets.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-03-05",
    "categories": [
      "random-code-snippets"
    ],
    "contents": "\nI’ve been organizing a bunch of random code that I find myself re-using time and time again in a Github Repo (Flight, n.d.). But the repo is not very searchable unless you are the one who maintains it, and some of the categories are not well defined.\nThis blog already contains a few posts in this vein, where I’ve previously described, including these for example:\nCreating custom Affymetrix CDF definitions (Flight 2012)\nCustom commit hooks to increment R package version numbers (Flight 2014)\nWriting a custom blog deployment script (Flight 2017)\nSo, in that vein, I’ve decided to organize those posts here, and move more of the code from the github repo from there to here, where it should be easier to find and search, as they can be organized under the one page, but have multiple tags to make it easier to find what you (or really myself) are looking for.\nThese will all share the category random-code-snippets, and be listed at Random Code Snippets. I hope others find these as useful as I do.\n\n\n\nFlight, Robert M. 2012. “Deciphering Life: One Bit at a Time: Creating Custom CDFs for Affymetrix Chips in Bioconductor.” https://rmflight.github.io/posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/.\n\n\n———. 2014. “Deciphering Life: One Bit at a Time: Package Version Increment Pre- and Post-Commit Hooks.” https://rmflight.github.io/posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/.\n\n\n———. 2017. “Deciphering Life: One Bit at a Time: Custom Deployment Script.” https://rmflight.github.io/posts/2017-12-27-custom-deployment-script/.\n\n\n———. n.d. “Resources.” https://github.com/rmflight/resources.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-05T10:27:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/",
    "title": "Packages Don't Work Well for Analyses in Practice",
    "description": "I was wrong about using packages to structure statistical analyses.\nAlso why I finally switched to {drake}.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-03-02",
    "categories": [
      "R",
      "development",
      "packages",
      "vignettes",
      "programming",
      "analysis",
      "workflow"
    ],
    "contents": "\n\nContents\nTL;DR\nUsing R Packages for Analysis\nWhat Happened in Practice?\nA Dim Bulb\nA Little Brighter\nLightbulb Goes Off\n\nTL;DR\nI was wrong about using an R package to bundle an analysis. That happens a lot I suppose. But I needed to own up here.\nUsing R Packages for Analysis\nA couple of years ago, I wrote at least three different blog posts describing how using R packages to bundle up an analysis was the best way to do things. I went into the benefits of this approach in general (Flight 2014a), how to do actually go through one in practice (Flight 2014b), and bashed on an alternative, ProjectTemplate (Flight 2014c). I was wrong. So, so very wrong.\nWhat Happened in Practice?\nAlthough I was espousing this philosophy for doing the analysis as part of an R package, in practice, it was a pain in the butt to actually carry out. Write functions, update installed package, update vignette, hope it all runs. I honestly would end up with a ton of functions written in the report vignette, and hope that {knitr} caching would save me (or have to delete the cache because something went wrong).\nEventually, I started having project directories with a report, and throwing all my functions into the top of the rmarkdown, and if I was lucky I might write objects (tables, figures, etc) out into directories that could then be sent to collaborators.\nA Dim Bulb\nHowever, {drake} was becoming popular as a proper workflow manager for R, with some very nice capabilities, and I started using it to help out with some large caching things I wanted for analyses for a manuscript. I still didn’t use it for anything else.\nThe documentation for {drake} very much encourages a functional workflow, where one writes functions that take inputs from other functions and generate outputs for other functions (Will Landau 2020). And though Will encourages this in the documentation, for some reason I was not grokking how this worked in practice, and how it could work easily for all of my projects with some simple conventions around folder structure borrowed from other projects (or even from R packages).\n\nWill Landau recommends not using {drake} anymore and to switch to {targets}. Given development on drake has stopped, that’s probably a good idea. There still isn’t a {targets} flavor of {dflow} (see below) available yet, however.\nA Little Brighter\nMiles McBain was talking about some of these things (around data workflows) intermittently on Twitter, and then posted about a functional approach to analyses using {drake} (McBain 2020a). When I read it, I was really, really impressed with it. Unfortunately, I still wasn’t really clear on how to implement and use his method with {drake}. This is not the fault of Miles’ post or {drake}’s documentation (see above). It is my fault for not reading it slowly, and taking the time to try and stop through it with the {dflow} implementation and {drake} documentation together.\n\nIf you haven’t read it yet, go read it to see if it might help you. Seriously, it’s really, really good, and well thought out.\nLightbulb Goes Off\nMiles’ post was at the end of April, 2020. In August 2020, he was a guest at the New York Open Statistical Programming Meetup virtual event where he presented his {dflow} workflow in That Feeling of Workflowing (Lander 2020) (McBain 2020b). Seeing and hearing Miles explain everything, and then actually work through an example that everything about his approach finally clicked for me. And really, any project based layout that combines a directory for R scripts, and inputs and outputs too (e.g. {ProjectTemplate} and others).\nNow, I know, that still sounds like an R package, with functions in /R, reports in /vignettes, and built in documentation. But it’s lighter weight than a package, and the functions are specific to just this one analysis.\n{dflow} includes some nice boilerplate code for keeping your list of packages in one place, and getting all of your functions into the workspace. I was so impressed that I installed {dflow} the next day and working out how to convert my current analysis project to use it. Shortly after experimenting with that one analysis, I was converted. Every new project I start is being done using {dflow}, and any old projects I’ve gone back to are being converted over to use it.\nI would now recommend using {drake} / {targets} / {dflow} or {ProjectTemplate} (or any other directory based system with caching) for R analysis projects. And if you have the discipline and find it works for you, then using R packages might still work (there is even a {drake} / {targets} way of working within packages that I hadn’t noticed before). However, I think the general project based ideas are just fine for many analyses, and packages are overkill and take too much work for the analysis authors to keep it up.\n{drake} (and now {targets}) is a modern make like engine specifically for R, with an R based caching system (saving things so they don’t have to be repeated again) for your outputs that really, really helps you keep track of things, organize your inputs and outputs, and work in a functional way, which are all good things. {dflow} is a very pretty wrapper for setting up {drake} analysis projects.\nEdited on 2021-03-03 to give more credence to Will Landau and the {drake} package itself, as Miles McBain felt I gave too much credence to him instead of {drake}.\n\n\n\nFlight, Robert M. 2014a. “Deciphering Life: One Bit at a Time: Analyses as Packages.” https://rmflight.github.io/posts/2014-07-28-analyses-as-packages/.\n\n\n———. 2014b. “Deciphering Life: One Bit at a Time: Creating an Analysis as a Package and Vignette.” https://rmflight.github.io/posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/.\n\n\n———. 2014c. “Deciphering Life: One Bit at a Time: Packages Vs ProjectTemplate.” https://rmflight.github.io/posts/2014-07-28-packages-vs-projecttemplate/.\n\n\nLander, Jared. 2020. “Miles McBain - That Feeling of Workflowing [Remote].” https://www.youtube.com/watch?v=jU1Zv21GvT4.\n\n\nMcBain, Miles. 2020a. “Before i Sleep: Benefits of a Function-Based Diet (the Drake Post).” https://milesmcbain.com/posts/the-drake-post/.\n\n\n———. 2020b. https://github.com/MilesMcBain/nycr_meetup_talk.\n\n\nWill Landau, Alex Axthelm, Kirill Müller. 2020. The Drake r Package User Manual, Ch 4. https://books.ropensci.org/drake/plans.html.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-05T09:50:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-28-things-i-learned-about-distill/",
    "title": "Things I Learned About distill",
    "description": "The various things I learned about the distill blog setup while converting posts over from my old blogdown site.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2021-02-28",
    "categories": [
      "distill",
      "blogdown",
      "rmarkdown"
    ],
    "contents": "\n\nContents\nConverting From Blogdown\nDescriptions: Do or Do Not, but Be Consistent\nTheming can be a bit of a pain\nTable of contents doesn’t seem to be site-wide\nUtterances was easy!\nSearch is awesome!\nHighlighting must be set on each article\nAdding Images\nCitations, Footnotes, Asides\n\nConverting From Blogdown\nSo I converted this site to use distill from my previous blogdown site. This involved a bit of a learning curve to get things right. And my blogdown site was pretty old, as in the default in blogdown at the time was for each post to be it’s own file, not it’s own directory, and the theme I used was the Academic theme (which has since changed names and I believe gotten more complicated). This is not to knock on blogdown and Yihui (and other’s) work in this area. distill just felt like a better fit. Hopefully I’m right. And hopefully this will be the last time I change where posts are located (yes, I’m still not using Netlify, so I still don’t have redirects …), because I wanted to use the date-title format, and I didn’t keep the [slug] of the old YAML. I could have, but I wanted post locations to make sense across the site.\nSo I first used {blogdown} to convert my old blog directory to a bundled version using blogdown::bundle_site(). I then wrote a script to create the new directories based on the bundled ones, and then copy the old index.Rmd to the new direcotry with the proper name. I also decided I wanted to add a refs.bib to each directory so there was no friction in setting up references.\n\n\nnew_post_dir = \"~/Projects/personal/researchBlog_distill/_posts\"\nold_post_dir = \"~/Projects/personal/researchBlog_blogdown_convertFolders-bundle/content/post\"\n\nold_posts = dir(old_post_dir, full.names = TRUE, recursive = FALSE)\n\nold_properties = file.info(old_posts)\n\nkeep_old = old_posts[old_properties$isdir]\n\nnew_posts = dir(new_post_dir)\n\nkeep_old = keep_old[!(basename(keep_old) %in% new_posts)]\n\npurrr::walk(keep_old, function(in_dir){\n  new_loc = file.path(new_post_dir, basename(in_dir))\n  dir.create(new_loc)\n  index_loc = file.path(in_dir, \"index.Rmd\")\n  split_name = strsplit(basename(new_loc), \"-\", fixed = TRUE)[[1]]\n  split_name = split_name[seq(4, length(split_name))]\n  new_index = file.path(new_loc, paste0(paste(split_name, collapse = \"-\"), \".Rmd\"))\n  file.copy(index_loc, new_index)\n})\n\nnew_posts2 = dir(new_post_dir, full.names = TRUE)\npurrr::walk(new_posts2, function(in_dir){\n  file.create(file.path(in_dir, \"refs.bib\"))\n})\n\n\n\nAs I figured out what extra I needed (see some of the sections below) in the YAML, I opened up each post, modified the categories as necessary, and then copied in the extra bits of YAML containing a mock description and the rest regarding TOC and code highlighting.\nDescriptions: Do or Do Not, but Be Consistent\ndistill has these nice descriptions that are displayed on the index page. However, if you render the site and some posts have descriptions and some don’t, you will get a really, really weird error about ETL text or something. What to do, is if you have some posts with and without descriptions, is add to the ones without (having an empty description is fine), and then delete the XML files in your output director (default is _site) directory, and re-knit the posts and rebuild the site. Hopefully that makes the error go away.\nTheming can be a bit of a pain\nOK, I brought this on myself, I admit. I really, really like dark themes. I know I could just leave it white, and keep using the excellent deluminate extension to keep things dark, but I believe if I like a dark website, then my actual website should be dark too. I had to modify a lot of divs to make my site look nice (you can see the theme here). Too many.\nTable of contents doesn’t seem to be site-wide\nIncluding table of contents in the yaml of _site.yml doesn’t make them magically appear on every page. You have to set them on each post. Therefore, you decide before you render the post.\nUtterances was easy!\nThanks to Miles McBain, getting the Utterances comment framework in was easy-peasy! And it’s something you can add after the fact, because it’s done as an HTML include and a simple addition to your _site.yml.\noutput:\n  distill::distill_article:\n    includes:\n      in_header: utterances.html\nTo check if it is working, you will need to serve the site using servr::httd(\"site_dir\") though, you can’t just open the html pages and look.\nSearch is awesome!\nHaving search on the website is soooo nice! And it searches the descriptions, so as long as you write something useful in the description, it will actually work. Again, testing it requires servr::httd().\nHighlighting must be set on each article\nIf I set the code highlight theme in _site.yml, it doesn’t seem to work.\nAdding Images\nIt took some reading to figure out I should be using knitr::include_graphics() to have images into my blog posts. From what I can tell, I should have been using this long ago.\nCitations, Footnotes, Asides\nThese are all really cool features that I want to use waaaayyy more in my posts. I started converting some of my posts to use them (see my code above that created refs.bib files in all of the post directories), and realized if I did that I’d never get the updated blog posted. So expect to see more of those things in new posts, and maybe as I get time (hah!) I’ll convert some older ones over.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-28T22:48:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-25-using-group-by-instead-of-splitting/",
    "title": "Using group_by Instead of Splits",
    "description": "How to use group_by instead of split's to summarize things.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2020-02-25",
    "categories": [
      "R",
      "dplyr",
      "split",
      "group-by",
      "programming",
      "development"
    ],
    "contents": "\n\nContents\nTL;DR\nMotivation\nGroup By\n\nTL;DR\nIt is relatively easy to use dplyr::group_by and summarise to find items that you might want to keep or remove based on a part_of the item or group in question. I used to use split and iterate, but group_by is much easier.\nMotivation\nI have some relatively large sets of data that fall naturally into groups of items. Often, I find that I want to remove a group that contains either any of or all of particular items. Let’s create some data as an example.\n\n\nlibrary(dplyr)\nset.seed(1234)\ngroups = as.character(seq(1, 1000))\ngrouped_data = data.frame(items = sample(letters, 10000, replace = TRUE),\n                          groups = sample(groups, 10000, replace = TRUE),\n                          stringsAsFactors = FALSE)\n\nknitr::kable(head(grouped_data))\n\n\nitems\ngroups\np\n891\nz\n646\nv\n795\ne\n49\nl\n19\no\n796\n\nIn this example, we have the 26 lowercase letters, that are part of one of groups 1-1000. Now, we might want to keep any groups that contain at least one “a”, for example.\nI would have previously used a split on the groups, and then purrr::map_lgl returning TRUE or FALSE to check if what we wanted to filter on was present, and then filter out the split groups, and finally put back together the full thing.\nGroup By\nWhat I’ve found instead is that I can use a combination of group_by, summarise and then filter to same effect, without splitting and iterating (yes, I know dplyr is doing it under the hood for me).\n\n\n# use group_by and summarize to find things we want\ngroups_to_keep = grouped_data %>% \n  group_by(groups) %>%\n  summarise(has_a = sum(items %in% \"a\") > 0) %>%\n  filter(has_a)\n\n# filter on original based on above\ngrouped_data2 = grouped_data %>%\n  filter(groups %in% groups_to_keep$groups)\n\n\n\nThis was a game changer for me in my thinking. As I’ve used group_by combined with summarise more and more, I’ve become amazed at what can be done without having to fully split the data apart to operate on it.\nThis combined with the use of dplyr::join_ in place of splits (see this other post for an example) is making my code faster, and often easier to reason over. I hope it helps you too!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-28T13:09:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-06-narrower-kable-tables/",
    "title": "Narrower PDF Kable Tables",
    "description": "This is how you should make narrower kable tables in rmarkdown PDF documents.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2019-11-06",
    "categories": [
      "R",
      "knitr",
      "kable",
      "learning",
      "rmarkdown"
    ],
    "contents": "\n\nContents\nTL;DR\nMotivation\nFirst Solution: Custom Function\nSecond Solution: kableExtra\n\nTL;DR\nDon’t bother trying to roll your own function to make narrower kable tables in a PDF document, just use kableExtra.\nMotivation\nI’ve been creating tables in a report where I really needed the table to fit, and because I am using PDF output, that means the tables can’t be any wider than the page. As I’m sure many readers might be aware, kable tables will gladly overrun the side of the page if they are too wide. I’ve previously used xtable tables when I’ve had this issue, but I really appreciate the simplicity of kable.\nFirst Solution: Custom Function\nAfter some serious Googling, I discovered the \\tiny latex environment to change font sizes. Wrapping pandoc tables in this was a no go, but I discovered that it could be embedded within latex table output. This lead me to create a simple function that allowed me to modify latex formatted tables.\nsmaller_latex_table = function(kable_table, size = \"tiny\"){\n  split_table = strsplit(kable_table, \"\\n\", )[[1]]\n  centering_loc = grepl(\"centering\", split_table)\n  top_table = split_table[seq(1, which(centering_loc))]\n  bottom_table = split_table[seq(which(centering_loc)+1, length(split_table))]\n  new_table = c(top_table,\n                paste0(\"\\\\\", size),\n                bottom_table)\n  structure(new_table, format = \"latex\", class = \"knitr_kable\")\n}\nThis worked! And worked quite well. However, the downside to this is that because I had to explicity use latex tables, the tables didn’t stay in place anymore and floated wherever there was free space in the document. Everything I tried with this to get the tables to hold in place failed. So back to the drawing board.\nSecond Solution: kableExtra\nBy this point I’ve spent a whole day’s worth of time trying to get this to work, just for some tables in my report. I had initially tried kableExtra on the suggestion of another StackOverflow post, but I had something odd in my latex environment, and odd things going on with tinyTex install that made some ugly tables. After re-installing tinyTex (no small feat to make it discoverable by apt installed RStudio on Linux), I finally got both smaller tables and held in place tables via kableExtra.\nTo make the tables fit the width of the page, we use latex_options = 'scale_down'.\nFor holding them to where they are declared, we use latex_options = 'HOLD_position'. However, this also requires the tex packages longtable and float, which should be declared in the yaml header.\nPutting it all together looks like this:\n## yaml header content\ntitle: \"Title\"\nauthor: \"Me\"\noutput: \n  pdf_document:\n    extra_dependencies: [\"longtable\", \"float\"]\n## table call\nknitr::kable(data.frame) %>%\n  kableExtra::kable_styling(latex_options = c(\"scale_down\", \"HOLD_position\"))\nNow the table will fit on the page, and stay where it was declared!\nI hope I can save someone else two days of trial and error and crazy Googling!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-28T13:08:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-23-introducing-scientific-programming/",
    "title": "Introducing Scientific Programming",
    "description": "How and when should we get people in academia programming? What if we had a unified front across the science labs?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2019-10-23",
    "categories": [
      "R",
      "reproducibility",
      "programming",
      "academia"
    ],
    "contents": "\n\nContents\nTL;DR\nWhy?\nUndergraduate Science Labs\nAn Alternative to Calculators and Excel\nChallenges in Implementation\nGetting all the Departments On Board\nConverting all the Labs\nSupporting the Students and Faculty\n\nHas Anyone Done This?\nInterested?\n\nTL;DR\nWe should get science undergraduate students programming by introducing R & Python in all first year science labs, and continuing throughout the undergraduate classes.\nWhy?\nI’ve previously encountered ideas around getting graduate students to get programming, because to do the analyses that modern science requires you need to be able to at least do some basic scripting, either in a language like Python or R or on the command line. As successful as programs like Software Carpentry are for getting graduate students and others further along their scientific career into a programming and command line mindset, I think it needs to be a lot earlier, and introduced in a way that it becomes second nature. Ergo, undergraduate science labs.\nUndergraduate Science Labs\nBased on my recollection of undergraduate labs and later being a chemistry lab teaching assistant (both during my undergraduate and graduate degrees), there is a lot of calculations going on. Physics labs involved performing experiments to determine underlying constants or known quantities. Chemistry labs often involved quantitative determinations, even more so as labs advanced over the years. Even my biology labs frequently involved calculations and generation of reports to hand in. In my first year, calculators were relied upon, along with example worksheets showing where to fill things in and how to do the calculations. Over the years, Microsoft Excel was eventually introduced, and at one point during my senior Analytical Lab was basically required.\nWhat if students were consistently introduced to another way to do the necessary calculations for their labs?\nAn Alternative to Calculators and Excel\nI’m imagining all of the undergraduate science labs, Biology, Chemistry, Physics and Geology, requiring the use of either Python or R to do the quantitative calculations and produce reports. It would likely require an immense effort on the part of teaching faculty for each lab, teaching assistants in the lab, as well as having one or two dedicated persons who are able to help students with issues running R / Python and getting packages installed. Ideally, students would be introduced to generating their full reports gradually, starting with highly scaffolded lab reports where they simply have to supply a few numbers, click knit (assuming we are using R within RStudio) and generate a report that can be submitted. Over time, the lab reports would involve more and more calculations being coded directly by the student, and more of the report written directly by them. This scaffolding would likely be repeated at each level of labs.\nChallenges in Implementation\nI see three main challenges in implementing something like the above.\nGetting all the Departments On Board\nSeriously, getting all of the lab teaching faculty on board so that this happens across all of the science departments and at all levels of labs. Although it would be very useful even if done consistently in a single department, I think the biggest bang for the buck is going to be all departments buying in.\nConverting all the Labs\nAfter convincing everyone that this is a worthy goal, then there is the herculean task of figuring out how to make the labs fit into this type of framework. The added wrinkle to this is that labs are frequently marked on how close one got to the right answer (known concentration of a standard, for example), which can depend both on how accurately one performs the experimental technique being taught, and how well one does the calculations (at least this was my experience as student and TA). Not only that, but many labs often had two parts, sometimes in the same lab period, where messing up the first part meant that you would have completely wrong answers in the second part. Getting even a first pass conversion would be a monumental effort on the part of someone with extensive scripting language knowledge and the teaching faculty.\nThere is also the question of how to convert the labs. I would imagine that first year would be done first, and then second, third, fourth. But this would also be interwoven with updates to the labs as issues are discovered within each year, and modifications made each year.\nSupporting the Students and Faculty\nLet’s face it, doing things this way requires more hardware than just having a calculator in lab with you. Python and R will install on almost any OS however, and the types of calculations necessary are not compute intensive. However, there will invariably be issues getting software and necessary packages installed, and keeping them up to date. There needs to be someone who is able to be present both in and outside lab time that can help diagnose package installation and update issues. This same person will also likely be tasked with helping teaching faculty and assistants install and update necessary software and packages.\nNot discussed above, but ideally the design of the lab reports should also make sure that they depend on a very low number of packages, and that those packages will install on any OS that the students come in with.\nHas Anyone Done This?\nI’d be really curious if anyone has attempted anything like this. Please leave a comment if you know of any!\nInterested?\nI’ll admit, being the person behind an effort like this is probably the one thing right now that would convince me to leave my current position in trying to solve cancer metabolomics. Drop me a line if this sounds interesting to you!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-28T13:06:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-16-comments-enabled-via-utterances/",
    "title": "Comments enabled via utterances",
    "description": "How I got utterances working on blogdown.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2019-10-16",
    "categories": [
      "blogdown",
      "commenting",
      "utterances"
    ],
    "contents": "\n\nContents\nTL;DR\nWhy Utterances\nHow\n\nTL;DR\nUtterances is a lightweight commenting platform built on GitHub issues. So you have to have a GitHub account, but I expect most people who comment on this blog already have one.\nWhy Utterances\nWhen I switched to blogdown, I lost my disqus comments. I had considered migrating them over, but never got around to it. I also thought that there had to be a way to link GitHub issues to blog posts, but didn’t investigate it much.\nThen, I came across Maëlle’s blog post about switching to utterances and I was sold. I had some free time last night, and dived into how to add it to my site that uses the hugo academic theme.\nI’m not expecting a lot of heavy commenting, but at least it’s now available!\nHow\nFor blogdown, just replace everything in layouts/partials/comments.html with the code snippet from utterances.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-28T13:04:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-13-for-loops-vs-split/",
    "title": "Comparisons using for loops vs split",
    "description": "for loops often hide much of the actual logic of your code because of all the necessary boilerplate of running a loop. split-ting your data can oftentimes be clearer, and faster.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2019-02-13",
    "categories": [
      "R",
      "for-loop",
      "split",
      "purrr",
      "development"
    ],
    "contents": "\n\nContents\nTL;DR\nComparing Groups\nLooping\n\nTL;DR\nSometimes for loops are useful, and sometimes they shouldn’t really be used, because they don’t really help you understand your data, and even if you try, they might still be slow(er) than other ways of doing things.\nComparing Groups\nI have some code where I am trying to determine duplicates of a group of things. This data looks something like this:\n\n\ncreate_random_sets = function(n_sets = 1000){\n  set.seed(1234)\n  \n  sets = purrr::map(seq(5, n_sets), ~ sample(seq(1, .x), 5))\n  \n  item_sets = sample(seq(1, length(sets)), 10000, replace = TRUE)\n  item_mapping = purrr::map2_df(item_sets, seq(1, length(item_sets)), function(.x, .y){\n    data.frame(v1 = as.character(.y), v2 = sets[[.x]], stringsAsFactors = FALSE)\n  })\n  item_mapping\n}\nlibrary(dplyr)\nmapped_items = create_random_sets()\n\nhead(mapped_items, 20)\n\n\n   v1  v2\n1   1 375\n2   1 255\n3   1 268\n4   1  52\n5   1 241\n6   2 143\n7   2 401\n8   2 127\n9   2 372\n10  2 100\n11  3  62\n12  3 109\n13  3  72\n14  3 390\n15  3  94\n16  4  57\n17  4  55\n18  4 147\n19  4 236\n20  4 120\n\nLooping\nIn this case, every item in v1 has 5 things in v2. I really want to group multiple things of v1 that have the same combination of things in v2. My initial function to do this splits everything in v2 by v1, and then compares all the splits to each other, removing things that have been compared and found to be the same, and saving them as we go. This required two loops, basically while there was data to check, check all the other things left in the list against it (the for). Pre-initialize the list of things that are identical to each other so we don’t take a hit on allocation, and delete the things that have been checked or noted as identical. Although the variable names are changed, the code for that function is below.\n\n\nloop_function = function(item_mapping){\n  split_items = split(item_mapping$v2, item_mapping$v1)\n  \n  matched_list = vector(\"list\", length(split_items))\n  \n  save_item = 1\n  save_index = 1\n  \n  while (length(split_items) > 0) {\n    curr_item = names(split_items)[save_item]\n    curr_set = split_items[[save_item]]\n    \n    for (i_item in seq_along(split_items)) {\n      if (sum(split_items[[i_item]] %in% curr_set) == length(curr_set)) {\n        matching_items = unique(c(curr_item, names(split_items)[i_item]))\n        save_item = unique(c(save_item, i_item))\n      }\n    }\n    matched_list[[save_index]] = curr_set\n    split_items = split_items[-save_item]\n    save_index = save_index + 1\n    save_item = 1\n  }\n  \n  n_in_set = purrr::map_int(matched_list, length)\n  matched_list = matched_list[n_in_set > 0]\n  n_in_set = n_in_set[n_in_set > 0]\n  matched_list\n}\n\n\n\nThe code works, but it doesn’t really make me think about what it’s doing, the two loops hide the fact that what is really going on is comparing things to one another. Miles McBain recently posted on this fact, that loops can be necessary, but one should really think about whether they are really necessary, or do they hide something about the data, and can we think about different ways to do the same thing.\nThis made me realize that what I really wanted to do was split the items in v1 by the unique combinations of things in v2, because split will group things together nicely for you, without any extra work. But I don’t have those combinations in a way that split can use them. So my solution is to iterate over the splits using purrr, create a representation of the group as a character value, and then call split again at the very end based on the character representation.\n\n\nsplit_function = function(item_mapping){\n  mapped_data = split(item_mapping$v2, item_mapping$v1) %>%\n    purrr::map2_dfr(., names(.), function(.x, .y){\n      set = unique(.x)\n      tmp_frame = data.frame(item = .y, set_chr = paste(set, collapse = \",\"), stringsAsFactors = FALSE)\n      tmp_frame$set = list(set)\n      tmp_frame\n    })\n  matched_list = split(mapped_data, mapped_data$set_chr)\n}\n\n\n\nNot only is the code cleaner, the grouping is explicit (as long as you know how split works), and its also 4x faster!\n\n\nmicrobenchmark::microbenchmark(\n  loop_function(mapped_items),\n  split_function(mapped_items),\n  times = 5\n)\n\n\nUnit: seconds\n                         expr      min       lq     mean   median\n  loop_function(mapped_items) 7.679313 7.897748 7.956645 7.907971\n split_function(mapped_items) 2.518144 2.597200 2.654516 2.623375\n       uq      max neval\n 8.117534 8.180659     5\n 2.722928 2.810932     5\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-28T11:56:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-06-nicer-png-graphics/",
    "title": "Nicer PNG Graphics",
    "description": "Here are some tips for getting nicer graphics in your rmarkdown outputs.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-12-06",
    "categories": [],
    "contents": "\n\nContents\nTL;DR\nPNG Graphics??\nIncreased Resolution\nUse SVG??\nPNG via Cairo\nIncorporating Into Reports\n\nTL;DR\nIf you are getting crappy looking png images from rmarkdown html or word documents, try using type='cairo' or dev='CairoPNG' in your chunk options.\nPNG Graphics??\nSo, I write a lot of reports using rmarkdown and knitr, and have been using knitr for quite a while. My job involves doing analyses for collaborators and communicating results. Most of the time, I will generate a pdf report, and I get beautiful graphics, thanks to the eps graphics device. However, there are times when I want to generate either word or html reports, and in those cases, I tend to get very crappy looking graphics. See this example image below:\n\n\nlibrary(ggplot2)\np = ggplot(mtcars, aes(mpg, wt)) +\n  geom_point(size = 3) +\n  labs(x=\"Fuel efficiency (mpg)\", y=\"Weight (tons)\",\n       title=\"Seminal ggplot2 scatterplot example\",\n       subtitle=\"A plot that is only useful for demonstration purposes\",\n       caption=\"Brought to you by the letter 'g'\")\np\n\n\n\n\nNote: This was generated on self-compiled R under Ubuntu 16.04. As we can see, knitr is using the png device, because we are generating html output.\n\n\nknitr::opts_chunk$get(\"dev\")\n\n\n[1] \"png\"\n\nIncreased Resolution\nOf course, we just need to increase the resolution! So let’s do so. Just to go whole hog on this, let’s increase it to 300!\n\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, 300 dpi\")\n\n\n\n\nIf you compare this one to the previous, you can see that the quality is marginally better, but doesn’t seem to be anything like what you should be able to get.\nUse SVG??\nAlternatively, we could tell knitr to use the svg device instead! Vector graphics always look nice!\n\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, dev = 'svg'\")\n\n\n\n\nIt’s so crisp! But, for word documents especially, this could be a problem, as the images might not show up. The nice thing about png is it should be usable in just about any format!\nAnd, if you have a plot with a lot of points (> 200), the svg will start to take up some serious disk space, as every single point is encoded in the svg file. This is also a good reason to use png.\nPNG via Cairo\nAfter pulling out my hair yesterday as I tried to generate nice png images embedded in a word report (and settling on converting every figure from svg to png and saving to a folder to pass on, see this), I finally decided to try a different device.\nNow, your R installation does need to have either cairo capabilities, or be able to use the Cairo package. Mine has both.\n\n\ncapabilities()\n\n\n       jpeg         png        tiff       tcltk         X11 \n       TRUE        TRUE       FALSE        TRUE        TRUE \n       aqua    http/ftp     sockets      libxml        fifo \n      FALSE        TRUE        TRUE        TRUE        TRUE \n     cledit       iconv         NLS     profmem       cairo \n      FALSE        TRUE        TRUE       FALSE        TRUE \n        ICU long.double     libcurl \n       TRUE        TRUE        TRUE \n\npackageVersion(\"Cairo\")\n\n\n[1] '1.5.12.2'\n\nLet’s change the device (two different ways) and plot it again. First, we will still use the png device, but add the type = \"cairo\" argument (see ?png). Just for information, that looks like the below in the chunk options:\nr plot_cairo, dev.args = list(type = \"cairo\")\n\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, type = 'cairo'\")\n\n\n\n\nWow! This looks great! So much nicer than the other device. Secondly, let’s use the CairoPNG device (dev = \"CairoPNG\")\n\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, dev = 'CairoPNG'\")\n\n\n\n\nFinally, we can also increase the resolution as well.\n\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, dev = 'CairoPNG', dpi = 300\")\n\n\n\n\nSo there you have it. Very crisp png images, with higher resolutions if needed, and no jaggedness, without resorting to conversion via inkscape (my previous go to).\nIncorporating Into Reports\nAs I previously mentioned, I often default to pdf reports, but will then generate a word or html report if necessary. How do you avoid changing the options even in a setup chunk if you want this to happen every time you specify word_document as the output type? This is what I settled on, the setup chunk checks the output type (based on being called from rmarkdown::render), and sets it appropriately.\nif (knitr::opts_knit$get(\"rmarkdown.pandoc.to\") != \"latex\") {\n  knitr::opts_chunk$set(dpi = 300, dev.args = list(type = \"cairo\"))\n})\n\n\n\n",
    "preview": "posts/2018-12-06-nicer-png-graphics/nicer-png-graphics_files/figure-html5/first_image-1.png",
    "last_modified": "2021-02-28T11:53:31-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/",
    "title": "Don't do PCA After Statistical Testing!",
    "description": "You might be tempted to do PCA after a statistical test. Read more to discover why this is a bad idea.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-09-14",
    "categories": [
      "pca",
      "bioinformatics",
      "R",
      "t-test"
    ],
    "contents": "\n\nContents\nTL;DR\nWait, Why??\nThe Problem\nAn Example\nRandom Data\n\nTake Away\n\nTL;DR\nIf you do a statistical test before a dimensional reduction method like PCA, the highest source of variance is likely to be whatever you tested statistically.\nWait, Why??\nLet me describe the situation. You’ve done an -omics level analysis on your system of interest. You run a t-test (or ANOVA, etc) on each of the features in your data (gene, protein, metabolite, etc). Filter down to those things that were statistically significant, and then finally, you decide to look at the data using a dimensionality reduction method such as principal components analysis (PCA) so you can see what is going on.\nI have seen this published at least once (in a Diabetes metabolomics paper, if anyone knows it, please send it to me so I can link it), and have seen collaborators do this after coaching from others in non-statistical departments.\nThe Problem\nThe problem is that PCA is just looking at either feature-feature covariances or sample-sample covariances. If you have trimmed the data to those things that have statistically significant differences, then you have completely modified the covariances, and PCA is likely to pick up on that.\nAn Example\nLet’s actually do an example where there are no differences initially, and then see if we can introduce an artificial difference.\nRandom Data\nWe start with completely random data, 10000 features, and 100 samples.\n\n\nn_feat = 10000\nn_sample = 100\nrandom_data = matrix(rnorm(n_feat * n_sample), nrow = n_feat, ncol = n_sample)\n\n\n\nNow we will do a t-test on each row, taking the first 50 samples as class 1 and the other 50 samples as class 2.\n\n\nt_test_res = purrr::map_df(seq(1, nrow(random_data)), function(in_row){\n  tidy(t.test(random_data[in_row, 1:50], random_data[in_row, 51:100]))\n})\n\n\n\nHow many are significant at a p-value of 0.05?\n\n\nfilter(t_test_res, p.value <= 0.05) %>% dim()\n\n\n[1] 514  10\n\nObviously, these are false positives, but they are enough for us to illustrate the problem.\nFirst, lets do PCA on the whole data set of 10000 features.\n\n\nsample_classes = data.frame(class = c(rep(\"A\", 50), rep(\"B\", 50)))\n\nall_pca = prcomp(t(random_data), center = TRUE, scale. = FALSE)\npca_scores = cbind(as.data.frame(all_pca$x), sample_classes)\nggplot(pca_scores, aes(x = PC1, y = PC2, color = class)) + geom_point()\n\n\n\n\nObviously, there is no difference in the groups, and the % explained variance is very low.\nSecond, lets do it on just those things that were significant:\n\n\nsig_pca = prcomp(t(random_data[which(t_test_res$p.value <= 0.05), ]), center = TRUE,\n                 scale. = FALSE)\nsig_scores = cbind(as.data.frame(sig_pca$x), sample_classes)\n\nggplot(sig_scores, aes(x = PC1, y = PC2, color = class)) + geom_point()\n\n\n\n\nAnd look at that! We have separation of the two groups! But …., this is completely random data, that didn’t have any separation, until we did the statistical test!\nTake Away\nBe careful of the order in which you do things. If you want to do dimensionality reduction to look for issues with the samples, then do that before any statistical testing on the individual features.\n\n\n\n",
    "preview": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/don-t-do-pca-after-statistical-testing_files/figure-html5/all_pca-1.png",
    "last_modified": "2021-02-28T11:51:23-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/",
    "title": "Finding Modes Using Kernel Density Estimates",
    "description": "Examples of finding the mode of a univeriate distribution in R and Python.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-07-19",
    "categories": [
      "R",
      "python",
      "kernel-density",
      "pdf",
      "probability-density",
      "programming"
    ],
    "contents": "\n\nContents\nTL; DR\nMode in R\nPython\n\nTL; DR\nIf you have a unimodal distribution of values, you can use R’s density or Scipy’s gaussian_kde to create density estimates of the data, and then take the maxima of the density estimate to get the mode. See below for actual examples in R and Python.\nMode in R\nFirst, lets do this in R. Need some values to work with.\n\n\nlibrary(ggplot2)\nset.seed(1234)\nn_point <- 1000\ndata_df <- data.frame(values = rnorm(n_point))\n\nggplot(data_df, aes(x = values)) + geom_histogram()\n\n\n\nggplot(data_df, aes(x = values)) + geom_density()\n\n\n\n\nWe can do a kernel density, which will return an object with a bunch of peices. One of these is y, which is the actual density value for each value of x that was used! So we can find the mode by querying x for the maxima in y!\n\n\ndensity_estimate <- density(data_df$values)\n\nmode_value <- density_estimate$x[which.max(density_estimate$y)]\nmode_value\n\n\n[1] -0.04599328\n\nPlot the density estimate with the mode location.\n\n\ndensity_df <- data.frame(value = density_estimate$x, density = density_estimate$y)\n\nggplot(density_df, aes(x = value, y = density)) + geom_line() + geom_vline(xintercept = mode_value, color = \"red\")\n\n\n\n\nPython\nLets do something similar in Python. Start by generating a set of random values.\n\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nvalues = np.random.normal(size = 1000)\n\nplt.hist(values)\n(array([  5.,  19.,  70., 145., 233., 254., 173.,  86.,  10.,   5.]), array([-3.40466956, -2.73500426, -2.06533895, -1.39567364, -0.72600834,\n       -0.05634303,  0.61332228,  1.28298759,  1.95265289,  2.6223182 ,\n        3.29198351]), <a list of 10 Patch objects>)\nplt.show()\n\n\nAnd then use gaussian_kde to get a kernel estimator of the density, and then call the pdf method on the original values.\n\nkernel = stats.gaussian_kde(values)\nheight = kernel.pdf(values)\n\nmode_value = values[np.argmax(height)]\nprint(mode_value)\n0.05435206135566903\n\nPlot to show indeed we have it right. Note we sort the values first so the PDF looks right.\n\nvalues2 = np.sort(values.copy())\nheight2 = kernel.pdf(values2)\n\nplt.clf()\nplt.cla()\nplt.close()\n\nplt.plot(values2, height2)\nplt.axvline(mode_value)\nplt.show()\n\n\n\n\n\n",
    "preview": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/finding-modes-using-kernel-density-estimates_files/figure-html5/density_mode-1.png",
    "last_modified": "2021-02-28T11:45:49-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2018-07-17-split-unsplit-anti-pattern/",
    "title": "Split - Unsplit Anti-Pattern",
    "description": "Getting some speed using dplyr::join than my more intuitive split --> unsplit pattern.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-07-17",
    "categories": [
      "R",
      "development",
      "programming",
      "purrr",
      "dplyr",
      "join"
    ],
    "contents": "\n\nContents\nTL;DR\nMotivation\nMatch Them!\nSplit Them!\nJoin Them!\nConclusions\n\nTL;DR\nIf you notice yourself using split -> unsplit / rbind on two object to match items up, maybe you should be using dplyr::join_ instead. Read below for concrete examples.\nMotivation\nI have had a lot of calculations lately that involve some sort of normalization or scaling a group of related values, each group by a different factor.\nLets setup an example where we will have 1e5 values in 10 groups, each group of values being normalized by their own value.\n\n\nlibrary(microbenchmark)\nlibrary(profvis)\nset.seed(1234)\nn_point <- 1e5\nto_normalize <- data.frame(value = rnorm(n_point), group = sample(seq_len(10), n_point, replace = TRUE))\n\nnormalization <- data.frame(group = seq_len(10), normalization = rnorm(10))\n\n\n\nFor each group in to_normalize, we want to apply the normalization factor in normalization. In this case, I’m going to do a simple subtraction.\nMatch Them!\nMy initial implementation was to iterate over the groups, and use %in% to match each group from the normalization factors and the data to be normalized, and modify in place. Don’t do this!! It was the slowest method I’ve used in my real package code!\n\n\nmatch_normalization <- function(normalize_data, normalization_factors){\n  use_groups <- normalization_factors$group\n  \n  for (igroup in use_groups) {\n    normalize_data[normalize_data$group %in% igroup, \"value\"] <- \n      normalize_data[normalize_data$group %in% igroup, \"value\"] - normalization_factors[normalization_factors$group %in% igroup, \"normalization\"]\n  }\n  normalize_data\n}\n\n\n\n\n\nmicro_results <- summary(microbenchmark(match_normalization(to_normalize, normalization)))\nknitr::kable(micro_results)\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\nmatch_normalization(to_normalize, normalization)\n33.52854\n35.79094\n36.64787\n36.27017\n36.78996\n65.96424\n100\n\nNot bad for the test data. But can we do better?\nSplit Them!\nMy next thought was to split them by their groups, and then iterate again over the groups using purrr::map, and then unlist them.\n\n\nsplit_normalization <- function(normalize_data, normalization_factors){\n  split_norm <- split(normalization_factors$normalization, normalization_factors$group)\n  \n  split_data <- split(normalize_data, normalize_data$group)\n  \n  out_data <- purrr::map2(split_data, split_norm, function(.x, .y){\n    .x$value <- .x$value - .y\n    .x\n  })\n  do.call(rbind, out_data)\n}\n\n\n\n\n\nmicro_results2 <- summary(microbenchmark(match_normalization(to_normalize, normalization),\n               split_normalization(to_normalize, normalization)))\nknitr::kable(micro_results2)\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\nmatch_normalization(to_normalize, normalization)\n33.71692\n41.09938\n43.17703\n42.23761\n43.82695\n91.57123\n100\nsplit_normalization(to_normalize, normalization)\n77.33925\n85.10808\n91.23737\n91.13441\n94.35732\n161.79138\n100\n\nJoin Them!\nMy final thought was to join the two data.frame’s together using dplyr, and then they are automatically matched up.\n\n\njoin_normalization <- function(normalize_data, normalization_factors){\n  normalize_data <- dplyr::right_join(normalize_data, normalization_factors,\n                                      by = \"group\")\n  \n  normalize_data$value <- normalize_data$value - normalize_data$normalization\n  normalize_data[, c(\"value\", \"group\")]\n}\n\n\n\n\n\nmicro_results3 <- summary(microbenchmark(match_normalization(to_normalize, normalization),\n               split_normalization(to_normalize, normalization),\n               join_normalization(to_normalize, normalization)))\nknitr::kable(micro_results3)\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\nmatch_normalization(to_normalize, normalization)\n33.974007\n41.813316\n45.47936\n43.089628\n46.33666\n104.7553\n100\nsplit_normalization(to_normalize, normalization)\n76.452060\n85.275248\n90.35035\n88.315268\n92.88813\n152.2833\n100\njoin_normalization(to_normalize, normalization)\n7.743028\n8.386193\n13.59934\n8.650142\n14.31905\n292.2638\n100\n\nConclusions\nSo on my computer, the split and match implementations are mostly comparable, although on my motivating real world example, I actually got a 3X speedup by using the split method. That may be because of issues related to DataFrame and matching elements within that structure. The join method is 10-14X faster than the others, which is what I’ve seen in my motivating work. I also think it makes the code easier to read and reason over, because you can see what is being subtracted from what directly in the code.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:29:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-23-iranges-for-non-integer-overlaps/",
    "title": "Using IRanges for Non-Integer Overlaps",
    "description": "I wanted to make use of IRanges awesome interval logic, but for non-integer data.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-06-23",
    "categories": [
      "R",
      "iranges",
      "development",
      "programming"
    ],
    "contents": "\n\nContents\nTL;DR\nIRanges??\nMotivation\nIRanges to the Rescue!\nHow Fast Is It?\nFunctions\nDefine Samples of Different Sizes\nRun It\nPlot Them\n\n\nTL;DR\nThe IRanges package implements interval algebra, and is very fast for finding overlaps of two ranges. If you have non-integer data, multiply values by a large constant factor and round them. The constant depends on how much accuracy you need.\nIRanges??\nIRanges is a bioconductor package for interval algebra of integer ranges. It is used extensively in the GenomicRanges package for finding overlaps between various genomic features. For genomic features, integers make sense, because one cannot have fractional base locations.\nHowever, IRanges uses red-black trees as its data structure, which provide very fast searches of overlaps. This makes it very attractive for any problem that involves overlapping ranges.\nMotivation\nMy motivation comes from mass-spectrometry data, where I want to count the number of raw data points and / or the number of peaks in a large number of M/Z windows. Large here means on the order of 1,000,000 M/Z windows.\nGenerating the windows is not hard, but searching the list of points / peaks for which ones are within the bounds of a window takes a really long time. Long enough that I needed some other method.\nIRanges to the Rescue!\nSo my idea was to use IRanges. But there is a problem, IRanges is for integer ranges. How do we use this for non-integer data? Simple, multiply and round the fractional numbers to generate integers.\nIt turns out that multiplying our mass-spec data by 20,000 gives us differences down to the 0.00005 place, which is more than enough accuracy for the size of the windows we are interested in. If needed, IRanges can handle 1600 * 1e6, but currently will crash at 1600 * 1e7.\nHow Fast Is It?\nLets actually test differences in speed by counting how many overlapping points there are.\n\n\nlibrary(IRanges)\nlibrary(ggplot2)\nload(\"iranges_example_data.rda\")\n\nhead(mz_points)\n\n\nIRanges object with 6 ranges and 1 metadata column:\n          start       end     width |        mz\n      <integer> <integer> <integer> | <numeric>\n  [1]   2970182   2970182         1 |   148.509\n  [2]   2970183   2970183         1 |   148.509\n  [3]   2970184   2970184         1 |   148.509\n  [4]   2970186   2970186         1 |   148.509\n  [5]   3000526   3000526         1 |   150.026\n  [6]   3000527   3000527         1 |   150.026\n\nhead(mz_windows)\n\n\nIRanges object with 6 ranges and 2 metadata columns:\n          start       end     width |  mz_start    mz_end\n      <integer> <integer> <integer> | <numeric> <numeric>\n  [1]   2960000   2960011        12 |       148   148.001\n  [2]   2960001   2960012        12 |       148   148.001\n  [3]   2960002   2960013        12 |       148   148.001\n  [4]   2960003   2960014        12 |       148   148.001\n  [5]   2960004   2960016        13 |       148   148.001\n  [6]   2960006   2960017        12 |       148   148.001\n\nI have some example data with 3447542 windows, and 991816 points. We will count how many point there are in each window using the below functions, with differing number of windows.\nFunctions\n\n\ncount_overlaps_naive <- function(mz_start, mz_end, points){\n  sum((points >= mz_start) & (points <= mz_end))\n}\n\niterate_windows <- function(windows, points){\n  purrr::pmap_int(windows, count_overlaps_naive, points)\n}\n\nrun_times_iterating <- function(windows, points){\n  t <- Sys.time()\n  window_counts <- iterate_windows(windows, points)\n  t2 <- Sys.time()\n  run_time <- difftime(t2, t, units = \"secs\")\n  run_time\n}\n\nrun_times_countoverlaps <- function(windows, points){\n  t <- Sys.time()\n  window_counts <- countOverlaps(points, windows)\n  t2 <- Sys.time()\n  run_time <- difftime(t2, t, units = \"secs\")\n  run_time\n}\n\n\n\nDefine Samples of Different Sizes\n\n\nset.seed(1234)\n\nsample_sizes <- c(10, 100, 1000, 10000, 50000, 100000)\n\nwindow_samples <- purrr::map(sample_sizes, function(x){sample(length(mz_windows), size = x)})\n\n\n\nRun It\n\n\niranges_times <- purrr::map_dbl(window_samples, function(x){\n  run_times_countoverlaps(mz_windows[x], mz_points)\n})\n\nwindow_frame <- as.data.frame(mcols(mz_windows))\n\nnaive_times <- purrr::map_dbl(window_samples, function(x){\n  run_times_iterating(window_frame[x, ], mz_points)\n})\n\n\n\nPlot Them\n\n\nall_times <- data.frame(size = rep(sample_sizes, 2),\n                        time = c(iranges_times, naive_times),\n                        method = rep(c(\"iranges\", \"naive\"), each = 6))\n\np <- ggplot(all_times, aes(x = log10(size), y = time, color = method)) + geom_point() + geom_line() + labs(y = \"time (s)\", x = \"log10(# of windows)\", title = \"Naive & IRanges Timings\") + theme(legend.position = c(0.2, 0.8))\np\n\n\n\np + ylim(c(0, 1))\n\n\n\n\nAs the two figures show, the naive solution, while a little faster under 1000 regions, is quickly outperformed by IRanges, whose time increases much more slowly.\n\n\n\n",
    "preview": "posts/2018-06-23-iranges-for-non-integer-overlaps/iranges-for-non-integer-overlaps_files/figure-html5/difference_times-1.svg",
    "last_modified": "2021-02-27T18:24:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-06-20-turn-roberts-beard-purple/",
    "title": "Turn Robert's Beard Purple!",
    "description": "I'm trying to raise money for the Walk for Alzheimer's, will you sponsor me?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-06-20",
    "categories": [
      "alzheimers",
      "fundraising"
    ],
    "contents": "\n\nContents\nTL;DR\nUpdates!\nJuly 10\n\nVideo\nWhy a Purple Beard??\n\nTL;DR\nIf I raise $100 by August 25ths The Walk to End Alzheimer’s, I will have my beard dyed purple in support of Alzheimer’s awareness.\nIf you are in another country, donate to your local Alzheimer’s charity and email me with the subject walk so I count it towards my total.\nLinks:\nMy donation page (Charity report on Alzheimer’s Association)\nFacebook Fundraising Page (if you want to share it!)\nAlzheimer Society Canada\nAlzheimer’s Society UK\nUpdates!\nJuly 10\nThanks to everyone on Facebook, we’ve raised $5 for the American Alzheimer’s Association, and $30 for the Alzheimer Society in Canada!\nBack on June 25, I went in to Bak 4 More Studio and the awesome Brittany B did some testing to see how hard it would be to lighten my beard so it can be colored. She was great, and thinks it will be able to be done all in one session. I am currently booked to go in on Monday, August 20th to get the coloring done. Thanks again to Helue and Brittany for their help in this!\nVideo\n\n\nWhy a Purple Beard??\nI decided to participate in the Walk To End Alzheimer’s this year, coming up on August 25th here in Lexington. I will be walking the 2 miles in support of my Mom, who was diagnosed with early onset Alzheimer’s some time ago (she will turn 68 this summer).\nWhy am I walking?? Because I’ve seen a little bit of what Alzheimer’s does, and although my Mom won’t benefit from new treatments, we need to find effective treatments for this devastating disease.\nAs an incentive to donate, if I reach my fundraising goal of $100, I will have my beard dyed Alzheimer’s Association purple! The wonderful folks at Bak 4 More studios have agreed to help me dye my beard purple before the Walk date. I will make sure to take lots of video and pictures of the process, which I am told will probably involve two trips to the studio, one to lighten my beard and a second to actually apply the purple color.\nUsing the magic of computers, I’ve tried to show here what that might look like. I’m sure it will look 100X better than these, this is just to give a possible idea.\n\nI also realize that not everyone may be able to donate to the American Alzheimer’s Association, so if you want to support my fundraising for the Walk (and turn my beard purple) by donating to your local Alzheimer’s charity, just send me an email with the subject walk with the amount you donated and to which charity, and I will count your donation towards your local charity towards my goal. I will also respond to your email so that you know I’ve counted it.\nI will NOT be trimming my beard between now and the Walk date, so there will be even more purple beard to go around, with 9 weeks between now and then.\nThank you for helping me raise funds for the Walk to End Alzheimer’s, and feel free to spread this post far and wide.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:13:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-02-19-knitrprogressbar/",
    "title": "knitrProgressBar Package",
    "description": "Ever wanted a progress bar output visible in a knitr document? Now you can!",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-02-19",
    "categories": [
      "packages",
      "R",
      "development"
    ],
    "contents": "\n\nContents\nTL;DR\nWhy Yet Another Progress Bar??\nHow??\nMulti-Processing\n\nTL;DR\nIf you like dplyr progress bars, and wished you could use them everywhere, including from within Rmd documents, non-interactive shells, etc, then you should check out knitrProgressBar (cran github).\nWhy Yet Another Progress Bar??\nI didn’t set out to create another progress bar package. But I really liked dplyrs style of progress bar, and how they worked under the hood (thanks to the examples from Bob Rudis).\nAs I used them, I noticed that no progress was displayed if you did rmarkdown::render() or knitr::knit(). That just didn’t seem right to me, as that means you get no progress indicator if you want to use caching facilities of knitr. So this package was born.\nHow??\nThese are pretty easy to setup and use.\n\n\nlibrary(knitrProgressBar)\n\n# borrowed from example by @hrbrmstr\narduously_long_nchar <- function(input_var, .pb=NULL) {\n  \n  update_progress(.pb) # function from knitrProgressBar\n  \n  Sys.sleep(0.01)\n  \n  nchar(input_var)\n  \n}\n\n# using stdout() here so progress is part of document\npb <- progress_estimated(26, progress_location = stdout())\n\npurrr::map(letters, arduously_long_nchar, .pb = pb)\n\n\n\n|===                                         |  8% ~1 s remaining     \n|===========                                 | 27% ~0 s remaining     \n|====================                        | 46% ~0 s remaining     \n|============================                | 65% ~0 s remaining     \n|=====================================       | 85% ~0 s remaining     \nCompleted after 0 s                                                   \n[[1]]\n[1] 1\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 1\n\n[[4]]\n[1] 1\n\n[[5]]\n[1] 1\n\n[[6]]\n[1] 1\n\n[[7]]\n[1] 1\n\n[[8]]\n[1] 1\n\n[[9]]\n[1] 1\n\n[[10]]\n[1] 1\n\n[[11]]\n[1] 1\n\n[[12]]\n[1] 1\n\n[[13]]\n[1] 1\n\n[[14]]\n[1] 1\n\n[[15]]\n[1] 1\n\n[[16]]\n[1] 1\n\n[[17]]\n[1] 1\n\n[[18]]\n[1] 1\n\n[[19]]\n[1] 1\n\n[[20]]\n[1] 1\n\n[[21]]\n[1] 1\n\n[[22]]\n[1] 1\n\n[[23]]\n[1] 1\n\n[[24]]\n[1] 1\n\n[[25]]\n[1] 1\n\n[[26]]\n[1] 1\n\nThe main difference to dplyrs progress bars is that here you have the option to set where the progress gets written to, either automatically using the built-in make_kpb_output_decisions(), or directly. Also, I have provided the update_progress function to actually do the updating or finalizing properly.\nThere are also package specific options to control how the decisions are made.\nSee the main documentation, as well as the included vignette.\nMulti-Processing\nAs of V1.1.0 (should be on CRAN soon), the package also supports indicating progress on multi-processed jobs. See the included vignette for more information.\nBy the way, I know this method is not ideal, but I could not get the combination of later and processx to work in my case. If anyone is willing to help out, that would be great.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:11:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-02-14-licensing-r-packages-that-include-others-code/",
    "title": "Licensing R Packages that Include Others Code",
    "description": "I wanted to include others code in my package, and couldn't find any good resources.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-02-14",
    "categories": [
      "R",
      "packages",
      "open-science",
      "licensing"
    ],
    "contents": "\n\nContents\nTL;DR\nMotivation\nInformation from CRAN\nFull Answer\n\nTL;DR\nIf you include others code in your own R package, list them as contributors with comments about what they contributed, and add a license statement in the file that includes their code.\nMotivation\nI recently created the knitrProgressBar package. It is a really simple package, that takes the dplyr progress bars and makes it possible for them to write progress to a supplied file connection. The dplyr package itself is licensed under MIT, so I felt fine taking the code directly from dplyr itself. In addition, I didn’t want my package to depend on dplyr, so I wanted that code self-contained in my own package, and I wanted to be able to modify underlying mechanics that might have been more complicated if I had just made a new class of progress bar that inherited from dplyr’s.\nI also wanted to be able to release my code on CRAN, not just on GitHub. I knew to accomplish that I would have to have all the license stuff correct. However, I had not seen any guide on how to license a package and give proper attribution in the Authors@R field.\nNote that I did ask this question on StackOverflow and ROpenSci forums as well.\nInformation from CRAN\nI should note here, that the CRAN author guidelines do provide a small hint in this regard in the Writing R Extensions guide:\n\nNote that all significant contributors must be included: if you wrote an R wrapper for the work of others included in the src directory, you are not the sole (and maybe not even the main) author.\n\nHowever, there is not any guidance provided on how these should ideally be listed.\nFull Answer\nI am the main author and maintainer of the new package, that is easy. The original code is MIT licensed, authored by several persons, and has copyright held by RStudio.\nMy solution then was to:\nadd the MIT license from dplyr to the file that has the progress bar code\nadd all the authors of dplyr as contributors to my package, with a comment as to why they are listed\nadd RStudio as a copyright holder to my package, with a comment that this only applies to the one file\nSo the Authors@R line in my DESCRIPTION ended up being:\nAuthors@R: c(person(given = c(\"Robert\", \"M\"), family = \"Flight\", email = \"rflight79@gmail.com\", role = c(\"aut\", \"cre\")),\n            person(\"Hadley\", \"Wickham\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"Romain\", \"Francois\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"Lionel\", \"Henry\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"Kirill\", \"Müller\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"RStudio\", role = \"cph\", comment = \"Copyright holder of included dplyr fragments\"))\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:10:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-01-17-docopt-numeric-optoins/",
    "title": "docopt & Numeric Options",
    "description": "Every input is a string in docopt. Every Input!!",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2018-01-17",
    "categories": [
      "R",
      "development",
      "programming",
      "docopt"
    ],
    "contents": "\n\nContents\nTL;DR\nSetup\nCan’t You Easily Tell It’s Character?\n\nTL;DR\nIf you use the docopt package to create command line R executables that take options, there is something to know about numeric command line options: they should have as.double before using them in your script.\nSetup\nLets set up a new docopt string, that includes both string and numeric arguments.\n\n\n\"\nUsage:\n  test_numeric.R [--string=<string_value>] [--numeric=<numeric_value>]\n  test_numeric.R (-h | --help)\n  test_numeric.R\n\nDescription: Testing how values are passed using docopt.\n\nOptions:\n  --string=<string_value>  A string value [default: Hi!]\n  --numeric=<numeric_value>   A numeric value [default: 10]\n\n\" -> doc\n\n\n\n\n\nlibrary(methods)\nlibrary(docopt)\n\nscript_options <- docopt(doc)\n\nscript_options\n\n\n\n## List of 8\n##  $ --string : chr \"Hi!\"\n##  $ --numeric: chr \"10\"\n##  $ -h       : logi FALSE\n##  $ --help   : logi FALSE\n##  $ string   : chr \"Hi!\"\n##  $ numeric  : chr \"10\"\n##  $ h        : logi FALSE\n##  $ help     : logi FALSE\n## NULL\nIt is very easy to see here, that the numeric argument is indeed a string, and if you want to use it as numeric, it should first be converted using as.double, as.integer, or even as.numeric.\nCan’t You Easily Tell It’s Character?\nI just bring this up because I recently used docopt to provide interfaces to three executables scripts, and I spent a lot of time printing the doc strings, and I somehow never noticed that the numeric values were actually character and needed to be converted to a numeric first. Hopefully this will save someone else some time in that regard.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:09:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-27-custom-deployment-script/",
    "title": "Custom Deployment Script",
    "description": "I don't want to use Netlify for hosting, so I came up with this simple script to deploy my blog.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2017-12-27",
    "categories": [
      "R",
      "blogdown",
      "development",
      "random-code-snippets"
    ],
    "contents": "\n\nContents\nTL;DR\nWhy?\nThe Script\n\nTL;DR\nUse a short bash script to do deployment from your own computer directly to your *.github.io domain.\nWhy?\nSo Yihui recommends using Netlify, or even Travis-CI in the Blogdown book. I wasn’t willing to setup a custom domain yet, and some of my posts involve a lot of personally created packages, etc, that I don’t want to debug installation on Travis. So, I wanted a simple script I could call on my laptop that would copy the /public directory to the repo for my github.io site, and then push the changes.\nThe Script\nHere is the simple script I ended up using:\n#!/bin/bash\norg_dir=`pwd`\ncd path/to/github.io/repo/\n#rm -rf *\ncp -Rfu path/to/blogdown/public/* .\n\ngit add *\ncommit_time=`date`\ngit commit -m \"update at $commit_time\"\ngit push origin master\n\ncd $org_dir\nIt changes directories, because to push from a git repo I’m pretty sure you need to be in the directory, so it also makes sure to go back there at the end. It then copies the contents of /public to the repo, adds all the files, and then uses the current time-stamp as the commit message, and finally pushes all the updates.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-05T10:24:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-27-differences-in-posted-date-vs-sessioninfo/",
    "title": "Differences in Posted Date vs sessionInfo()",
    "description": "If you see differences in the sessionInfo output and the date the post was published, this is why.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2017-12-27",
    "categories": [
      "development",
      "blogdown"
    ],
    "contents": "\nIf you are a newcomer to my weblog, you may notice that some posts that are R tutorials generally include the output of Sys.time() at the end. If you look closeley at that time and the Posted on date, you may notice that some posts show disagreement between them. This is because I decided to move all of my old blog posts from blogspot to here, and keep the original posted dates.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:06:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-12-27-linking-to-manually-inserted-images-in-hugo/",
    "title": "Linking to Manually Inserted Images in Blogdown / Hugo",
    "description": "This is my method to include something manually in a blogdown post.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2017-12-27",
    "categories": [
      "hugo",
      "R",
      "blogdown"
    ],
    "contents": "\n\nContents\nManual Linking?\nWhere to??\n\nManual Linking?\nUsing blogdown for generating websites and blog-posts from Rmarkdown files with lots of inserted code and figures seems pretty awesome, but sometimes you want to include a figure manually, either because you want to generate something manually and convert it (say for going from SVG of lots of points to hi-res PNG), or because it is a figure from something else (like this figure from wikipedia).\nWhere to??\nTo do this, you want the text of your <img> tag to your image to be:\n<img src = \"/img/image_file.png\"><\/img>\nAnd then put the image itself in the directory /static/img/image_file.png\n\nBy M. W. Toews, CC BY 2.5, via Wikimedia Commons, source\nThis information is also mentioned in section 2.7 of the Blogdown book. Obviously I need to do more reading.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:07:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-10-18-i-was-part-of-the-problem/",
    "title": "I was Part of the Problem",
    "description": "Why do so many men think it's OK to lavish unwanted attention on women who don't want it?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2017-10-18",
    "categories": [
      "metoo",
      "academia"
    ],
    "contents": "\n\nContents\nTL;DR\nHow Could I Be?\nGrade School\nUndergraduate\nPost-Graduate\nThe Real Problem\nSolutions\nCaveats\n\nTL;DR\nWith the recent charges of sexual harassment against some high-profile individuals, and so many women coming forward with #metoo (and the understanding that this is really something almost all women have faced), I realized that my younger self was #partoftheproblem. I think many other men are part of the problem, even though they might not think so. I didn’t think I was part of the problem either. I hope that other men might read this and critically evaluate if they are #partoftheproblem. I also hope and pray that my own sons will do better at this if I teach them right.\nHow Could I Be?\nLet me be up front. I have never sexually assaulted anyone, let alone considered such a thing. But that’s not really the problem, because the way I acted towards women, I think they may have been scared that I might, as I have put tons of unwanted attention on several women over the years, starting with when I was 12 years old, in the sixth grade.\nI also want to be clear, I was a horrible guy friend to women (even if they didn’t think so). If I knew a girl had a boyfriend, well then, I would not even consider trying to hit on or express interest in that girl, and I was “friends” with plenty of women over the course of my school years who had boyfriends. But in most cases I secretly hoped they might dump their boyfriends and go out with me instead. Also, if I knew they didn’t have a boyfriend, and I found them remotely attractive, then I would do all I could to try to become friends with them in the hope to eventually become their boyfriend. So, my sole reason for being friends with women, really, was to eventually become romantically involved. That was my primary motivation. Looking back on it now, it makes me sick.\nI’ve never had a woman tell me she was assaulted by anyone either, but given my past behavior, even if someone I knew had, I don’t think my actions made me someone that a woman would trust to tell.\nLet me give you some examples of my behavior.\nGrade School\nIn 6th grade, I decided that I wanted a girlfriend, and I picked out one girl in my class who I wanted to be my girlfriend. I am very sure I never asked her out, to be my girlfriend, but I made sure to spend tons of time with her, and if I recall correctly, she eventually got the gist of my interest, and told me very clearly she wasn’t interested. But her telling me no did not stop my unwanted advances or attention. I am sure that I made her very uncomfortable the rest of that grade.\nIn middle school (7-9 at the time), I pretty much continued this process unabated. I would latch onto a woman that I found attractive, and make her the target of my affections, and pour out my unwanted attention upon her, not taking no for an answer. I only stopped after long periods of continued rejection, or when that person acquired a significant other. Although not an excuse for my actions, my tactics and hopes were largely fueled by rampaging hormones, way too many romantic comedies where the nice guy always got the girl by virtue of sheer persistence (this was the 90’s), and nascent exposure to pornography.\nI would find out girls numbers and call them without being asked. I would know where these girls were at all times through the day, even during lunch and between classes. I would find any excuse to be near them. Every sock-hop (weekly lunch time dance on gym floor) I would ask these girls to dance with me. I would give them valentines cards, Christmas cards, etc, in the hopes that they would realize what a great guy I was and go out with me.\nJust so we are clear, none of this got me any dates in grade school.\nUndergraduate\nNow I’ve graduated high-school, I’m heading off to a local university, with lots of girls. I made lots of friends with girls who had boyfriends, in fact I think my circle of friends had way more girls in it than guys. But, I was always finding one girl who I wanted to date, and would make sure to spend extra time around them, helping them whenever possible, etc, and dropping subtle and not so subtle hints that I wanted to be their boyfriend. And there was always the hope that someone would break-up with their current boyfriend and find me, the faithful friend, waiting to comfort them.\nOver the course of this time, I had three women agree to be my date. Two of those did not result in an actual date, because I started acting like a stalker after they said yes, and they wisely stayed away. In the third case, we went out twice, but me calling at random hours, and showing up at her house un-announced because I thought she was really sad freaked her out, and she stopped talking to my creepy, stalkerish, clingy self.\nPost-Graduate\nSomehow, it seems, by the time I got to my PhD, I had mostly given up on finding a girlfriend, settling down and getting married (really, that was my goal). I say mostly. I don’t know if I hadn’t met my now spouse in the first couple of months of my PhD that I would not have continued making unwanted advances on the women in my PhD program. (By the way, I met my spouse outside of work, at a Church actually, and was introduced by a mutual friend. In the 13 years I’ve known her, there are only a handful of days we haven’t talked to each other since we went on our first date).\nThe Real Problem\nAnd this is the real problem. Too many men, my past self included, think women owe them something for being their friend, for being a nice guy. For giving them any kind of attention, or any kind of help. Too many men believe these things, and then use their power and prestige, to demand things of women. Guys, women don’t owe you anything. They definitely don’t owe you sex or reciprocated romantic interest because of something you did for them. They are another person worthy of respect, simply because they are a person.\nIn addition, real life is not a romantic comedy. Non-romantic friendships are a good thing, because we need other peoples perspectives in our lives. So, if a woman tells you no, she doesn’t want to date you, accept it, and move on. Don’t make it awkward, especially if you are in the same work environment. Don’t assume that a woman is romantically interested just because she is friendly. I know, radical thought. Maybe try being friends, colleagues, whatever with no romantic intentions, and no expectations of them either. Don’t be #partoftheproblem.\nSolutions\nTeach your children that they can be friends with people of the opposite sex without being romantically involved, especially as they hit puberty. Teach them that no means no, not no means maybe in 3 weeks, or no means maybe if I try hard enough. And if you see other men engaging in putting unwanted attention on women, call them out on it, whatever form it may take. I wish someone had said something to me.\nCaveats\nI realize that our general culture is really #partoftheproblem, when we have highly sexualized advertising (especially of women to men), and the idea that boys will be boys, tell jokes about sexual assault, and propagate the idea that women want it, based on how they act or dress. Those are all wrong too, and our culture needs to change.\nI also realize that some of what I describe about myself is rather mild in comparison to much of what gets reported, but that’s not the point. It is still unwanted attention, and I didn’t know how to take no for an answer. Those women didn’t want my attention, and I couldn’t accept that. If I had a different temperament, I don’t know what I would have done. Enough people realized it that some friends in Undergrad stopped being around me, but no one ever told me that what I was doing was wrong, and my parents weren’t involved enough in my so-called love life to know what was going on. If they had, I think they would have told me to knock it off and stop being an idiot.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T18:02:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/",
    "title": "Criticizing a Publication, and Lying About It",
    "description": "Critics of our last publication claimed we didn't make our data available, which is an outright lie.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2017-03-29",
    "categories": [
      "publications",
      "peer-review",
      "zinc",
      "academia"
    ],
    "contents": "\n\nContents\nTL;DR\nOriginal Publication\nCritique\nResponse\nFollow-Up Paper on 5 Metals\nCritique Direct Response\nOpen Results and Code\n\nPeer Review\nPapers Discussed\n\n\nTL;DR\nOther researchers directly criticized a recent publication of ours in a “research article”. Although they raised valid points, they outright lied about the availability of our results. In addition, they did not provide access to their own results. We have published new work supporting our original results, and a direct rebuttal of their critique in a perspective article. The peer reviewers of their “research article” must have been asleep at the wheel to allow the major point, lack of access to our results, to stand.\nOriginal Publication\nBack in the summer of 2015, I was second author on a publication (Yao et al., 2015, hereafter YS2015) describing an automated method to characterize zinc ion coordination geometries (CGs). Applying our automated method to all zinc sites in the worldwide Protein Data Bank (wwPDB), we found abberrant zinc CGs that don’t fit the canonical CGs. We were pretty sure that these aberrant CGs are real, and they have always existed, but had not been previously characterized because methods assumed that only the canonical geometries should be observed in biological systems, and were excluding the abberrant ones because they didn’t have good methods to detect and characterize them.\nAlso of note, the proteins with aberrant zinc geometries showed enrichment for different types of enzyme classifications than those with canonical zinc geometries.\nFor this publication, we made all of our code and results available in a tarball that could be downloaded from our website. This data went up while the paper was in review, on Dec 7, 2015 (with a correction on Dec 15). Recently, we’ve also put a copy of the tarball on FigShare. Every draft of the publication, from initial submission through to accepted publication, included the link to the tarball on the website.\nCritique\nLess than a year later, Raczynska, Wlodawer, and Jaskolski (RJW2016) published a critique of YS2015 as a “research article”. In their publication, they questioned the existence of the abberrant sites completely, based on the examination and remodeling of four aberrant structures highlighted in YS2015. To be fair, they did have some valid criticisms of the methods, and Sen Yao did a lot of work in our latest paper to address them.\nAs part of the critique, however, they claimed that they could only evaluate the four structures listed in two figures because we didn’t provide all of our results. However, we had previously made our full results available as a tarball from our website. As you can see in the below figure, all of the results were really available in that tarball.\n<img src = “/img/ys2017_figure1.png”, width = “600”>\nIn addition, although RWJ2016 went to all the trouble to actually remodel those four structures by going back to the original X-ray density, they didn’t make any of their models available.\nFinally, no one from RWJ2016 ever contacted our research group to see if the results might be available.\nResponse\nFollow-Up Paper on 5 Metals\nBy the time the critiques appeared in RJW2016, Sen was already hard at work showing that the previously developed methods could be modified and then applied to other metal ion CGs, and that they also contained aberrant CGs (see YS2017-1).\nCritique Direct Response\nIn addition to YS2017-1, we felt that the critique deserved separate response (YS2017-2). To that end, we began drafting a response, wherein we pointed out some of the problems with RJW2016, the first being that we did indeed provide the full set of results from YS2015, and therefore it was possible to evaluate our full work. We also addressed each of their other criticisms of YS2015, in many cases going beyond the original criticism, and explaining how it was being addressed in YS2017-1.\nOpen Results and Code\nA major part of the conclusions in YS2017-2 was also devoted to the idea that code and results in science need to be shared, highlighting the fact that RJW2016 did not share their models they used to try and discredit our work, lied about the fact that we did not share our own results, and pointing out some other projects in this research area that have shared well and others that have shared badly, and that the previous attitude of competition among research groups does not move science forward.\nPeer Review\nLet’s just say that the peer-review of both of the papers was interesting. Both manuscripts had the same set of reviewers. YS2017-1, the five metal paper, had some rather rigorous peer review, and was definitely improved by the reviewer’s comments. YS2017-2, our perspective, in contrast, was attacked by one peer reviewer right from submission, and was questioned almost continually as to whether it should even be published. I am thankful that one reviewer saw the need for it to be published, and that the Editor ultimately decided that it should be published, and that we were able to rebut each of the reviewer’s criticisms.\nFinally, I really don’t know what happened in the peer review of RWJ2016. The first major claim was that our data wasn’t available, it should have taken a reviewer 10 minutes to verify and debunk that claim. I would have expected a much different critique from the authors had they actually examined our full data set. But, because of traditional closed peer review, that record is closed to us.\nOverall though, I’m very happy both of our publications are now out, and we can move on to new stages of our analyses. Looking forward to continuing to work with my co-authors to move the work forward.\nPapers Discussed\nOriginal Zinc CGs: Yao et al 2015\nCritique of Zinc CGs: Raczynska, Wlodawer & Jaskolski 2016, publisher, sci-hub\n5 Metal CGs: Yao et al 2017\nResponse to critique: Yao et al 2017, publisher, copy on figshare\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:59:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/",
    "title": "Authentication of Key Resources for Data Analysis",
    "description": "NIH is asking for authentication of key resources. How does this apply to data analyses?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2016-03-23",
    "categories": [
      "reproducibility",
      "open-science",
      "analysis"
    ],
    "contents": "\n\nContents\nTL;DR\nNIH and Reproducibility\nData / Code Authentication\nData Authentication?\nCode Authentication\n\nConclusion\n\nTL;DR\nNIH recently introduced a reproducibility initiative, extending to including the “Authentication of Key Resources” page in grant applications from Jan 25, 2016. Seems to be intended for grants involving biological reagents, but we included it in our recent R03 grant developing new data analysis methods. We believe that this type of thing should become common for all grants, not just those that use biological/chemical resources.\nNIH and Reproducibility\nThere has been a lot of things published recently about the reproducibility crisis in science (see refs). The federal funding agencies are starting to respond to this, and beginning with grants submitted after January 25, 2016, grants are supposed to address the reproducibility of the work proposed, including the presence of various confounding factors (i.e. sex of animals, the source of cell lines, etc). In addition to this, there is a new document that can be added to grants, the Authentication Plan, which as far as I can tell is intended specifically for:\n\nkey biological and/or chemical resources used in the proposed studies\n\nNow, this makes sense. Some sources of irreproducibility include, but are not limited to:\nunvalidated antibodies\ncell lines that are not what was thought\nimpure chemicals\nI think this is a good thing. What does it have to do with data analysis?\nData / Code Authentication\nWhen we were submitting a recent R03 proposal for developing novel data analysis methods and statistical tools, the grant management office asked us about the Authentication of Key Resources attachment, which we completely missed. Upon review of the guidelines, we initially determined that this document did not apply. However, we decided to go ahead and take some initiative.\nData Authentication?\nWhen dealing with multiple samples from high-throughput samples, there are frequently a few easy ways to examine the data quality, and although it can be hard to verify that the data is what the supplier says it is, which would be true authentication, there are some ways to verify that the various samples in the dataset are at least self-consistent within each sample class (normal and disease, condition 1 and condition 2).\nMy go-to for data self-consistency are principal components analysis (PCA) and correlation heat-maps. Correlation heat-maps involve calculating all of the pairwise sample to sample correlations using all of the non-zero sample features (those that are non-zero in the two pairs being compared). These heatmaps, combined with the sample class information, and clustering within each class, are a nice visual way to eyeball samples that have potential problems. A simple example for RNA-seq transcriptomics was shown in Gierliński et al., Statistical models for RNA-seq data derived from a two-condition 48-replicate experiment Bioinformatics (2015) 31 (22): 3625-3630, Figure 1.\n\n\nknitr::include_graphics(\"yeast_48rep_cor_heatmap.jpg\")\n\n\n\n\nThe other measures they used in this paper are also very nice, in plotting the median correlation of a sample against all other samples, and the fraction of outlier features in a given sample (see figure 2 of Gierlinkski et al). The final measure they propose is not generally applicable to all -omics data however.\nPCA on the data, followed by visualizing the scores on the first few principal components, and colored by sample class (or experimental condition) is similar in spirit to the correlation heat-map. In fact, it is very similar, because PCA is actually decomposing on the covariance of the samples, which is very related to the correlations (an early algorithm actually used the correlation matrix).\nBoth of these methods can highlight possible problems with individual samples, and make sure that the set of data going into the analysis is at least self-consistent, which is important when doing classification or differential abundance analyses.\nCode Authentication\nThe other thing we highlighted in the document was code authentication. In this case, we highlighted the use of unit-testing in the R packages that we are planning to develop. Even though this is software coming out of a research lab, we need to have confidence that the functions we write return the correct values given various inputs. In addition, code testing coverage helps evaluate that we are testing all of the functionality by checking that all of the lines in our code are run by the tests. Finally, we are also planning to write tests for core functions provided by others (i.e. functions in other R packages), in that they work as we expect, by returning correct values given specific inputs.\nConclusion\nGoing forward, I think it would be a good thing if people writing research grants for data analysis methods would discuss how they are going to look at the data to assess it’s quality, and how they are going to do unit testing, and will have to start saying that they are going to do unit testing of their analysis method.\nI’d be interested in others’ thoughts on this as well.\n\n\n\n",
    "preview": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/yeast_48rep_cor_heatmap.jpg",
    "last_modified": "2021-02-27T17:58:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-12-12-random-forest-vs-pls-on-random-data/",
    "title": "Random Forest vs PLS on Random Data",
    "description": "Comparing random-forest and partial-least-squares discriminant-analysis on random data to show the problems inherent in PLS-DA.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2015-12-12",
    "categories": [
      "random-forest",
      "machine-learning",
      "partial-least-squares",
      "statistics",
      "analysis"
    ],
    "contents": "\n\nContents\nTL;DR\nWhy?\nRandom Data\nFeature Intensities\n\nPCA\nRandom Forest\nPLS-DA\nCross-validated PLS-DA\n\nTL;DR\nPartial least squares (PLS) discriminant-analysis (DA) can ridiculously over fit even on completely random data. The quality of the PLS-DA model can be assessed using cross-validation, but cross-validation is not typically performed in many metabolomics publications. Random forest, in contrast, because of the forest of decision tree learners, and the out-of-bag (OOB) samples used for testing each tree, automatically provides an indication of the quality of the model.\nWhy?\nI’ve recently been working on some machine learning work using random forests (RF) Breimann, 2001 on metabolomics data. This has been relatively successful, with decent sensitivity and specificity, and hopefully I’ll be able to post more soon. However, PLS (Wold, 1975) is a standard technique used in metabolomics due to the prevalence of analytical chemists in metabolomics and a long familiarity with the method. Importantly, my collaborators frequently use PLS-DA to generate plots to show that the various classes of samples are separable.\nHowever, it has long been known that PLS (and all of it’s variants, PLS-DA, OPLS, OPLS-DA, etc) can easily generate models that over fit the data, and that over fitting of the model needs to be assessed if the model is going to be used in subsequent analyses.\nRandom Data\nTo illustrate the behavior of both RF and PLS-DA, we will generate some random data where each of the samples are randomly assigned to one of two classes.\nFeature Intensities\nWe will generate a data set with 1000 features, where each feature’s mean value is from a uniform distribution with a range of 1-10000.\n\n\nlibrary(ggplot2)\ntheme_set(cowplot::theme_cowplot())\nlibrary(fakeDataWithError)\nset.seed(1234)\nn_point <- 1000\nmax_value <- 10000\ninit_values <- runif(n_point, 0, max_value)\n\n\n\n\n\ninit_data <- data.frame(data = init_values)\nggplot(init_data, aes(x = data)) + geom_histogram() + ggtitle(\"Initial Data\")\n\n\n\n\nFor each of these features, their distribution across samples will be based on a random normal distribution where the mean is the initial feature value and a standard deviation of 200. The number of samples is 100.\n\n\nn_sample <- 100\nerror_values <- add_uniform_noise(n_sample, init_values, 200)\n\n\n\nJust for information, the add_uniform_noise function is this:\n\n\nadd_uniform_noise\n\n\nfunction (n_rep, value, sd, use_zero = FALSE) \n{\n    n_value <- length(value)\n    n_sd <- n_rep * n_value\n    out_sd <- rnorm(n_sd, 0, sd)\n    out_sd <- matrix(out_sd, nrow = n_value, ncol = n_rep)\n    if (!use_zero) {\n        tmp_value <- matrix(value, nrow = n_value, ncol = n_rep, \n            byrow = FALSE)\n        out_value <- tmp_value + out_sd\n    }\n    else {\n        out_value <- out_sd\n    }\n    return(out_value)\n}\n<bytecode: 0x55bf33f2aa58>\n<environment: namespace:fakeDataWithError>\n\nI created it as part of a package that is able to add different kinds of noise to data.\nThe distribution of values for a single feature looks like this:\n\n\nerror_data <- data.frame(feature_1 = error_values[1,])\nggplot(error_data, aes(x = feature_1)) + geom_histogram() + ggtitle(\"Error Data\")\n\n\n\n\nAnd we will assign the first 50 samples to class_1 and the second 50 samples to class_2.\n\n\nsample_class <- rep(c(\"class_1\", \"class_2\"), each = 50)\nsample_class\n\n\n  [1] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n  [7] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [13] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [19] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [25] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [31] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [37] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [43] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [49] \"class_1\" \"class_1\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [55] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [61] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [67] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [73] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [79] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [85] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [91] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [97] \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n\nPCA\nJust to show that the data is pretty random, lets use principal components analysis (PCA) to do a decomposition, and plot the first two components:\n\n\ntmp_pca <- prcomp(t(error_values), center = TRUE, scale. = TRUE)\npca_data <- as.data.frame(tmp_pca$x[, 1:2])\npca_data$class <- as.factor(sample_class)\nggplot(pca_data, aes(x = PC1, y = PC2, color = class)) + geom_point(size = 4)\n\n\n\n\nRandom Forest\nLet’s use RF first, and see how things look.\n\n\nlibrary(randomForest)\nrf_model <- randomForest(t(error_values), y = as.factor(sample_class))\n\n\n\nThe confusion matrix comparing actual vs predicted classes based on the out of bag (OOB) samples:\n\n\nknitr::kable(rf_model$confusion)\n\n\n\nclass_1\nclass_2\nclass.error\nclass_1\n21\n29\n0.58\nclass_2\n28\n22\n0.56\n\nAnd an overall error of 0.5760364.\nPLS-DA\nSo PLS-DA is really just PLS with y variable that is binary.\n\n\nlibrary(caret)\npls_model <- plsda(t(error_values), as.factor(sample_class), ncomp = 2)\npls_scores <- data.frame(comp1 = pls_model$scores[,1], comp2 = pls_model$scores[,2], class = sample_class)\n\n\n\nAnd plot the PLS scores:\n\n\nggplot(pls_scores, aes(x = comp1, y = comp2, color = class)) + geom_point(size = 4) + ggtitle(\"PLS-DA of Random Data\")\n\n\n\n\nAnd voila! Perfectly separated data! If I didn’t tell you that it was random, would you suspect it?\nCross-validated PLS-DA\nOf course, one way to truly assess the worth of the model would be to use cross-validation, where a fraction of data is held back, and the model trained on the rest. Predictions are then made on the held back fraction, and because we know the truth, we will then calculate the area under the reciever operator curve (AUROC) or area under the curve (AUC) created by plotting true positives vs false positives.\nTo do this we will need two functions:\nGenerates all of the CV folds\nGenerates PLS-DA model, does prediction on hold out, calculates AUC\n\n\nlibrary(cvTools)\nlibrary(ROCR)\n\ngen_cv <- function(xdata, ydata, nrep, kfold){\n  n_sample <- length(ydata)\n  all_index <- seq(1, n_sample)\n  cv_data <- cvFolds(n_sample, K = kfold, R = nrep, type = \"random\")\n  \n  rep_values <- vapply(seq(1, nrep), function(in_rep){\n    use_rep <- cv_data$subsets[, in_rep]\n    cv_values <- vapply(seq(1, kfold), function(in_fold){\n      test_index <- use_rep[cv_data$which == in_fold]\n      train_index <- all_index[-test_index]\n      \n      plsda_cv(xdata[train_index, ], ydata[train_index], xdata[test_index, ],\n               ydata[test_index])\n    }, numeric(1))\n  }, numeric(kfold))\n}\n\nplsda_cv <- function(xtrain, ytrain, xtest, ytest){\n  pls_model <- plsda(xtrain, ytrain, ncomp = 2)\n  pls_pred <- predict(pls_model, xtest, type = \"prob\")\n  \n  use_pred <- pls_pred[, 2, 1]\n  \n  pred_perf <- ROCR::prediction(use_pred, ytest)\n  pred_auc <- ROCR::performance(pred_perf, \"auc\")@y.values[[1]]\n  return(pred_auc)\n}\n\n\n\nAnd now lets do a bunch of replicates (100).\n\n\ncv_vals <- gen_cv(t(error_values), factor(sample_class), nrep = 100, kfold = 5)\n\nmean(cv_vals)\n\n\n[1] 0.4260387\n\nsd(cv_vals)\n\n\n[1] 0.1188491\n\ncv_frame <- data.frame(auc = as.vector(cv_vals))\nggplot(cv_frame, aes(x = auc)) + geom_histogram(binwidth = 0.01)\n\n\n\n\nSo we get an average AUC of 0.4260387, which is pretty awful. This implies that even though there was good separation on the scores, maybe the model is not actually that good, and we should be cautious of any predictions being made.\nOf course, the PCA at the beginning of the analysis shows that there is no real separation in the data in the first place.\n\n\ndevtools::session_info()\n\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 4.0.0 (2020-04-24)\n os       Pop!_OS 20.04 LTS           \n system   x86_64, linux-gnu           \n ui       X11                         \n language en_US:en                    \n collate  en_US.UTF-8                 \n ctype    en_US.UTF-8                 \n tz       America/New_York            \n date     2021-02-27                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package           * version    date       lib source        \n assertthat          0.2.1      2019-03-21 [1] CRAN (R 4.0.0)\n cachem              1.0.4      2021-02-13 [1] CRAN (R 4.0.0)\n callr               3.5.1      2020-10-13 [1] CRAN (R 4.0.0)\n caret             * 6.0-86     2020-03-20 [1] CRAN (R 4.0.0)\n class               7.3-18     2021-01-24 [1] CRAN (R 4.0.0)\n cli                 2.3.0      2021-01-31 [1] CRAN (R 4.0.0)\n codetools           0.2-18     2020-11-04 [1] CRAN (R 4.0.0)\n colorspace          2.0-0      2020-11-11 [1] CRAN (R 4.0.0)\n cowplot             1.1.1      2020-12-30 [1] CRAN (R 4.0.0)\n crayon              1.4.1      2021-02-08 [1] CRAN (R 4.0.0)\n cvTools           * 0.3.2      2012-05-14 [1] CRAN (R 4.0.0)\n data.table          1.13.6     2020-12-30 [1] CRAN (R 4.0.0)\n DBI                 1.1.1      2021-01-15 [1] CRAN (R 4.0.0)\n DEoptimR            1.0-8      2016-11-19 [1] CRAN (R 4.0.0)\n desc                1.2.0      2018-05-01 [1] CRAN (R 4.0.0)\n devtools            2.3.2      2020-09-18 [1] CRAN (R 4.0.0)\n digest              0.6.27     2020-10-24 [1] CRAN (R 4.0.0)\n distill             1.2        2021-01-13 [1] CRAN (R 4.0.0)\n downlit             0.2.1      2020-11-04 [1] CRAN (R 4.0.0)\n dplyr               1.0.4      2021-02-02 [1] CRAN (R 4.0.0)\n ellipsis            0.3.1      2020-05-15 [1] CRAN (R 4.0.0)\n evaluate            0.14       2019-05-28 [1] CRAN (R 4.0.0)\n fakeDataWithError * 0.0.1      2020-05-27 [1] local         \n fansi               0.4.2      2021-01-15 [1] CRAN (R 4.0.0)\n farver              2.0.3      2020-01-16 [1] CRAN (R 4.0.0)\n fastmap             1.1.0      2021-01-25 [1] CRAN (R 4.0.0)\n foreach             1.5.1      2020-10-15 [1] CRAN (R 4.0.0)\n fs                  1.5.0      2020-07-31 [1] CRAN (R 4.0.0)\n generics            0.1.0      2020-10-31 [1] CRAN (R 4.0.0)\n ggplot2           * 3.3.3      2020-12-30 [1] CRAN (R 4.0.0)\n glue                1.4.2      2020-08-27 [1] CRAN (R 4.0.0)\n gower               0.2.2      2020-06-23 [1] CRAN (R 4.0.0)\n gtable              0.3.0      2019-03-25 [1] CRAN (R 4.0.0)\n highr               0.8        2019-03-20 [1] CRAN (R 4.0.0)\n htmltools           0.5.1.1    2021-01-22 [1] CRAN (R 4.0.0)\n ipred               0.9-9      2019-04-28 [1] CRAN (R 4.0.0)\n iterators           1.0.13     2020-10-15 [1] CRAN (R 4.0.0)\n knitr               1.31       2021-01-27 [1] CRAN (R 4.0.0)\n labeling            0.4.2      2020-10-20 [1] CRAN (R 4.0.0)\n lattice           * 0.20-41    2020-04-02 [1] CRAN (R 4.0.0)\n lava                1.6.8.1    2020-11-04 [1] CRAN (R 4.0.0)\n lifecycle           1.0.0      2021-02-15 [1] CRAN (R 4.0.0)\n lubridate           1.7.9.2    2020-11-13 [1] CRAN (R 4.0.0)\n magrittr            2.0.1      2020-11-17 [1] CRAN (R 4.0.0)\n MASS                7.3-53.1   2021-02-12 [1] CRAN (R 4.0.0)\n Matrix              1.3-2      2021-01-06 [1] CRAN (R 4.0.0)\n memoise             2.0.0      2021-01-26 [1] CRAN (R 4.0.0)\n ModelMetrics        1.2.2.2    2020-03-17 [1] CRAN (R 4.0.0)\n munsell             0.5.0      2018-06-12 [1] CRAN (R 4.0.0)\n nlme                3.1-152    2021-02-04 [1] CRAN (R 4.0.0)\n nnet                7.3-15     2021-01-24 [1] CRAN (R 4.0.0)\n pillar              1.4.7      2020-11-20 [1] CRAN (R 4.0.0)\n pkgbuild            1.2.0      2020-12-15 [1] CRAN (R 4.0.0)\n pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.0.0)\n pkgload             1.1.0      2020-05-29 [1] CRAN (R 4.0.0)\n pls                 2.7-3      2020-08-07 [1] CRAN (R 4.0.0)\n plyr                1.8.6      2020-03-03 [1] CRAN (R 4.0.0)\n prettyunits         1.1.1      2020-01-24 [1] CRAN (R 4.0.0)\n pROC                1.17.0.1   2021-01-13 [1] CRAN (R 4.0.0)\n processx            3.4.5      2020-11-30 [1] CRAN (R 4.0.0)\n prodlim             2019.11.13 2019-11-17 [1] CRAN (R 4.0.0)\n ps                  1.5.0      2020-12-05 [1] CRAN (R 4.0.0)\n purrr               0.3.4      2020-04-17 [1] CRAN (R 4.0.0)\n R6                  2.5.0      2020-10-28 [1] CRAN (R 4.0.0)\n randomForest      * 4.6-14     2018-03-25 [1] CRAN (R 4.0.0)\n Rcpp                1.0.6      2021-01-15 [1] CRAN (R 4.0.0)\n recipes             0.1.15     2020-11-11 [1] CRAN (R 4.0.0)\n remotes             2.2.0      2020-07-21 [1] CRAN (R 4.0.0)\n reshape2            1.4.4      2020-04-09 [1] CRAN (R 4.0.0)\n rlang               0.4.10     2020-12-30 [1] CRAN (R 4.0.0)\n rmarkdown           2.6        2020-12-14 [1] CRAN (R 4.0.0)\n robustbase        * 0.93-7     2021-01-04 [1] CRAN (R 4.0.0)\n ROCR              * 1.0-11     2020-05-02 [1] CRAN (R 4.0.0)\n rpart               4.1-15     2019-04-12 [1] CRAN (R 4.0.0)\n rprojroot           2.0.2      2020-11-15 [1] CRAN (R 4.0.0)\n scales              1.1.1      2020-05-11 [1] CRAN (R 4.0.0)\n sessioninfo         1.1.1      2018-11-05 [1] CRAN (R 4.0.0)\n stringi             1.5.3      2020-09-09 [1] CRAN (R 4.0.0)\n stringr             1.4.0      2019-02-10 [1] CRAN (R 4.0.0)\n survival            3.2-7      2020-09-28 [1] CRAN (R 4.0.0)\n testthat            3.0.2      2021-02-14 [1] CRAN (R 4.0.0)\n tibble              3.0.6      2021-01-29 [1] CRAN (R 4.0.0)\n tidyselect          1.1.0      2020-05-11 [1] CRAN (R 4.0.0)\n timeDate            3043.102   2018-02-21 [1] CRAN (R 4.0.0)\n usethis             2.0.1      2021-02-10 [1] CRAN (R 4.0.0)\n vctrs               0.3.6      2020-12-17 [1] CRAN (R 4.0.0)\n withr               2.4.1      2021-01-26 [1] CRAN (R 4.0.0)\n xfun                0.21       2021-02-10 [1] CRAN (R 4.0.0)\n yaml                2.2.1      2020-02-01 [1] CRAN (R 4.0.0)\n\n[1] /software/R_libs/R400\n[2] /software/R-4.0.0/lib/R/library\n\n\n\n\n",
    "preview": "posts/2015-12-12-random-forest-vs-pls-on-random-data/random-forest-vs-pls-on-random-data_files/figure-html5/plot_initial-1.png",
    "last_modified": "2021-02-27T17:57:04-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2015-06-17-novel-zinc-coordination-geometries/",
    "title": "Novel Zinc Coordination Geometries",
    "description": "A bit of an explainer on our labs recent publication on finding and classifying zinc coordination geometries in protein structures.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2015-06-17",
    "categories": [
      "zinc",
      "structural-biochemistry",
      "open-science",
      "reproducibility",
      "visualization",
      "publications"
    ],
    "contents": "\n\nContents\nTL;DR\nZinc Coordination\nPrevious Attempts\nOur Initial Attempt\nSeparate Out Compressed and Cluster!\nClustering\nNovel CGs\nFunctional Characterization\nImplications\nReproducibility\n\nTL;DR\nCurrently available methods to discover metal geometries make too many assumptions. We were able to discover novel zinc coordination geometries using a less-biased method that makes fewer assumptions. These novel geometries seem to also have specific functionality. This work was recently published under an #openaccess license in Proteins Journal (Yao et al. 2015).\nZinc Coordination\nAs I’m sure many people know, zinc is a very important metal for biology. It is one of the most numerous metal ions, and plays a role in many different types of proteins. Zinc ions in protein structures can have anywhere from 4 to 6 ligands, in many different coordination geometries.\n\n\n\nFigure 1: Fig 1 from (Yao et al. 2015)\n\n\n\nHow the zinc ion is coordinated is going to depend on the protein sequence of amino acids that surround it, and knowledge of the protein sequence should enable knowledge of how the zinc ions are coordinated. Therefore, being able to characterize zinc ion coordination geometries (CGs) from structural data (such as protein structures in the world-wide protein data bank) and associate them with protein sequences is very important.\nPrevious Attempts\nOther groups had done this previously, and we now have hidden-markov models for determining zinc binding thanks to this work. However, in all the cases that we could find, the determination of CG was made by comparison to previously known geometries that have been described from zinc-compound crystal structures.\nWhen the biologically based CGs are compared to previously known, there will be a bunch of CGs that are outliers or will remain unclassified.\nOur Initial Attempt\nWhen Sen (the lead author, currently a third year PhD student in the University of Louisville Bioinformatics program) first tried to use bootstrapping and a expectation-maximization algorithm to automatically classify the zinc CGs, she started getting rather funny results, as in, why is the algorithm not converging and we are getting these huge variances in the various measures that characterize the CGs.\nA single visualization was key in unlocking what was going on. Figure 2 in the paper is a histogram of the minimum angle (where angle is ligand-zinc-ligand) in degrees.\n\n\n\nFigure 2: Fig 2 from (Yao et al. 2015)\n\n\n\nBased on Figure 1, we would expect the minimum angle to be 90, but as you can see in Figure 2, there are an awful lot of angles less than 90. We are very sure that they are real given the definition of zinc-ligand bond-length that was used to define potential ligands, and the statistical decision used to define how many ligands a given zinc ion has.\nSo we have a bunch of zinc-ions that make ligand-zinc-ligand bond angles at 60, and even 30 degrees! It turns out that these small angles are largely (but not all) due to bidentate ligands, where for example the zinc ion forms bonds with two oxygen’s from an aspartate or glutamate amino acid.\nSeparate Out Compressed and Cluster!\nTo make the problem tractable, we first have to separate out the compressed angle zinc sites, and then we can do clustering on the set of ligand-zinc-ligand angles to determine in a mostly un-biased fashion what CGs are present and which zinc-site belongs to which CG.\nClustering\nThe use of k-means clustering on the angles also required developing a way to order the ligand-zinc-ligand angles in such a way that they are comparable across all of the different CGs. The final method used in the paper was to order them using largest-sortedmiddle-opposite, which is the largest angle, the middle angles sorted in order, and lastly the opposite of the largest angle. This keeps them from being scrambled from site to site, and allows them to be comparable across zinc sites.\nAlso, because the number of true clusters was unknown as we are trying to discover in an unbiased way all CGs, the number of clusters was varied, and for each k-clusters, replicate clusterings done, and the stability of the clusters was assessed by cluster membership and locations across replicates.\nNovel CGs\nBased on all this work, we were able to compare generated clusters from both the normal and compressed groups with canonical CGs to determine if a CG from clustering corresponds to a known CG or a novel CG. The normal clusters mostly corresponded to known CGs or unsurprising variants thereof. What was really interesting is that there were multiple clusters corresponding to a tetrahedral CG. To determine if the compressed CGs were merely a compressed variant of the canonical, the compressed angle was removed from the comparison to generate a probability of assignment. At least one of the compressed groups appear to be novel, in that it has not been described before, either structurally or functionally.\nFunctional Characterization\nWe also wanted to determine if there was a functional component to the CGs, i.e. different CGs have different functionality (this is my primary but not only contribution to this manuscript). To do this we annotated all of the PDB sequences using InterProScan, and kept annotations that intersected the range of amino-acids that potentially interact with the zinc ion. This bit is tricky, because many different CGs can have many different functionality, merely doing hypergeometric-enrichment of annotations in each cluster doesn’t lead to a convincing picture; as we quickly found out (it was the first thing I tried). However, if we generate a measure of functional similarity between clusters (based on a faux covariance, really, read the paper, it is rather neat) and compare this functional cluster similarity measure with a cluster distance based on the angles, there is an extremely high Spearman correlation of 0.88 for the normal and 0.66 for the compressed CGs, implying that CG and function are intimately related.\n\n\n\nFigure 3: Fig 9 from Yao et al., 2015\n\n\n\nFinally, we considered all normal and compressed clusters as separate groups and performed hypergeometric-enrichment on both the InterProScan and EC number annotations, finding enriched annotations specific to each group of clusters.\n\n\n\nFigure 4: Fig 10 from Yao et al, 2015\n\n\n\nImplications\nMaking too many assumptions about biological structure can be a bad thing. Given that, however, if you are using canonical structures for assignment and you get a ton of outliers, maybe you need to re-examine your data and methods.\nMachine learning methods such as random-forest, k-means clustering, and statistical classification can be readily used to discover and assign metal-ion CG. If you are careful, and separate out the compressed from normal first.\nThere is a rather tight relationship between a zinc-ion’s CG and the functionality of the protein it is embedded within.\nReproducibility\nAlthough we have admittedly dropped the ball on this, there will be a tarball of all of the scripts used to generate the results including a README explaining how to run them available soon.\nEdit: We have a tarball of the code and data used on our website and from figshare (Yao et al. 2016)\n\n\n\nYao, Sen, Robert M. Flight, Eric C. Rouchka, and Hunter N. B. Moseley. 2015. “A Less-Biased Analysis of Metalloproteins Reveals Novel Zinc Coordination Geometries.” Proteins: Structure, Function, and Bioinformatics 83 (8): 1470–87. https://doi.org/https://doi.org/10.1002/prot.24834.\n\n\nYao, Sen, Robert M Flight, Eric Rouchka, and Hunter Moseley. 2016. “Zn metalloprotein paper,” November. https://doi.org/10.6084/m9.figshare.4229333.v1.\n\n\n\n\n",
    "preview": "posts/2015-06-17-novel-zinc-coordination-geometries/prot24834-fig-0002.png",
    "last_modified": "2021-02-27T17:56:08-05:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 256
  },
  {
    "path": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/",
    "title": "Mouse / Human Transcriptomics and Batch Effects",
    "description": "A recent paper dug into some data from another paper, casting doubts on the first, all thanks to the data being available.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2015-06-01",
    "categories": [
      "open-science",
      "transcriptomics",
      "batch-effects",
      "publications",
      "peer-review"
    ],
    "contents": "\n\nContents\nTL;DR\nMouse / Human Transcriptomic Differences\nIs Everything As it Seems?\nBatch Effects\nIs the Data Still Useful?\n Addendum\n\nTL;DR\nThis 2014 PNAS paper by S. Lin et al (Lin et al., PNAS, 2014) that compares transcription of tissues between species has a flawed experimental design, where species is almost perfectly confounded with machine / lane on which the sequencing was done. Y. Golad and O. Mizrahi-Man have published a manuscript describing the confounding and the results of removing it. This was possible because the original authors supplied the information about which publically available files were used in the original analysis. The data from this experiment is probably only suitable as an example of what not to do in high-throughput biology experimental design, and that there may be similarities in human and mouse transcriptional programs.\nWe discussed these papers in the Systems Biology and Omics Integration University of Kentucky Journal Club on June 1, 2015. I lead that discussion.\nMouse / Human Transcriptomic Differences\nIn 2014, two papers were published by members of the ENCODE project purporting that tissue gene expression clustered more by species than by tissue. In the first (ENCODE Consortium, Nature, 2014, doi:10.1038/nature13992 2014), a large number of experiments were combined and compared. Figure 2a shows a PCA plot of expression in across 10 tissues in human and mouse.\n\n\n\nInterestingly enough, if you collapse PC1 (definitely species), then the tissues start to look quite similar. Which does not seem that unexpected, there are species specific differences, but the tissues are doing something similar in each species.\nThis was not the end of the story, however. A subsequent publication (S Lin et al., PNAS, 2014, doi: 10.1073/pnas.1413624111) went further in doing fresh sequencing of 13 tissues in both species, still showing bigger differences between species than between tissues.\nIs Everything As it Seems?\nOn April 28, Y. Gilad sent out this tweet:\n\nWe reanalyzed the data from http://t.co/Fv7z9WwLJ4 and found the following: pic.twitter.com/37eVs8Kln9\n\nThe left figure shows the original data, clustering by species, and on the right, reprocessed data, clustering by tissue. Now, I have to admit when he posted this I was kind of ticked off. Where was the blog-post or manuscript showing what exactly was done? If you look at the comments to Yoav on that tweet, it seems others were wondering the same thing. Thankfully, on May 19, the manuscript hit F1000Research (Gilad Y and Mizrahi-Man O. A reanalysis of mouse ENCODE comparative gene expression data [v1; ref status: approved with reservations 1, http://f1000r.es/5ez] F1000Research 2015, 4:121 (doi: 10.12688/f1000research.6536.1)).\nBatch Effects\nYoav asked for the list of data files that were used in the PNAS paper, and then examined the read id line to extract the experimental design. This experimental design is captured in Figure 1:\n\n\n\nDo you notice a problem with this design?\n. . . . .\nHopefully you noticed that species (one of the main effects to investigate) is not randomly or even semi-randomly distributed across sequencers and / or lanes, but is almost perfectly confounded with sequencer / lane. It doesn’t matter if one technician handled all the samples, this is potentially a large batch effect / confounding variable.\nAnd Yoav shows that ignoring the batch effect produces data much like that reported in the Lin et al PNAS pub, while removing the batch effect using ComBat results in the tissues clustering together.\nIs the Data Still Useful?\nI presented these papers and the discussion around them at our weekly Systems Biology and Omics Integration (SBOI) Journal Club on June 1, 2015. There were a couple of concerns with the overall study:\nIs ENCODE data still useful?\nsome people seemed to be concerned that this brought the general ENCODE project into question. I think by the end we agreed that in general it is probably ok to be using ENCODE data, but this particular study was questionable.\n\nWhat does the data tell us?\nbased on the correction that Yoav did and reanalysis, is there anything actually telling in the data? Unfortunately, because species is so confounded with machine, I don’t think there is much of a conclusion to draw about species differences vs tissue similarity based on this data. There was some disagreement on this point.\n\nFull experimental designs should be published, and requested by reviewers\nReally, why a reviewer on the paper did not request more information about the experimental design is beyond me, especially given the claims of the manuscript. Why it is not the norm to provide this kind of experimental design information in a manuscript is also a good question.\n\nFinally, I think the best use of the 2014 PNAS pub and this dataset is an example of how not to design a biological experiment.\n Addendum\nAs I looked at the comment section of the Gilad & Man article yesterday (June 1, 2015), I noticed that there were direct replies from S. Lin in a couple of places. In particular is a comment that Lin et al did a second set of sequencing with a new design, and reanalysed the data. Links are provided to two figures, a table of the new design and a new 3D PCA plot:\n\n\n\n\n\n\nThe new sequencing design seems much more reasonable, and the PCA plot has many characteristics of the original one from the original comparative analysis by Mouse ENCODE (see above), in that yes, there are species specific differences, but there also appears to be a way to collapse along PC2 and PC3 where the tissues will line up with each other, which I kind of would expect.\n\n\n\n",
    "preview": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/slin_table1part2.jpg",
    "last_modified": "2021-02-27T17:55:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/",
    "title": "First Open Post-Publication Peer Review, with Credit!",
    "description": "A story about my first open peer-review.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2015-03-25",
    "categories": [
      "open-science",
      "peer-review",
      "publications"
    ],
    "contents": "\n\nContents\nTL;DR\nF1000Research!\nHow Did I End Up Reviewing This??\nThe Open Part\nPostPublication\nDisclaimers\n\nTL;DR\nReviewed Jason McDermott’s MDRPred paper on F1000Research!, where my review is posted along side the paper, with a DOI, completely in the open with my name attached. Was a pleasant experience, aided by the fact that Jason wrote a good paper.\nF1000Research!\nF1000Research! is a new publishing startup from F1000 that has a model of post-publication peer review, whereby upon submission the manuscript undergoes basic quality checks (no real editorial control), and then is published. Once the article is published, reviewers are invited to review, and they have 10 days to submit their review. Reviews are signed, and given a DOI. Reviewers are asked to assign one of Approved, Approved with Reservations, Not Approved status to the article.\nWhen the article gets two revieweres giving Approved status (likely after at least one round of review and resubmission), then it will be submitted to PubMed for indexing.\nHow Did I End Up Reviewing This??\nLong story short, I’ve been following Jason on twitter for a while, and I happened to see him tweet and blog about trying to get an F1000Research article up as a citeable supporting publication for a grant going in. I thought it was a nifty idea, and although I actually at the time did not have much of an idea of what Jason’s research actually involved, communicated with him that when the article went live I would be willing to be recommended as a reviewer. Imagine my surprise that I actually had enough domain knowledge that I could actually review the article.\nThat is saying something, given that the paper has a neat combination of genetic algorithms, regular expressions, and protein function prediction (really, you should go give it a read). By the way, this is also the first paper I’ve reviewed in a long time that did not have serious methodological problems, or where claims are made with no substantiation, and I could not identify any serious statistical issues.\nThe Open Part\nAlthough I don’t do a lot of reviewing, I had to admit that this was one of the best reviewing assignments I’ve had in a while. Although I believe that peer review is essential to science, and it needs to be more open (I’ve started signing my reviews for other journals), this was the first time I knew my review was open (my name would be known to the authors’ by default) and public. I have to say that this likely improved the care and thoroughness in doing the actual review as I went through the article, given that both the authors and anyone else who comes across the article and my review can see if the issues I raise are real, or if I’m trying to make myself look good. And my name will be publicly associated with it!\nNote that this openness did not keep me from criticizing particular aspects of the paper. There are lots of things that need to be changed in the article before I will give it an Approved status, and I laid those out in my review.\nPostPublication\nI really appreciated that I get to review a published article, because it means that the whole thing is typeset, figures and tables are in their logical place (not at the end of the document!!!), the line spacing is readable, etc. Note to other publishers, at least let authors put the figures in-line for review if you are not going to type-set the article before it goes to reviewers.\nDisclaimers\nI was given an F1000Research! t-shirt as a reward for commenting on a blog-post regarding incentives for peer-review. I like the shirt. Having done my review, I am also eligible to get a 50% discount on the article processing charges if I submit an article to F1000Research in the next 12 months.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:55:01-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "description": "What it's like having migraines as a PhD student and PostDoc.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-12-31",
    "categories": [
      "phdisabled",
      "migraines",
      "academia"
    ],
    "contents": "\n\nContents\nTL;DR\nMigraine\nMy Migraines\nPhD\nSeminars and Conferences\nPostDoc\nChallenges\nProphylactic\n\nA blog post on the Weecology group blog by Elita Baldridge on being a PhD student with fibromyalgia, and how they are working through that, caused me to pause and reflect on my experience as a PhD student and PostDoc with migraines. For those who haven’t read my blog, I do research in bioinformatics, specifically in transcriptomics and metabolomics. I spend almost all of my research hours in front of a computer writing code, generating plots, and trying to make sense of -omics level data.\nTL;DR\nBeing in academia with migraines is a challenge, but probably less challenging than some other fields or disabilities. Moving cities and discovering Excedrin has made my migraines more bearable.\nMigraine\nFor those who don’t know, migraines are a rather unusual neurological event that result in a bunch of symptoms, the most well known generally being aura (frequently visual, although auditory and olfactory are also known) followed by what is described as intense, debilitating pain, which may or may not be accompanied by extreme nausea and / or vomiting and tiredness.\nMy Migraines\nI have suffered from migraines since at least my early teens (possibly younger), but mine seem to be rather mild in comparison to what I have read about others experiences. In contrast to many other migraineurs, I do not experience visual aura, and in general my pain levels tend to be on the milder side. However, I do experience mood swings during my prodrome (period prior to pain), going from a generally nice guy to someone who considers killing you just for looking at me wrong; as well as confusion, and I also experience expressive aphasia, wherein I can no longer remember proper nouns, but can describe properties of the object in question (for example, instead of a pencil, I might say “the writing implement with a lead center”). In addition to the pain of the migraine, which is often a pounding localized to one location of my skull, I often also experience extreme sensitivity to touch, to the point of not being able to sleep on a few occasions; as well as nausea, however never to the point of actually throwing up. Less frequently I will experience sensitivity to light or sound.\nAs far as I can tell, my primary migraine trigger is changes in the weather, particularly large or frequent changes in barometric pressure. If the weather is stable (either nice or bad), the frequency and severity of my migraines are greatly reduced.\nPhD\nDuring my undergraduate and masters degrees, I actually didn’t notice that much of an effect from my migraines. At some point, after a lot of investigation of my headaches, and alternative therapies, I did finally get a diagnosis of migraine, and a prescription for Maxalt, a triptan used to treat migraines. I used this very sparingly, due to the side effects, and the cost.\nMy PhD was at Dalhousie University in Halifax, NS, Canada. If you don’t know, Halifax is a harbor city on the east coast of Nova Scotia, and as a result of its location has extremely variable weather, as it gets the west -> east weather from the rest of Canada, as well as weather systems going south -> north up the eastern seaboard of the United States. This results in a large amount of barometric pressure changes, and resulted in a lot of migraines for me. In general, my coping strategy when possible was to go home, take 2 tylenol, and go to bed for the rest of the day and sleep it off. As a PhD student in an analytical chemistry lab doing no wet lab work but only programming, this was often a viable option. My PhD supervisor was very supportive, in that as long as grades were good and project progress was being made, he was not particular about hours spent in the lab.\nOf course, as a PhD student, there were often times this was not possible. I took regular upper level classes in my first 2 years, and I never informed any of my instructors about my migraines, but often just suffered through classes when necessary. In addition, I TA’d first and second year chemistry lab. Given the other symptoms of my migraines, I actually feel sorry for the students I instructed. I know many times I had trouble resolving what were probably trivial mistakes by first year students, and losing my cool over something that was not that difficult to fix. Second year was a bit better in that the students had more experience in a lab setting, however I TA’d analytical chemistry, which essentially involved generating a series of standards, measuring the standards, and then measuring a sample with unknown concentration. There was generally fewer ways for any given lab experiment to fail in the 2nd year lab, putting a lighter cognitive load on me most days.\nSeminars and Conferences\nOf course, as a student I had to attend weekly departmental seminars, and present a couple of times as well. I also traveled to a few conferences and presented talks and posters. To my recollection, I have not suffered a severe migraine during my own talks at a conference. However, I have had different occasions where I have had to skip out on conference sessions due to a migraine, and have been even less likely to socialize and network than I usually would because of migraines. This is hard, because one of the primary purposes of attending conferences is exposure to new research and people.\nPostDoc\nMy first PostDoc was at the University of Louisville in Louisville, KY, USA. My move to the US coincided with two things: 1) The introduction of Excedrin Migraine, and 2) My wife giving birth to our son. 1 is important because Excedrin has allowed me to manage my migraines. The side effects from Maxalt were almost as bad as the migraine itself, and it was bloody expensive (even with my wifes great drug plan). As a PostDoc supporting my wife and son, I always stopped to think about whether I actually needed it. Excedrin Migraine worked quite well in alleviating the pain from my migraines without introducing new side effects. I should note at this time that I was not a regular tea or coffee drinker, so the effect is not likely due to a caffeine addiction.\nThen I discovered that the regular Excedrin worked about the same, and was available in a generic form at many big box stores at an extremely low price ($4 for 100 tablets). I haven’t taken any Maxalt in over 3 years, and probably buy the generic Excedrin 100 count for $4 at WalMart every few months. Is this good for my stomach?? Maybe not. Do I need to watch myself if I get a cut within a short amount of time after taking Excedrin (ASA inhibits clotting)?? Yep. But it seems to mess with my head a lot less than the triptans I have taken (I have also tried Zomig in addition to the Maxalt).\n2 is important because the arrival of our son resulted in a lot of increased stress (and don’t get me wrong, a lot of happiness and joy as well). Although not a primary trigger for my migraines, it definitely did not help. And having a migraine with a crying baby in the background is a special kind of torture (I was reminded of this again the other day with my 8 month old daughter).\nHowever, the move to the midwest US has resulted in a general decrease in the severity of my migraines, but I’m not sure about the frequency. Since moving here, I’ve only had to leave work because of a migraine once or twice a year, instead of the once a month that I used to in Nova Scotia.\nObviously, my productivity at work during a migraine is reduced compared to not being in a migraine. However, there are often periods in academia when things just need to get done (collaborator has a paper or grant deadline, etc), and going home for me means a combined hour of walking and public transit, and a very small likelyhood of getting anything done even after the migraine has passed (having a 4 year old and 8 month old contributes to this a lot, but I wouldn’t have it any other way).\nChallenges\nAs I previously said, my migraines appear to be rather mild based on the descriptions of other peoples migraines I have encountered (see the migraine subreddit for some painful descriptions from other migaineurs). However, they can make some interactions difficult. My normally cheerful and upbeat demeanor can quickly turn sour, and I have been acutely aware of some rather awkward social interactions during a migraine. Thankfully, many times I am able to simply avoid others at work when I have a migraine, and I generally take pains to think twice before speaking as well (which is a good thing to do in general). I don’t teach, and the number of meetings that I have to attend in a given week is rather small, thankfully (not just because of my migraines).\nThe pain and mental confusion from my migraines can make getting work done difficult, and although Excedrin seems to do a wonderful job of dulling the pain, nothing can help with the mental confusion. Having a migraine in the middle of working on a large data analysis with many possible directions, or managing many different projects takes an incredible amount of mental effort when I am in a migraine. I have had times when I sit in front of my computer completely stunned because I cannot figure out what needs to be done next. At these points I frequently occupy myself with mindless tasks that need to be done, sending meeting emails, downloading papers to read, and trying to stay off of social media time sucks like Facebook and Reddit, as I tend to get heavily engrossed in those more so than usual.\nProphylactic\nThis past year I finally tried CBD (cannibidiol) oil, an oil where only CBD has been extracted from the marijuana plant. Taking this twice a day, every day, has greatly reduced the number of migraines I have, and seems to reduce their intensity as well. It’s greatest effect is on the pain levels I encounter, and not so much the other neurological effects of the migraines.\nUpdated on 2017-12-28 with information on CBD oil.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:52:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-11-05-travis-ci-to-github-pages/",
    "title": "Travis-CI to GitHub Pages",
    "description": "How I automatically have some stuff get pushed to GitHub pages from a Travis CI job.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-11-05",
    "categories": [
      "R",
      "reproducibility",
      "travis-ci",
      "github",
      "publishing"
    ],
    "contents": "\n\nContents\nOAUTH\nDeploy Script\nModifying .travis.yml\nUpdate\n\nI don’t remember how I got on this, but I believe I had a recent twitter exchange with some persons (or saw it fly by) about pushing R package vignettes to the web after building and checking on travis-ci. Hadley Wickham pointed to using such a scheme to push the web version of his book after each update and the S3 deploy hooks on travis-ci. Deploying your html content to S3 is great, but given the availability of the gh-pages branch on GitHub, I thought it would be neat to work out how to deploy the html output from an R package vignette to the gh-pages branch on GitHub. This is useful because more and more packages are being hosted only on GitHub and building and testing use the travis-ci service, and it takes work to remember to knit and push stuff separately to the gh-pages branch. In addition, although it is possible to deploy one’s html output to other services from within travis-ci, there is not an easy pushbutton solution to deploying to GitHub pages. After some searching and looking, this is what I have come up with for my own package.\nAll subsequent steps assume that:\nYou are hosting your package development on GitHub\nYou have html content in the gh-pages branch of your package repo (see more about gh-pages here and here)\nYou already have your package doing building and testing using travis-ci. See the r-travis project for more information on how to set this up.\nOAUTH\nTo start, you will need to generate an OAUTH token on GitHub that will be used to allow you to push back to your GitHub repo. This will be some really long alphanumeric string. You can generate one by going to settings -> applications -> personal access tokens -> generate new token. Make sure to copy this into a file that is not under version control.\nYou will also need to install the travis ruby framework.\ngem install travis\nAfter installing, navigate to your git repo for the project you want to enable automatic pushing of content for, and then login to travis-ci and secure the GitHub token.\ntravis login\n\ntravis encrypt GH_TOKEN=\"yourgithubtoken\"\nThis will generate output that should be copied to your .travis.yml file. Essentially we have created an environment variable GH_TOKEN with your actual GitHub token, that is encrypted on the travis-ci servers. So this way you don’t expose your actual GitHub token to anyone who looks at your .travis.yml file.\nDeploy Script\nWe also need a bash script that will actually push the content for us. As part of the R process on travis-ci, we get a tar.gz file of the package with the compiled vignette. So we just need to untar that file, copy the html file, and create the git repo and push. The code below is what I have done for my own package, categoryCompare. I saved this code in the file .push_gh_pages.sh.\n#!/bin/bash\n\nrm -rf out || exit 0;\nmkdir out;\n\nGH_REPO=\"@github.com/rmflight/categoryCompare.git\"\n\nFULL_REPO=\"https://$GH_TOKEN$GH_REPO\"\n\nfor files in '*.tar.gz'; do\n        tar xfz $files\ndone\n\ncd out\ngit init\ngit config user.name \"rmflight-travis\"\ngit config user.email \"travis\"\ncp ../categoryCompare/inst/doc/categoryCompare_vignette.html index.html\n\ngit add .\ngit commit -m \"deployed to github pages\"\ngit push --force --quiet $FULL_REPO master:gh-pages\nNote that we remove the directory where we want to create our git repo, create it, setup the remote repo with the token string, and then we untar and unzip the previously built package, and copy over the file that we want to be the index.html page on the gh-pages branch. Finally we add it, commit it, and do a force push.\nThis script is actually part of the package repo, but does not get included in the built tar.gz file (add it to .Rbuildignore). This makes it easy to keep it in sync with any changes to the overall package itself.\nNote that this is completely overriding the current contents of the gh-pages branch. If you wanted to do something nicer (i.e. preserving commits or working with an index page pointing to multiple vignettes), you could pull just the gh-pages branch first, and then make modifications.\nModifying .travis.yml\nIn addition, we need to add three lines to the .travis.yml file.\n# under env: global:\n  - secure: \"yoursecurestring\"\n\n# under before_install:\n  - chmod 755 ./.push_gh_pages.sh\n\n# under after_success:\n  - ./.push_gh_pages.sh\nAdding the GH_TOKEN to the global environment variables\nMaking the deploy script executable\nAdding the running of the deploy script after_success, so only when build and check and tests run successfully\nAnd it seems to work quite nicely. As an example, my categoryCompare package now has it’s vignette on the gh-pages branch, and this will get updated every time I push a commit.\nUpdate\nAs Carson pointed out below, there was an error in the second code block. travis secure should be travis encrypt, as you are encrypting the credentials. secure is for decrypting something that was already encrypted. Thanks for catching it!\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:52:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-08-08-my-career-goals/",
    "title": "My Career Goals",
    "description": "I don't want to be a lab PI, but I want to stay in academia.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-08-08",
    "categories": [
      "post-doc",
      "career",
      "academia"
    ],
    "contents": "\n\nContents\nTL;DR\nThe Bad News\nThe Good News\nMy Own Path\n\nTL;DR\nI don’t want to be a PI because I enjoy spending time with my family, and don’t think I can handle the stress of juggling multiple grants, people, and deadlines. I want to be a staff member in a group that affords relative autonomy, while providing some security. If I’m lucky enough, my current position will enable that.\nThe Bad News\nIf you keep up with the news in academia, this is a horrible time to be a postdoc (post-doctoral) or PI (principal investigator). The amount of money available for grants is at an all time low, and the likelihood of getting a grant is dismal. If you want to be successful, then you will need to work incredibly hard, sacrifice everything, and it is likely that you still won’t get grants that allow you to have an independent research lab at a decent university.\nOf course, this is worst in the biomedical field, due to a glut of NIH research money and universities allowing researchers salary to be composed largely of grant money, with very little money from the university itself. In addition grant money has been allowed to be used to support trainees; undergraduate, graduate, and postdoctoral. The compensation for the trainees was relatively minimal, leading to a glut of highly trained, poorly paid people who hope to move up the academic ladder. The large numbers of which has likely contributed to the current funding situation.\nThe Good News\nThe NIH and NSF are starting to encourage research labs to hire staff at reasonable pay, and support trainees from specific grants. Although I keep hearing about this in general, I don’t know of any specific changes in grant applications that are causing any real changes.\nMy Own Path\nDuring my senior undergraduate and masters work, I realized that I probably couldn’t be a student forever, but I had hopes. During my PhD, however, I saw many PI’s who sacrified their time with families for their teaching, research, and administrative duties. This was even more pronounced during my first PostDoc, and made me question my continued progress in academia. I actually had a job offer as a research programmer with a research hospital during my first PostDoc, which I turned down because I realized that it was likely in that position (at least as advertised) there would be limited opportunity to pursue my own research ideas. However, grant funding success rate was continuing to plummet (see above), and I was not able to push out a first author paper before finishing my first PostDoc, so being successful in a faculty search was looking grim.\nWhen I started my second PostDoc position (transitioning from transcriptomics to metabolomics), I was up front with my supervisor that I did not feel that I was the type of person who could manage their own lab with independent funding, but that I wanted to find a place where I could stay on as staff while contributing research. My PI was sympathetic, and we worked well together, so he was immediately amenable to that, as long as the funding situation would provide.\nFortunately, within a year of joining the lab, we and our collaborators landed a large center grant that provides stable funding for 5 years, and everyone involved (my PI, the other PI’s, myself) would be happy if I stayed on as staff after my time as a PostDoc. Already, as a PostDoc in this group I have a lot of latitude and freedom to pursue avenues of research, as long as they are somehow aligned with the overall goals of the lab. I am also finding myself involved in supervising various undergraduate and graduate projects, as well as proposing new avenues for research.\nI know the opportunity to stay on as staff in a research group is relatively rare in the current funding climate, and my job will be largely to develop novel methods and software to advance our understanding of metabolism in various biological states, as well as provide means to secure additional funding for the lab.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:51:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-07-28-analyses-as-packages/",
    "title": "Analyses as Packages",
    "description": "Why I think packages make good ways to structure an analysis.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-07-28",
    "categories": [
      "R",
      "development",
      "packages",
      "vignettes",
      "programming",
      "analysis"
    ],
    "contents": "\n\nContents\nTL;DR\nAnalyses and Reports\nWhy Packages\nStructure\nFunctions\nFunction Documentation\nData\n\nWhy Vignettes as a Reporting Method\n\nTL;DR\nInstead of writing an analysis as a single or set of R scripts, use a package and include the analysis as a vignette of the package. Read below for the why, the how is in the next post.\nAnalyses and Reports\nAs data science or statistical researchers, we tend to do a lot of analyses, whether for our own research or as part of a collaboration, or even for supervisors depending on where we work. As I have continued working in R, I have progressed from having a simple .R script (or collection of related scripts) to using a package to structure as much of my research as possible, including analyses that generate reports.\nNote that I have been meaning to write this post for a while, but the tipping point was seeing these tweets from Hilary Parker and David Nusinow\n\nI am all about the many short scripts rather than one long script when doing an analysis. I think I am alone here.\n\nWhy Packages\nPackages are R’s method for sharing code in a sensible way, making it possible for others to easily (more often than not) use functions that you have written (I’m looking at you python!). Why not use them? They also give you access to R’s facilities for documentation and sharing computable documents. Hadley Wickham has a nice section on packages in his Advanced R book.\nI use a lot of Hadley’s packages in the following sections, because they are useful, and promote practices that make it extremely practical to use packages as a way to make an analysis a self-contained unit.\nDuncan Murdoch has a nice slide deck on why to use packages and vignettes here\nStructure\nI want to breifly review the structure of package directories, you can read more about packages in Hadley’s book (link above), and in the official R documentation from CRAN.\nPackages impose a relatively simple structure on your project directory. /R contains the .R files with your actual functions, and /data can contain any .RData or .rda files that you might need. Other data types (.txt, .tab, .csv) can also go in /data, or they may go in /inst/extdata. Note that in /inst/extdata you can specify any directory structure that seems appropriate.\nOther required files are DESCRIPTION and NAMESPACE.\nYou may also have .Rmd or .Rnw / .Rtex files in /vignettes that generate html or pdf output that combines prose and R code into a single document. This is where things get really interesting in being able to package up an analysis, especially when combined with functions.\nFunctions\nAlmost any analysis I have done involves writing at least one function, generally more, because I almost never do anything once in an analysis. Packages are the primary method of sharing functions in R that make sure that your functions play nice with the R NAMESPACE, and allow one to define function dependencies from other packages. If you define a function in a package (and export it), it immediately becomes re-usable in multiple analyses, without worrying about suffering from copypasta.\nFunction Documentation\nThe easiest way to document functions is by using roxygen2 (see the intro vignette). This allows you to worry about the documentation right next to the function itself, and not worry about writing separate documentation files in /inst/doc (really, you don’t want to do it, I have, and it is painful). The keywords in roxygen2 make sense, and are not hard to remember.\nData\nAs mentioned above, you can include data with your package. The neat thing about including data, is you can document it and have that documentation available as part of the package.\nI find it really useful to put any raw data that you want to work with in /inst/extdata in whatever format it exists, and then process the data and save it as an .RData file in /data, with associated documentation. It is also really useful if part of the calculations are long running, then you can save the results as an associated data file, and simply load it when needed in the analysis.\nSmall note about documenting data sets. You put the roxygen2 comments in another file, and also need to provide @name explicitly, and follow the documentation block with NULL. Check the roxygen2 vignette “Generating Rd files” for a specific example.\nWhy Vignettes as a Reporting Method\nOne great feature of packages is that one can include multiple vignettes, long form text mixed with code (and/or figures) to explain or highlight functionality in a package. Normally these are used to write tutorials, demonstrate features, or group together documentation that wouldn’t normally be together in the general documentation. However, there are no limits as to what can actually be contained within the vignette as far as content, or how many vignettes a package can have.\nFor packages hosted by CRAN, vignettes are an optional component. However, the Bioconductor project requires that vignettes be included in each package.\nSo, R packages have a method to include long form prose that can be mixed with R code directly as part of the package, within which you have already put your functions and associated data.\nPrior to R 3.0, one generally had to write vignettes using sweave, a combination of latex and R code that generates a PDF file. However, since v3.0, it is possible to write vignettes using R markdown (and actually some other markup formats), which generates HTML output. The advantages to using R markdown over sweave are that the syntax for writing markdown is much simpler, and much more readable in it’s raw format.\nGiven that a package allows us to define sets of related functions, data, and documentation (with dependencies defined) all in one place that others can subsequently install and make use of and build on, why wouldn’t you want to use packages and vignettes to write long form analyses?\nFrom some of my descriptions above, it may appear that this incurs some overhead. However, thanks to the #hadleyverse and rstudio, it is rather trivial (note that rstudio is not essential, but I find it does make it easier). In my next post I am going to give a worked example from start to finish of generating an analysis that is a vignette as part of a package.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:48:05-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/",
    "title": "Creating an Analysis as a Package and Vignette",
    "description": "A walkthrough creating an analysis project as a package.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-07-28",
    "categories": [
      "R",
      "development",
      "packages",
      "vignettes",
      "analysis",
      "programming"
    ],
    "contents": "\n\nContents\nSetup\nInitialization\nRStudio Project Options\n\nData\nVignette\n\nStart the Analysis!\nPreview Report\nGenerate Vignette\nCommit and Push it ALL!\nNot Covered\n\n\nFollowing from my last post, I am going to go step by step through the process I use to generate an analysis as a package vignette. This will be an analysis of the tweets from the 2012 and 2014 ISMB conference (thanks to Neil and Stephen for compiling the data).\nI will link to individual commits so that you can see how things change as we go along.\nSetup\nInitialization\nTo start, we will initialize the package. devtools or rstudio make this rather easy:\n\n\nlibrary(devtools)\ncreate(\"~/Documents/projects/personal/ismbTweetAnalysis\")\n\n\n\nCreating package ismbTweetAnalysis in ~/Documents/projects/personal\nNo DESCRIPTION found. Creating with values:\n\nPackage: ismbTweetAnalysis\nTitle: What the package does (short line)\nVersion: 0.1\nAuthors@R: \"First Last <first.last@example.com> [aut, cre]\"\nDescription: What the package does (paragraph)\nDepends: R (>= 3.0.3)\nLicense: What license is it under?\nLazyData: true\nAdding Rstudio project file to ismbTweetAnalysis\nAlternatively, you can use File > New Project > New Directory > R Package in rstudio. Don’t forget to Create a git repository (or git init in the directory). Note that the devtools created package will pass CRAN tests, whereas the rstudio will not.\nOpen the DESCRIPTION file, and you will need to change the Title, Authors or Authors@R, Description, License, and add VignetteBuilder: knitr at the end. Here is what my initial setup looks like.\nRStudio Project Options\nIn addition, to make our life easier, we will change some options in the rstudio project.\nTools > Project Options > Build Tools, check Generate documentation with Roxygen, and select turn on all the options. We want to roxygenize when we Build & Reload especially, and have roxygen control the NAMESPACE file so we don’t worry about it.\nAlternatively, you can use document with reload=TRUE in devtools to update documentation and reload the functions.\nHaving this particular option of documenting and reloading the package every time I write a new function is what makes this easy. I write the new function, document/reload, and I can keep chugging along with my analysis document. And if I have to restart, I just run all chunks to get back to where I need to be.\nData\nNow we need some data. Neil’s data from 2012 uses a CSV format, however the tweets themselves have commas, so we will download the rdata file and use that, and also Stephen’s data from 2014. However, there are three separate files for 2014, so we will download all three files and combine them. Both initial data sets will go in the /inst/extdata folder, and we will clean them up.\nHere we have added our 4 data files.\nVignette\nWe are going to write this analysis as the vignette of the package, using R markdown as the language. To do that we need to create the file and add some boilerplate at the top so that the vignette gets generated properly. Here is the initial vignette, it is nothing but the engine and index definition, which are important.\nStart the Analysis!\nAt this point we can start the analysis. The actual analysis will be done in the Rmd vignette file. The basic process is to add prose describing the analysis, with actual code to generate results and figures embedded in the Rmd, and adding functions and documentation (as roxygen tags) in the .R file, while doing iterations of document or Build/Reload along the way. Iterations of document / Build/Reload after writing new functions in the .R file will make them available to us in our workspace, with tab-completion in rstudio.\nThe following are bullet point summaries of points when I committed or built/reloaded, with links to the commit so you can see what has changed in the package.\nAdding description of data sources to analysis\nMunging 2012 data a little, saving, and documenting\nNow we can load up this data with data(ismb2012)\n\nFunction written and exported for reading ST’s data files\nRead in, combine, and re-save ST’s archive\nNote that this and previous chunk have eval=FALSE, so that they are not run in the analysis, but they were run interactively while I was doing the work.\n\nSimple histogram of 2012 tweets by day\nMaking a counting function by screenName\nExamining the top tweeters using previous function\nFixing data files, because of issues with having the same named object in different RData files\nExamining top tweeters in 2014\nDensity of tweets with respect to starting time of conference\nCounting how often a specific tweet was retweeted\nGetting raw tweet ranks for each individual\nExamining the ranks by total retweeted tweets per user\nAnd at this point I’m going to stop there. Now we have an analysis (that we will make into a nice output shortly), and we have munged 2 data sets, and wrote 6 functions, that may be useful in other contexts.\nPreview Report\nTo preview the report, you can use the Knit HTML button in rstudio, or also use knitr directly. This will give you an html preview of the final report.\nGenerate Vignette\nOnce happy with the report, you can use devtools::build_vignettes() to generate the vignette files that will be copied to the relevant locations.\nCommit and Push it ALL!\nAt this point, if you are happy with the package and analysis as a whole, you should commit all the package files to version control and make it available. In this case this means:\ninst/doc: the output vignette\nman: the function documentation\nDESCRIPTION: our description file\nNAMESPACE: the file documenting our namespace\nYou can see this commit here.\nNow your package can be installed by others using devtools::install_github(). You could also submit your package to CRAN or Bioconductor if so desired.\nNot Covered\nNow this was a simple example. Ideally I should have included tests for my functions, you can read up 1 2 on how to do that. In addition, none of my functions use methods (see why they are useful).\nI hope that you find this example useful, and will consider using packages more often even for simple analyses.\nReproducibility: One issue that may come up is how to make sure that you or someone else can directly reproduce the work in your package. Again, Hadley Wickham and the rstudio team have been thinking about this, and there is now the packrat package to make a project completely self-contained with all of it’s dependencies.\nEdit 2014-07-28 - added note on reproducibility at the end.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:46:19-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-07-28-packages-vs-projecttemplate/",
    "title": "Packages vs ProjectTemplate",
    "description": "Why I think packages are better than the projectTemplate package.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-07-28",
    "categories": [
      "R",
      "packages",
      "analysis",
      "development"
    ],
    "contents": "\n\nContents\ntl;dr\nProjectTemplate ??\nDifferences\nKeeping Directories Straight\nDocumentation\nDependencies\n\nPackage Reproducibility\nOutputs\nFinal Thoughts\n\ntl;dr\nImposing a different structure than R packages for distributing R code is a bad idea, especially now that R package tools have gotten to the point where managing a package has become much easier.\nProjectTemplate ??\nMy last two posts (1, 2) provided an argument and an example of why one should use R packages to contain analyses. They were partly motivated by trends I had seen in other areas, including the appearance of the package ProjectTemplate. I was reminded about it thanks to Hadley Wickham:\n\nwould be interesting to see comparison to ProjectTemplate\n\nJohn Myles White has an R package, ProjectTemplate for handling analyses. In contrast to the package philosophy that I espoused previously, ProjectTemplate has a large, logical folder layout (you can read all about each folder and why it exists here). There is a folder for raw data, a folder for R scripts, a folder for reports, etc.\nHadley probably wanted me to do the same analysis using ProjectTemplate, but I don’t have the time for that. Instead I’m going to write below about the features ProjectTemplate has, and why I think packages are a better general solution. So, just to be clear, I have not done any analyses using ProjectTemplate, I have only the website descriptions to go on.\nNow, I think I understand the philosophy of ProjectTemplate, in that when writing a manuscript or report in an external editor such as LaTex, Word, LibreOffice (or even something else), you want a directory of outputs (graphs, tables, etc) resulting from applying a series of transformations on some data (R scripts applied to .txt or .csv, etc). In addition, a package is for storing general functions that will work across multiple projects.\nDifferences\nKeeping Directories Straight\nI can understand this POV, and in the past would have largely agreed. However, with the development of devtools and other tools that make managing R packages rather easy, I think it makes more sense to use the R package mechanism instead of a custom format. In the ProjectTemplate you may write functions or code in the /lib, /munge or /src directory depending on their purpose (general, data munging, actual analysis, respectively). Keeping all of this straight seems to me a waste of time from doing the actual analysis.\nIn a package, functions are in the /R directory, and that is where they live. They may be organized into different files depending on their purpose, names, or methods and classes (not fun S4, not fun), but at least I know where they are.\nDocumentation\nThis for me is the kicker. With packages (and roxygen2 and devtools), I can document everything in such a way that the documentation is available without looking at the source file; (i.e. ?function) be reminded of the various arguments, or look up the properties of my documented data set.\nDependencies\nR packages naturally tell you what other packages they depend on, and will try to resolve those dependencies when you install them. Now, granted, this isn’t a perfect process (anyone trying to upgrade their full R package library knows this), and with the proliferation of packages on github is likely to get more difficult (hmmm, would be nice to have a central registry of github available packages), but in general it does work (for a different solution, check out packrat. This means that an analysis that is a package can naturally get it’s dependencies at install time (really try it, go ahead and install my example package without having stringr installed).\nIn contrast, ProjectTemplate assumes all of the required packages are already installed, and if you shared your analysis directory with someone else, they have to look at the list and install everything before they are able to run it (although this is probably not that bad).\nPackage Reproducibility\nIn this day and age of seeking to provide reproducible analyses, having all of the entities (code, data, final report) in a container that knows how to install it’s dependencies, and that works within the language ecosystem to provide documentation on the functions used, seems really useful. I believe that R packages provide that better than most other solutions I have seen lately.\nOutputs\n“But I don’t want to write my final report using Markdown or Latex” you cry! “How will I get my graphs or tables otherwise?” if using a package? I didn’t mention this in the previous posts, but it is possible to write the code in such a way to save graphics or write text files into a specific location (I would recommend /inst/extdata/output if it was me).\nFinal Thoughts\nAs I said, I think I understand John’s thought process into designing ProjectTemplate, but given how easy it is to work with packages in the current R ecosystem, and that R packages are the built-in way to share things, I think it is more natural to use packages directly with prose of the analysis as a package vignette. But everyone works differently. If you are writing analyses in R, please use a sane directory setup (either packages, or ProjectTemplate or something else), and use version control (really, please learn how to use mercurial or git, your future self will thank you for it).\nRemember, when the reviewer (or your boss, or you) asks for something to be added, or to change something slightly in a figure, or add a new dataset, you want to be able to do it easily, and regenerate all of your results without manual intervention.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:51:09-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-19-researcher-discoverability/",
    "title": "Researcher Discoverability",
    "description": "Why do we need corporate products to enhance \"researcher discoverability\"?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-06-19",
    "categories": [
      "github",
      "open-science",
      "academia",
      "research"
    ],
    "contents": "\n\nContents\nPreprints\nGithub\nTwitter\nOpen Science\nHow to implement\n\nUniversity of Kentucky (UK) recently partnered with the discovery portal KNODE, for helping others to discover potential collaborators at UK. KNODE looks like a large corporate venture, that is probably costing a large amount of capital to the university (and other places that use it). I wonder if the universities money would be better spent on encouraging submission of preprints, a Github Enterprise/Education package and teaching researchers and faculty how to use social media like twitter. Here is why I think that.\nPreprints\nSubmitting manuscripts to a preprint server such as arxiv, biorxiv or peerj gives a researchers work possible visibility, and can result in immediate feedback to improve a work prior to submission, or as part of the peer review.\nGithub\nMuch of our scientific output is generated as text. Whether it is lab notes, tables of data, draft manuscripts, or scientific scripts and programs, a lot of it is text. The Git version control system is well placed to handle iterations of text, and Github has become the defacto location for sharing version controlled text with others, and collaborating on it. Although I would imagine that it would take some encouragement, I think more researchers would use Git and Github if they knew about it, and had easy options to make Github repos private until they were ready to share their work.\nIn addition to putting general text under version control, more researchers should be encouraged to use version control on any analysis scripts / programs that they write. This is becoming important across disciplines, not just in STEM fields. There is also more and more encouragement from journals and funding bodies to make underlying data and the processing of the data available. By putting data, and code on Github, moving from private collaboration to fully shared can be changed with the flick of a switch. Furthermore, master branches of repos are crawled by Google, making your text discoverable, making webpages for a group and on a project basis is relatively painless using github-pages, and repos can also have a wiki associated with them to allow further documentation.\nTwitter\nI’m going to use twitter as a proxy for social media in general (blogs, facebook, twitter, and others), although I believe twitter combined with a blog is probably the best combination for science outreach.\nHaving a blog to put announcements about when a new publication comes out, when a new presentation has been done (you are putting presentations on figshare or slideshare), with links to the work in question. Twitter (and other social media platforms) allow quick communication to a wide audience that your research group has a new research output, and can allow engagement with other researchers in the field as well as non-experts, all of which improve ones visibility and provide opportunities for collaboration.\nBy the way, github-pages as mentioned above, also makes it easy to set up a static blog (non database blog site) for rather easy blogging.\nOpen Science\nAll of the above encourages open science, sharing of results early and often. It does not have to, but as people get used to being able to share their work early and often, one would hope that this would become the norm, and it would lead to other researchers and possibly interested companies discovering applicable research for either collaboration, licensing, or simply re-use.\nHow to implement\nNow, if you’ve read this and you are not already doing these things, you might think this is all a lot of effort. Well, yes, it is. It takes time. This blog post didn’t write itself, you know. My twitter use sometimes gets a bit excessive and addictive. And some of what I have described is rather technical in nature, and might take training to implement. For example, best practices around code and version control. I’m sure Software Carpentry would love to do workshops at UK for researchers (disclaimer: I have done the SC training, but haven’t done a workshop yet). But overall, I know for me all of this (I’m speaking from experience and what I have observed in others) has been a plus to my research, not a minus.\nSo what say you, University of Kentucky? Why not try something different to make your researchers stand out to the world, and improve the scientific process?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:40:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-06-05-bioinformatics-presentations-that-lack-results-or-biological-relevance/",
    "title": "Bioinformatics Presentations that Lack Results (or Biological Relevance)",
    "description": "Why do people have bionformatics presentations lacking relevance or results?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-06-05",
    "categories": [
      "bioinformatics",
      "academia",
      "research"
    ],
    "contents": "\n\nContents\nTL;DR\nSeminar Without Results\n\nTL;DR\nIn bioinformatics research we need to show validated results (if doing classification or discovery of new things), or show biological relevance. If you do neither of those things in a paper or presentation, then I’m not going to believe your method is worth anything.\nSeminar Without Results\nI attended a seminar yesterday (I’m not going to comment on who gave the seminar or what it was about, so please don’t ask) where the presenter had a distinct lack of any useful results. Their presentation consisted almost entirely of methods description, with various discussions of why they did what they did, with the only result being:\nWe found X number of known things, and predicted Y number of novel things\nJust to clarify, they had a methodology that generated classifiers based on features from known items to find novel items, and then associate both known and novel with biological conditions (e.g. disease vs non-disease).\nIn all of the slides shown, there was no discussion of whether the features in the classifier were relevant, how well the classifier worked when splitting the known stuff into training / test data sets, and whether anything discovered (both of the known and unknown) had any biological relevance or had been validated by wet-lab collaborators. This is the worst kind of bioinformatics communication, because the audience had no idea in the end if the method was actually useful or generated biologically relevant results. This is really the point of bioinformatics and more broadly systems biology research, is making something that helps us decipher biological systems, even in some small way. If you don’t have any data on either the accuracy of classification (in this case finding novel things that were unknown) or the biological relevance of things found to be different, not even one data point, then you should not be giving the talk or writing the paper. Period.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:39:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-04-08-categorycompare-paper-finally-out/",
    "title": "categoryCompare Paper Finally Out!",
    "description": "My first first author publication since starting my PostDoc is finally out, about my meta-annotation-enrichment software package categoyrCompare.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-04-08",
    "categories": [
      "R",
      "bioconductor",
      "meta-analysis",
      "publications",
      "git",
      "github",
      "open-science",
      "visualization",
      "annotation-enrichment"
    ],
    "contents": "\n\nContents\nTL;DR\nInspiration\nV 0.0000001\ncategoryCompare Method – Summary\nOthers\nBioconductor Package\nGraphviz to RCytoscape\n\nFirst Publication Attempt\nSecond Publication Attempt\nFrontiers Interactive Review\n\nGithub package of supplementary materials\nPublication specific branch\n\nTo Do\nSide Effects\nWhat Would I Do Differently?\n\nI can finally say that the publication on my Bioconductor package categoryCompare (Flight 2012) is finally published in the Bioinformatics and Computational Biology section of Frontiers in Genetics (Flight et al. 2014). This has been a long time coming, and I wanted to give some background on the inspiration and development of the method and software.\nTL;DR\nThe software package has been in development in one form or another since 2010, released to Bioconductor in summer 2012, and the publication has bounced around and been revised since spring of 2013, and it is finally available to you. All of the supplementary data and methods are available as an R package on github. Version control using git was instrumental in getting this work out in a timely manner. There is still a bunch of work to do on the package.\nIf I did it again, I would: * Write the manuscript using R markdown, as a vignette in a package * Ask to be able to make reviewer points issues on Github * Submit a preprint with submission\nInspiration\nIn spring of 2010 I started as a PostDoc with Eric Rouchka. One of his collaborators, Jeff Petruska is interested in the process of collateral sprouting of neurons, especially as it compares to regeneration. Early in my PostDoc, Jeff wanted to do a gene-level comparison of his microarray data of collateral sprouting in skin compared to previously published studies with muscle.\nCombing through the literature produced a number of genes differentially expressed in denervated muscle. However, when comparing with the genes resulting from the skin data, there was almost nothing in common and nothing that made sense from a functional standpoint. Skimming around the Bioconductor literature in GOStats, I was struck by Robert Gentleman’s example of coloring Gene Ontology nodes by which data set they originated from. This is a very simple meta-analysis and visualization. When I tried it with the skin - muscle comparison, I got some very interesting (i.e. the Petruska group thought the results were very interpretable) results.\nNote: At this point, because of the data sources (gene lists from publications with little to no original data), I was using the hypergeometric enrichment test in Category to determine significant GO terms from the two tissues.\nV 0.0000001\nI started developing this idea into a simple package (i.e. collection of R scripts), that was able to do at least GO term enrichment, and that could be hosted on our group webserver to enable others to make use of it. Visualization and interrogation used the imageMaps function in Robert Gentleman’s original demonstration, however any number of data sets could now be compared.\ncategoryCompare Method – Summary\nThe basic method is to take gene (or really any annotated feature) lists from multiple experiments, and perform annotation (Gene Ontology Terms, KEGG Pathways, etc) enrichment (either hypergeometric or GSEA type) on each gene list, determine significant annotations from each list, and then examine which annotations come from which list.\nBecause this results in a lot of data to parse through, exploration of the results is facilitated by considering the annotations as a network of annotations related by the number of shared genes between them, and interacting with the networks in Cytoscape.\nIf you want to know more, check out the paper (Flight et al. 2014), or the vignette in the Bioconductor package (Flight 2012)\nOthers\nAs I developed this idea, I also started looking into other possible software implementations.\nShen & Tseng had recently published similar work in MAPE-I, but at my level of understanding it required the same identifiers (Shen and Tseng 2010). In retrospect, I should have read the paper better. However, they also did not have a released implementation with their publication.\nConceptGen is another interesting application, that allows very similar analyses to categoryCompare, except that it is not possible to explore how the concepts map between multiple user supplied data sets (Sartor et al. 2009). The only way one can relate multiple data sets is if the data sets map as a concept to another, but one cannot visualize the interrelated concepts between user supplied data sets.\nEnrichment Map was another alternative, but did all of comparison mathematics in Cytoscape itself, based on enrichments calculated outside of Cytoscape (Merico 2010). The publication does give an example of a similar type of analysis as performed by categoryCompare. However, I wanted everything, from enrichment calculation to visualization controlled by R. I did use their method of weighting the edges between annotations, however, ditching the GO directed acyclic graph (DAG) view I used initially.\nBioconductor Package\nAbout this time I realized that I wanted the package to fit into the Bioconductor ecosystem. This required a complete redesign and rewrite of the code, as I moved to actually used some sort of OOP model (S4 in this case), and creating an actual package.\nThe package was released to the wild in the fall of 2012, as part of Bioconductor v 2.10. Of course, this was the last Bioconductor release based on R 2.15, which brought some particular challenges in namespaces with the switch in R 3.0.0 the next year.\nGraphviz to RCytoscape\nThe original code used the graphviz package to do layouts for visualization, but it was difficult to install, and did not work the way I wanted. Thankfully Paul Shannon had just developed the RCytoscape package the year previous (Shannon 2013), and this enabled truly interactive visualization and passing data back and forth between R and Cytoscape.\nMy use of RCytoscape has actually lead to finding improvements in the package, and better use of it. I am also hoping to make much more use of RCytoscape and the igraph package and combining them in novel ways.\nFirst Publication Attempt\nOur first attempt at publication focussed on the original data that inspired the method, and comparing it with gene-gene comparisons directly. We submitted to BMC Bioinformatics, and the reviews were not favorable, and took forever to get back. We actually wondered if the reviewers had read the paper that we submitted. We gave up on this venue. BTW, our last three publications have been submitted there, and the publication process has gotten worse and worse with every submission there. I don’t think our papers are getting worse over the years. I don’t know if we will bother submitting to BMC Bioinformatics again.\nSecond Publication Attempt\nThe second attempt was submitting to the current home in the Bioinformatics and Computational Biology section of Frontiers in Genetics. This time, although the reviews were harsh (i.e. they were not immediately favorable), they were fair, and actually contained useful critique, and pointed to a way forward.\nUnfortunately for me, revising the manuscript to address the reviewers criticisms meant a lot of work to construct theoretical examples, as well as a lot of thought in order to pare the manuscript down to make sure our primary message was clearly communicated.\nFrontiers Interactive Review\nI would like to note that the Frontiers interactive review system, with the ability to discuss individual points with the reviewers (still anonymously) really helped make it possible to determine which points were make or break, and discuss different ways to approach things. This was the best review experience I have ever had, and a large part of that I think was due to being able to interact with the reviewers directly, and not just through letters mediated by the editor.\nI think it would be nice if Frontiers had an option for making the review history available if the authors and reviewers were agreeable to it.\nGithub package of supplementary materials\nIn the initial publication, I had included the set of scripts used for analysis, data files, results, etc. However, the amount of work required in the rewrite was so substantial, that I created an R package specifically for the analyses that went into the paper, with separate R markdown vignettes for each result type (hypothetical, two different experimental comparisons). This package has documents on how raw data was processed, as well as the semi-processed experimental data.\nPublication specific branch\nDue to specific changes to the categoryCompare software needed to address the reviewer comments, a publication specific branch of the development version was created (paper). This allowed me to quickly introduce code and features that the reviewers had asked for, without worrying about breaking the current development version that will be released in Bioconductor shortly.\nTo Do\nAs with most software projects, there is still plenty to be done.\nIncorporate new functionality from paper branch into dev\nSpecifically ability to do GSEA built in (probably using limma’s romer and roast functions), and new visualization options\n\nChange from latex vignette to R markdown (this is technically done, but hasn’t made it into the dev branch)\nSwitch to roxygen2 documentation (will be interesting due to use of S4 objects and methods)\nImplement proper testing using testthat\nRefactor a lot of code to improve speed, use R conventions\nI was still a relatively R newbie when I wrote the package, and as I looked at the code while making changes for the reviewers, I noticed some places where I had done some silly things, mainly because I didn’t know better when I did it.\n\nConsider splitting visualization into its own package\nAlthough I developed the visualization specifically for categoryCompare, I think it is generic enough that others might benefit from having it available separately. Therefore I need to think about separating it out, and how best to go about it without making it hard for categoryCompare to keep working as it already does.\n\nMake it easy to investigate the actual genes with particularly interesting annotations, and how they are linked together by annotations or other data sources, as well as their original expression levels in the experiments.\nSide Effects\nA nice side effect of the package is that any annotation enrichment I do now, I almost always do it in the framework of categoryCompare, just because it is a lot easier to make sense of using the visualization capabilities and coupling with Cytoscape.\nWhat Would I Do Differently?\nKeeping in mind that I started this work almost 4 years ago, when I still didn’t know any R, and had yet to be exposed to the reproducibility and open science movements, or knitr, or pandoc, here are some things that if I started today I would do differently (you can probably guess some of these from above):\nPut the package under version control immediately! Thankfully I didn’t have any moments early in the process when things were not in a git repo, but I am very thankful later on I was able to do diff and branch on my code to figure out where things broke and introduce new features.\nStart thinking of the analysis as a standalone package from the very beginning, instead of a directory of data and scripts. This is what I do now (blogpost to come), and it makes it much easier if I come up with novel methods to spin them off into a fully fledged R / Bioconductor package\nDon’t underestimate the novelty of something as simple as visualization, and how much it may make or break your method. We ended up adding a good chunk of text to the manuscript on the visualization because we realized how important it was, but only after the reviewers pointed it out to us.\nWrite the paper as a vignette of the ccPaper package itself, and generate Word documents for collaborators who insist on Word docs using Pandoc\nStart a github repo for the paper, and ask collaborators to try and work on it there\nSubmit a preprint when submitted, so that we start getting feedback on the manuscript early\nAsk for permission to set up reviewer comments as issues on the github repo to easily track how well we are addressing them.\nWouldn’t it be cool in a totally open peer review journal to actually do all of the peer review on a service like Github, and have reviewers leave issues, tag them, and comment directly on the text of the publication using the commenting feature of commenting on commits?\n\n\n\nFlight, Robert M. 2012. “categoryCompare: Meta-Analysis of High-Throughput Experiments Using Feature Annotations.” https://doi.org/10.18129/B9.bioc.categoryCompare.\n\n\nFlight, Robert M, Benjamin J Harrison, Fahim Mohammad, Mary B Bunge, Lawrence D F Moon, Jeffrey C Petruska, and Eric C Rouchka. 2014. “categoryCompare, an Analytical Tool Based on Feature Annotations.” Frontiers in Genetics. https://doi.org/10.3389/fgene.2014.00098.\n\n\nMerico, Ruth AND Stueker, Daniele AND Isserlin. 2010. “Enrichment Map: A Network-Based Method for Gene-Set Enrichment Visualization and Interpretation.” PLOS ONE 5 (11): 1–12. https://doi.org/10.1371/journal.pone.0013984.\n\n\nSartor, Maureen A., Vasudeva Mahavisno, Venkateshwar G. Keshamouni, James Cavalcoli, Zachary Wright, Alla Karnovsky, Rork Kuick, et al. 2009. “ConceptGen: a gene set enrichment and gene set relation mapping tool.” Bioinformatics 26 (4): 456–63. https://doi.org/10.1093/bioinformatics/btp683.\n\n\nShannon, Grimes, P. T. 2013. “RCytoscape: Tools for Exploratory Network Analysis.” BMC Bioinformatics 14. https://doi.org/10.1186/1471-2105-14-217.\n\n\nShen, Kui, and George C. Tseng. 2010. “Meta-analysis for pathway enrichment analysis when combining multiple genomic studies.” Bioinformatics 26 (10): 1316–23. https://doi.org/10.1093/bioinformatics/btq148.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T17:39:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-20-self-written-function-help/",
    "title": "Self-Written Function Help",
    "description": "Do you want to be able to read function documentation for your own functions? Make your own package.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-02-20",
    "categories": [
      "R",
      "packages",
      "documentation",
      "development",
      "devtools",
      "roxygen2",
      "docstrings"
    ],
    "contents": "\n\nContents\nRStudio\nPackages\nRoxygen2 Documentation\n\nI have noted at least one instance (and there are probably others) about how Python’s docStrings are so great, and wouldn’t it be nice to have a similar system in R. Especially when you can have your new function tab completion available depending on your development environment.\nThis is a false statement, however. If you set up your R development environment properly, you can have these features available in R. It will take a little bit of work, however.\nThis approach heavily depends on devtools and roxygen2.\nRStudio\nI do recommend using the RStudio IDE, as much of what I am discussing is very much integrated into the IDE itself. However, much of what I discuss is applicable regardless of what development environment you use.\nPackages\nFirst off, you probably want to put your functions into packages. Really, it’s not that hard. A DESCRIPTION file, NAMESPACE, and an R directory with your function definitions. If you are using RStudio, then you can create a new project as a package. Alternatively, you can set up a new package directory using package.skeleton.\nIf you are using RStudio, I recommend setting your package options:\nGenerate package documentation with roxygen (roxygen2)\nSelect all the options (especially regenerate Rd files on Build & Reload)\n\nWhenever you make changes to your package functions, you simply commit (you are using version control, right??), and then Build & Reload (if using RStudio) or install() if using devtools.\nRoxygen2 Documentation\nFor documentation that lives with your functions, I heavily recommend using roxygen2. Although the normal way to document stuff in R is through the use of Rd files, roxygen2 allows you to put the following in your R\\functions.r file:\n#' this is a silly function\n#' @param input1 this is an input to our function\n#' @param input2 this is another input\n#' @return some value\n#' @export\nsillyFunction <- function(input1, input2){\n  FunctionBody\n}\nWhen you do Build & Reload (or install), the required Rd file will be generated automatically, and upon Reloading the package, you will have full access to your documentation, and tab completion of your new function, along with descriptions of the parameters if you are using RStudio. Note, if you are not using RStudio, then you should do document to re-generate Rd files prior to doing install.\nThis particular workflow is how I now work in R, for almost every project that includes any self written functions, including analysis projects. Why I use this (and not another format such as ProjectTemplate) is another post, hopefully soon.\nUpdated 2014.02.21: Note that the original post mentioned roxygen instead of roxygen2. Thanks Carl for pointing that out. You should use roxygen2 for documentation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T14:56:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-10-installing-matlab-vs-installing-r/",
    "title": "Installing MatLab vs Installing R",
    "description": "Personal frustrations around installing MatLab led to this particular rant.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-02-10",
    "categories": [
      "MatLab",
      "R"
    ],
    "contents": "\n\nContents\nInstalling MatLab\nInstalling R\n\nI retweeted this a few days ago:\n1. Open MATLAB for first time in a few years after using #rstats. 2. Site license doesn’t work right. 3. F*** MATLAB, I’ll try to do it in R\nAnd as I have started the process of installing MatLab on my own machine because I want to translate a published MatLab package into R, I am reminded of how painful the process can be.\nAs Andrew noted above, often times a university will have a site license for MatLab, and the license server will be local, and your installation must just contact the license server to know it is legit and run. However, if the server crashes (which they seem to be prone to in my experience), then you can’t run MatLab at all.\nInstalling MatLab\nAt my university, the process for even getting and installing a copy of MatLab is rather tedious.\nProve you are someone who is in a position to be allowed to install it\nReceive email with license file and further instructions\nOpen an IT ticket to have an account created at MathWorks\nLogin to MathWorks and download MatLab\nInstall\nGive license key\nSave license file\nRun MatLab\nNow, I understand that MathWorks wants to protect their intellectual property, and I also understand that they have previously been ripped off rather badly in the past (having a company purchase a single license and run a hundred copies). But this process is not conducive to getting research done.\nInstalling R\nDownload appropriate R installer from http://r-project.org\nInstall\nInstall desired packages\nWhich one of these is going to make it easiest to actually get coding and getting stuff done?\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-21T16:47:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "description": "Two git commit hooks for incrementing the package version as part of commits.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-02-07",
    "categories": [
      "R",
      "git",
      "packages",
      "development",
      "programming",
      "random-code-snippets"
    ],
    "contents": "\n\nContents\nPackage Version Incrementing\nGit Hooks\nRscript & read and write.dcf\nOverriding Increment\nInstallation\nCaveats\n\nIf you just want the hook scripts, check this gist. If you want to know some of the motivation behind writing them, and about the internals, then read on.\nPackage Version Incrementing\nA good practice to get into is incrementing the minor version number (i.e. going from 0.0.1 to 0.0.2) after each git commit when developing packages (this is recommended by the Bioconductor devs as well ). This makes it very easy to know what changes are in your currently installed version, and if you remembered to actually install the most recent version for testing.\nIf you are like me, this is a hard habit to get into, especially because I have to manually remember to go over to the DESCRIPTION file and up the version number when I make a commit. It seems I am not the only one according to this tweet from Kevin Ushey:\n\n\nIs there a git hook / alias that can auto-increment a package patch number (in DESCRIPTION) with each commit? #rstats #lazyweb\n\n— Kevin Ushey (@kevin_ushey) February 6, 2014\n\nI thought it sounded like a good idea, and got to work.\nGit Hooks\nFor those who don’t know, git allows you to have custom scripts (hooks) that are run at different steps in an overall git workflow. These are essentially executable scripts stored in the .git/hooks/ folder of an individual repo.\nFor our purposes, we would want something that runs either just before commit (pre-commit hook) or just after (post-commit hook), because we want to make modifications that are associated with commits themselves. The pre-commit hook is most appropriate, because we can make file modifications before our actual commit. However, if the user is already using a validation pre-commit hook, or wants the version change separate, a post-commit hook might be better. Both are available in the gist.\nI suppose this could have been done using an alias to run a script, but the nice thing about commit hooks is they are tied to the action of committing itself, instead of as a separate action.\nRscript & read and write.dcf\nR already has some nice functionality for reading and writing the debian control format (dcf) DESCRIPTION files that are required at the root of an R package. So ideally we could write our script in R and not have to do parsing from the bash shell. Rscript lets us do this easily by defining in the shebang (#!) where to find the executable to run the script.\n#!/path/2/Rscript\nThe rest is simply reading the DESCRIPTION file (read.dcf), getting the current version, incrementing, writing a new DESCRIPTION file (write.dcf), and making the changes to the git commit.\n\n\ncurrDCF <- read.dcf(\"DESCRIPTION\")\ncurrVersion <- currDCF[1,\"Version\"]\nsplitVersion <- strsplit(currVersion, \".\", fixed=TRUE)[[1]]\nnVer <- length(splitVersion)\ncurrEndVersion <- as.integer(splitVersion[nVer])\nnewEndVersion <- as.character(currEndVersion + 1)\nsplitVersion[nVer] <- newEndVersion\nnewVersion <- paste(splitVersion, collapse=\".\")\ncurrDCF[1,\"Version\"] <- newVersion\nwrite.dcf(currDCF, \"DESCRIPTION\")\n\n\n\nIn order to add the new DESCRIPTION file (pre-commit), or do a subsequent commit, we do need a couple of system calls to git itself.\n\n\nsystem(\"git add DESCRIPTION\")\n\n\n\nWe also need to check that there are files to actually be commited (for the pre-commit) before we go ahead and increment the version. The git diff command will tell us if there is anything or not.\n\n\nfileDiff <- system(\"git diff HEAD --name-only\", intern=TRUE)\n\n\n\nOverriding Increment\nThere is two wrinkles to this process. What if you don’t want to increment the version of your package for a good reason, like you’ve just changed the major version number? Or, in the case of the post-commit, you don’t want to end up in an infinite loop of incrementing.\nBut you can’t pass arguments from the git call to the script. It turns out we can prepend an environment variable definition, and then get it using Sys.getenv. So our script checks for the environment variable, and if it is defined, sets it. Otherwise it uses the default that is initialized at the beginning of the script.\n\n\ndoIncrement <- TRUE # default\n \n# get the environment variable and modify if necessary\ntmpEnv <- as.logical(Sys.getenv(\"doIncrement\"))\nif (!is.na(tmpEnv)){\n  doIncrement <- tmpEnv\n}\n\n\n\nSo to change the default value of doIncrement, you make your commit like this:\ndoIncrement=FALSE git commit -m \"commit message\"\nThis same behavior is used to keep from making an infinite number of increments in the post-commit script.\nInstallation\nTo install the hooks, create the pre-commit or post-commit file in your .git/hooks directory, paste in the commands from the appropriate one in the gist, save the file, and make it executable (chmod on ’nix systems). I have been able to use both of these on ’nix and Windows systems. Also don’t forget to provide the path to your Rscript executable (either in the same directory as your R binary, or at /usr/bin/Rscript).\nJust in testing these scripts I have become surprised at the potential utility, and I hope you find them useful as well if you are developing R packages (which you should be if you are writing any analysis).\nIf you have comments, suggestions, or improvements feel free to modify the gist, or leave a comment, contact me on twitter, or send me an email (check the About link above).\nCaveats\nThe script assumes the version separator is “.”, and that you always are incrementing the last value. If you don’t like it, feel free to change it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-05T10:22:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2014-01-28-motivated-learning/",
    "title": "Motivated Learning",
    "description": "Two personal stories on times I was very motivated to learn, as part of my Software-Carpentry instructor training.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2014-01-28",
    "categories": [
      "learning",
      "software-carpentry",
      "git",
      "calculus"
    ],
    "contents": "\n\nContents\nBeing Motivated to Learn Calculus\nPersonal Experience with Git\n\nAs part of the instructor training of Software-Carpentry, we were asked to write a blog post about two things:\nA story about a time you were motivated/demotivated to learn\nA story that will help motivate our learners drawn from personal experience\nHere are mine. I thought others who read my ramblings might find them useful. Note that these are cross-posted on the Software-Carpentry teaching blog.\nBeing Motivated to Learn Calculus\nAs part of my undergraduate degree, I was required to take higher level mathematics, either calculus or linear algebra. I had previously taken introductory calculus in high school, that consisted mainly in learning how to do derivatives of functions. Unfortunately, I tended to coast in high school, and my first year of university I did the same. I got a C in my first semester of calculus (differential calculus), and then a D in my second (integral calculus). I tried a different course, and recieved an F. This was my first (and thankfully my only) F in all of my school years. By this time I was in my third year, and was slowly starting to figure out how to study effectively. However, this class was required if I was to be awarded my degree in my chosen majors (biology and chemistry double major). So, I buckled down, and started truly trying to understand the material. I did the problems, and when I couldn’t figure them out, I went and got help. I asked questions, I worked with others in the class, and I kept trying, and trying, and really trying to understand the material. I finally finished that class with I think a B grade, and a much better understanding of why integrals, and calculus, is important.\nPersonal Experience with Git\nI do bioinformatics type research for a living, which necessarily means writing code, because I need code to do my analyses. As part of writing code, in the last year I have started heavily using the git revision control system. This past year, I was co-author on a publication about software that I had written. That software project is in a git repo, and I am very glad it is. The reviewers of my manuscript requested many changes to the underlying software package. Because I had it in git, I was able to create a new branch and start making requested changes, all without worrying about breaking the current codebase. Due to keeping the code on GitHub, I was also able to point to actual commits that address the reviewers concerns.\nIn addition, some of the reviewer points required writing entirely new analyses. As I worked on this, I needed to write over 500 lines of new functionality, as well as the new analyses. All of this went into the git repo, allowing me to keep track of progress, and easily revert or branch off as necessary when things went wrong. I don’t think I could have done the work that I did in a timely manner without version control.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T14:55:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-22-pubmedcommons-api/",
    "title": "PubmedCommons API",
    "description": "Pubmed commons is a new commenting system for pubmed articles.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-10-22",
    "categories": [
      "open-science",
      "pubmed"
    ],
    "contents": "\n\nContents\nLet me illustrate with an example.\n\nThe announcements are out, Pubmed is introducing a commenting system pubmedcommons, theoretically providing a single location for true post-publication peer review. This is a really good idea, as NCBI is likely to be around for a lot longer than a given publisher, and the requirement for all NIH funded research to be deposited into Pubmed.\nThere are some detractors, and they may have some valid points link. However, the alternative, pubpeer, I had not heard about. F1000 puts the comments in one location, in a way that is no better than a single publisher site.\nWhat would truly let the pubmedcommons proliferate would be if it became the equivalent of disqus, but for scientific articles. For those who don’t know, disqus is essentially a remote commenting platform that provides commenting on other websites. For example, my blog is a static site, and comments are provided by disqus. This is enabled with an account on disqus and a javascript snippet on my website.\nIt would be neat if pubmedcommons became a similar platform for scientific articles, with a little bit more flexibility. i.e. someone would be able to include some javascript, with relevant pubmed IDs, and the comments for that article would be displayed, and others would be able to make comments on that site.\nLet me illustrate with an example.\nImagine I publish a paper in PLOS One (or any other journal, for that matter). Upon acceptance, the paper also gets a pubmed id. This instantly creates a forum for comments using pubmedcommons. In addition, PLOS One adds a little bit of javascript to their page for the paper that enables the same commenting system. Therefore, any comments made on the paper at pubmed or PLOS One appear in both places (maybe with a tag for where they originated?). In addition, I post a copy of the paper on my own blog, and add the same javascript, and readers will see the comments made at pubmed, PLOS One, and can comment on the paper on my own blog. In addition, someone else could do the same to display the comments on their own site, etc, etc.\nI think the above example might have the potential to revolutionize post-pub peer review, because the actual comments are hosted in a central location, but are accessible in many different locations where the publication might exist.\nNow, it could still go awry. They could take too long to open things up, they might make the policies regarding comments too restrictive, they may police the comments too heavily, etc, etc. But\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T14:53:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-21-open-vs-closed-analysis-languages/",
    "title": "Open vs Closed Analysis Languages",
    "description": "Talking about R & Python vs MatLab as examples of open and closed data analysis languages.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-10-21",
    "categories": [
      "open-science",
      "R",
      "python",
      "MatLab",
      "programming",
      "development"
    ],
    "contents": "\n\nContents\nTL;DR\nWhy the debate?\nClosed\nClosed problem: Checking results\nClosed problem: Re-using code\n\nSolution: Open languages\nWhat should you use?\n\nTL;DR\nI think data scientists should choose to learn open languages such as R and python because they are open in the sense that anyone can obtain them, use them and modify them for free, and this has lead to large, robust groups of users, making it more likely that packages exist that you can use, and others can easily build on your own work.\nWhy the debate?\nThis was sparked by a comment on twitter suggesting that data scientists and analysts need to be polyglots, that they should know more than one programming language or analysis framework (the full conversation of tweets can be found here)\n\n\nData Scientists need to be Polyglots - know 2 or more of #python #rstats #sas #spss #matlab #julia #octave http://t.co/LtzIOzZ4XH\n\n— Gregory Piatetsky (@kdnuggets) October 8, 2013\n\nThe commenter suggested knowing at least two of:\nR\nPython\nMatlab\nSPSS\nSAS\nJulia\nOctave\nMy comment back was that one should really evaluate whether SPSS, SAS and Matlab should be on this list, as they are closed languages, not open, or free.\nI want to expand on why I made that comment. Let me be forthright, I have not used SPSS, nor SAS, but I have programmed in MatLab and R extensively, and dabbled in Python.\nI also think it is a good thing for data scientists to know more than one language. Just to be clear, I am NOT arguing that point.\nClosed\nWhat is a closed analysis language? I would say that there are three types of closed languages:\nthose that are not free (but still may their source code openly available)\nthose where the underlying engine is closed source\nthose where one cannot write their own functions to expand on what is already available\nNow, MatLab fits the first two categories. It is rather expensive to get a license for, the license can be restrictive (I know they have had a lot of abuse of licenses in the past, and they are trying to avoid that), and you are not expected to poke around in the internals of the MatLab engine. Oh, and if you want more than the base engine, expect to pay heavily for add-on packages.\nHowever, it is possible to write add-on’s for MatLab. I have previously written a few.\nClosed problem: Checking results\nSo why are closed languages a problem? A closed language that does not make it possible to examine the underlying functionality of the analysis engine has two problems:\nsurety that calculations are done correctly\nthe ability of others to run and check results\nBoth of these are issues that are really important in science. I would consider a data scientist to be doing actual science, so others should be able to scrutinize their work. The best scrutiny, is for others to be able to actually run their code. If I can’t run your code, then how do I know what you did is right?? If I can’t afford a copy of MatLab to run your code (assuming you made it available, you did provide the source for the analysis, right?), that is a bad thing.\nClosed problem: Re-using code\nOf course, the other problem is that with a closed language you have made it impossible for others to easily make use of your analysis. Sure, they could code it up in another language, but unless it is the be-all and end-all of analysis methods, I’m not going to bother. I don’t have a license for MatLab, or SPSS, or SAS, and I can’t afford it; therefore I’m not going to use your method / code nor give you a citation or credit.\nSolution: Open languages\nLanguages like R and python, they don’t have these problems. If I wonder how a function in the base distribution of R or python works, I can go look at the source. If I find a bug, I can suggest a fix, or fix it myself and tell others about it. In addition, if I write code to do an analysis, I can make it available and know that others should have the ability to examine it, including re-running it, in addition to using it for themselves, if it is licensed appropriately. This is the way science should work.\nWhat should you use?\nSome would argue that you should use MatLab, SAS, and SPSS because they have been around for a while, and are the standard. I would argue that you should not use them because they are controlled by single corporate entities, who are only interested in what will get people to buy their product and use it. You should use software that others are using, and that others will be able to use, regardless of income.\nR is being used in lots of different places, by lots of different people for statistics, bioinformatics, visualization, and as a general functional language. Python is a great general purpose language that provides a lot of functional glue for doing lots of different things.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T14:53:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-10-17-pre-calculating-large-tables-of-values/",
    "title": "Pre-Calculating Large Tables of Values",
    "description": "Demonstrating a way to generate a large amount of numbers that otherwise might take a long time to calculate.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-10-17",
    "categories": [
      "R",
      "pre-calulations",
      "programming",
      "development",
      "c++"
    ],
    "contents": "\n\nContents\nR\nRandom number set\nThe R way\nNaive way\nLookup table\n\nC++\nRaw calculations\n\n\nI’m currently working on a project where we want to know, based on a euclidian distance measure, what is the probability that the value is a match to the another value. i.e. given an actual value, and a theoretical value from calculation, what is the probability that they are the same? This can be calculated using a chi-square distribution with one degree-of-freedom, easily enough by considering how much of the chi-cdf we are taking up.\npMatch <- 1 - pchisq(distVal, 1)\nThe catch is, we want to do this a whole lot of times, in c++. We could use the boost library to calculate the chi-square each time we need it. Or we could generate a lookup table that is able to find the p-value simply based on the distance. This is especially attractive if we have a limit past which we consider the probability of a match as being zero, and if we use enough decimal points that we don’t suffer too much in precision.\nAlthough our goal is to implement this in c++, I also want to prototype, demonstrate and evaluate the approach in R.\nR\nRandom number set\nWe are going to consider 25 (5 standard deviations squared) as our cutoff for saying the probability is zero. So to make sure we are doing all calculations using the exact same thing, we will pre-generate the values for testing on real data, in this case a set of 1000000 random numbers from zero to 25.\n\n\nnPoint <- 1000000\nrandomData <- abs(rnorm(nPoint, mean=5, sd=5)) # take absolute so we have only positive values\nrandomData[randomData > 25] <- 25\nhist(randomData, 100)\n\n\n\n\nThe R way\nOf course, the way to calculate p-values for all this data in R is to do the whole vector at once. How long does that take?\n\n\nbTime <- Sys.time()\nactPVals <- 1 - pchisq(randomData, 1)\neTime <- Sys.time()\nrwayDiff <- difftime(eTime, bTime)\nrwayDiff\n\n\nTime difference of 0.3295307 secs\n\nNaive way\nA naive way to do this is to do it piecemeal, in a for loop. I will pre-allocate the result vector so we don’t take a hit on memory allocation.\n\n\nnaiveRes <- numeric(nPoint)\nbTime <- Sys.time()\nfor (iP in 1:nPoint){\n  naiveRes[iP] <- 1 - pchisq(randomData[iP], 1)\n}\neTime <- Sys.time()\nnaiveDiff <- difftime(eTime, bTime)\nnaiveDiff\n\n\nTime difference of 1.611617 secs\n\nSo this takes almost 10 times longer. Which of course, is why you are encouraged to do vectorized calculations whenever possible in R, but you already knew that, didn’t you?\nLookup table\nWhat if we now store a set of p-value results in a table, doing it in such a way as to use the actual distance as an index into the table?\n\n\nnDivision <- 10000\ndof <- 1\nnSD <- 25\n\nnElements <- nSD * nDivision\n\nchiVals <- seq(0, nElements, 1) / nDivision # the chi-squared values (or distances), also used as indices when multiplied by 10000\n\npTable <- 1 - pchisq(chiVals, 1) # the actual chi-square p-values for those distances\n\n\n\nTo find a value, we just multiply the distance by 10000 (the number of divisions), and add 1 (because R uses 1 based indexing instead of zero).\n\n\ntestVal <- sample(chiVals, 1) # grab a value from the chiVals previously generated\npTable[(testVal * nDivision) + 1]\n\n\n[1] 0.0001605463\n\n1 - pchisq(testVal, 1)\n\n\n[1] 0.0001605463\n\nHow long does this take compared to the other approaches?\n\n\ntableRes <- numeric(nPoint)\nbTime <- Sys.time()\nfor (iP in 1:nPoint){\n  tableRes[iP] <- pTable[(randomData[iP] * nDivision) + 1]\n}\neTime <- Sys.time()\ntableDiff <- difftime(eTime, bTime)\ntableDiff\n\n\nTime difference of 0.1302102 secs\n\nSo somewhere in-between the two. So not as good as doing a vectorized call, but better than making a call each time. Which is actually what I expected. What about any loss in precision of the values returned?\n\n\ntableRawPrecision <- abs(tableRes - actPVals) / actPVals * 100\n\nprecTable <- data.frame(org=actPVals, table=tableRes, percError=tableRawPrecision)\nggplot(precTable, aes(x=org, y=table)) + geom_point()\n\n\n\nggplot(precTable, aes(x=org, y=percError)) + geom_point()\n\n\n\n\nSo, according to this, we are only introducing error at 0.7388186%, which isn’t much. And the values look like the are well correlated, so we should be good.\nNow, how do these approaches compare when using c++?\nC++\nSo it’s a fair comparison, the code below actually writes the c++ program we are going to use, with the random numbers for the p-value calculation stored as part of the code file.\nA couple of notes:\nTo be fair, both versions of the code have the set of random numbers and the lookup table as float variables, so that there is no difference in each for memory allocation.\nNeither one stores the results of the calculation, we don’t need it for this demonstration.\nRaw calculations\n\n\ncppRaw <- c('#include <iostream>',\n               '#include <boost/math/distributions/chi_squared.hpp>',\n               'int nVal = 1000000;',\n               'double dof = 1.0;',\n               'int i;',\n               paste('float randVals[1000000] = {', paste(as.character(randomData), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               paste('float pTable[250001] = {', paste(as.character(pTable), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               'int main() {',\n               'using boost::math::chi_squared_distribution;',\n               'chi_squared_distribution<> myChi(dof);',\n               'for (i = 0; i < nVal; i++){',\n               '1 - cdf(myChi, randVals[i]);',\n               '};',\n               'return(0);',\n               '};')\ncat(cppRaw, sep=\"\\n\", file=\"cppRaw.cpp\")\n\nsystem(\"g++ cppRaw.cpp -o cppRaw.out\")\nsystem(\"time ./cppRaw.out\")\n\n\n\n\n\ncppLookup <- c('#include <iostream>',\n               '#include <boost/math/distributions/chi_squared.hpp>',\n               'int nVal = 1000000;',\n               'double dof = 1.0;',\n               'int i;',\n               paste('float randVals[1000000] = {', paste(as.character(randomData), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               paste('float pTable[250001] = {', paste(as.character(pTable), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               'int main() {',\n               'using boost::math::chi_squared_distribution;',\n               'chi_squared_distribution<> myChi(dof);',\n               'for (i = 0; i < nVal; i++){',\n               'pTable[(int(randVals[i] * nVal))];',\n               '};',\n               'return(0);',\n               '};')\ncat(cppLookup, sep=\"\\n\", file=\"cppLookup.cpp\")\nsystem(\"g++ cppLookup.cpp -o cppLookup.out\")\nsystem(\"time ./cppLookup.out\")\n\n\n\nSo bypassing boost in this case is a good thing, we get some extra speed, and reduce a dependency. We have to generate the lookup table first, but the cpp file can be generated once, with a static variable in a class that is initialized to the lookup values. We do have some error, but in our case we can live with it, as the relative rankings should still be pretty good.\n\n\n\n",
    "preview": "posts/2013-10-17-pre-calculating-large-tables-of-values/pre-calculating-large-tables-of-numbers_files/figure-html5/genRandomData-1.png",
    "last_modified": "2021-02-27T14:52:23-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2013-09-23-portable-peronal-packages/",
    "title": "Portable, Personal Packages",
    "description": "My take on creating simple little packages for your own commonly used functions.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-09-23",
    "categories": [
      "R",
      "packages",
      "development"
    ],
    "contents": "\nProgrammingR had an interesting post recently about keeping a set of R functions that are used often as a gist on Github, and sourceing that file at the beginning of R analysis scripts. There is nothing inherently wrong with this, but it does end up cluttering the user workspace, and there is no real documentation on the functions, and no good way to implement unit testing.\nHowever, the best way to have sets of R functions is as a package, that can then be installed and loaded by anyone. Normally packages are compiled and hosted on CRAN, R-forge, or Bioconductor. However, Github is becoming a common place to host packages, and thanks to Hadley Wickham’s install_github function in the devtools package, it is rather easy to install a package directly from Github. This does require that you have r-tools installed if you are using Windows (I know that can take a bit of work, but it’s not impossible), and do some extra work to create a proper package, but the overhead is probably worth it if you are using these functions all the time.\nOnce you have the package created and hosted on Github, it is simple to install it once, and load it when needed. If there is a particular version of the package that is required, it is even possible to tell install_github to install a particular version based on the commit, or a tag.\nSome examples of this type of package can be found on Github: 1 2 3\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T14:49:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-19-k-12-wants-scientists/",
    "title": "K-12 Wants Scientists!!",
    "description": "I recently attended a talk about PhD scientists in grade school teaching.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-09-19",
    "categories": [
      "post-doc",
      "academia",
      "science",
      "teaching"
    ],
    "contents": "\nIt seems that the PostDoc committee here at UK has an interest in providing some information on alternative careers for PostDocs outside of academia. We all know that there are not enough PI slots at universities for all of the PhDs who are going on to do PostDocs, and many will end up in alternative (I use that term loosely) careers.\nYesterday (2013-09-18), Scott Diamond gave a seminar to PostDocs in the Medical Center at UK on getting involved in teaching science at the K-12 and college level. Scott was previously a professor here at UK, who got involved in science teaching at local schools, saw how miserable many students were in science classes, and wanted to change that. So he quit academia, got his teaching certification, and is now teaching science at a school for high-risk youth, The Learning Center, here in Fayette County.\nThis school takes 180 of the kids who are extremely truant, i.e. they like to skip school. Of course, we know that poverty is a big factor in the decision to skip school, but so is boredom from lack of engagement with the material. At The Learning Center they have been finding that if you find ways to make the material engaging (especially science), then the kids will want to be at school, learning, and they will still do well on the standard testing. Scott mentioned that these kids in regular school were truant at least 50% of the time, and at The Learning Center they have an attendance rate of 95%.\nAs scientists, we are out to make discoveries. We look at something happening in the natural world, make hypotheses, and test them. Even in bioinformatics, the goal is to use computers to analyze biological data so we can understand better how these biological systems work, and make better hypotheses and test them. This is not how science is taught in K-12 right now here in the US. In many school systems, it is merely recitation of science facts, with no actual “investigational” science going on.\nScott (and others) are working to change this. But to do it right, they need trained scientists in the system. So if you are thinking about doing something other than academia following your PostDoc, and want to help kids be truly interested in science and how it works, maybe you should consider going into teaching K-12??\nIf you want more info about The Learning Center, about their model, or about getting into teaching at this level, you can reach Scott at scott dot diamond near uky circular shape edu.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T14:49:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/",
    "title": "R, RStudio, and Release and Dev Bioconductor",
    "description": "Working with the development version of Bioconductor on linux can be a pain. This is one way to do it.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-09-17",
    "categories": [
      "R",
      "bioconductor",
      "rstudio",
      "programming",
      "packages",
      "development"
    ],
    "contents": "\n\nContents\nSetup\nChanging Versions\nuseRDev.sh\nresetR.sh\n\nBioconductor Dev Version\n\nUpdate 2021-02-18: Now I would just use the r-docker image and the RStudio interface. I actually just did this recently to test updates to my package.\nI have one Bioconductor package that I am currently responsible for. Each bi-annual release of Bioconductor requires testing and squashing errors, warnings and bugs in a given package. Doing this means being able to work with multiple versions of R and multiple versions of Bioconductor libraries on a single system (assuming that you do production work and development on the same machine, right?).\nI really, really like RStudio as my working R environment, as some of you have read before. So how do we get RStudio on our Linux system to respect which version of R and libraries we want to use?\nSetup\nThis assumes that you have your R installs set somewhere properly, and a normal library for production level packages. You should install whichever Bioconductor packages you want into the normal library, and then make a copy of that. This copy will be your development library.\ncp -R productionLibrary developmentLibrary\nI also assume that you are using a local (i.e. sits in your home directory) .Renviron file to control where R installs the packages.\nChanging Versions\nRStudio really needs the environment variable RSTUDIO_WHICH_R set to know where R is, and R looks at R_LIBS in the .Renviron file. So I simply create two shell scripts that get sourced.\nuseRDev.sh\n#!/bin/sh\nexport RSTUDIO_WHICH_R=/pathToDevR/bin/R\necho \"R_LIBS=pathtoDevLibs\" > .Renviron\nThen I can simply do source useRDev.sh when I need to use the development R and library. Note that you will need to start RStudio from the shell for it to respect this environment variable. RStudio generally seems to install to /usr/lib/rstudio/bin/rstudio.\nresetR.sh\n#!/bin/sh\nexport RSTUDIO_WHICH_R=/pathtoReleaseR/bin/R\necho \"R_LIBS=pathtoReleaseLibs\" > .Renviron\nThis resets my variables by doint source resetR.sh.\nBioconductor Dev Version\nTo setup Bioconductor to use the develoment version, simply:\nsource useRDev.sh\nrstudio\n\n# once in RStudio\nlibrary(BiocInstaller)\nuseDev()\nbiocLite(ask=F) # this will update all the installed bioconductor packages\nI know this is not the most ideal situation, because I am rewriting over files, but it is working for me, and I thought it might help somone else.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-27T14:48:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-08-16-reproducible-methods/",
    "title": "Reproducible Methods",
    "description": "A short missive on reproducibility, especially within computational work.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-08-16",
    "categories": [
      "open-science",
      "reproducibility",
      "bioinformatics"
    ],
    "contents": "\n\nContents\nPuthanveettil et al., Synaptic Transcriptome\nGulsuner et al., Schizophrenia SNPs\nWhat to do??\n\nScience is built on the whole idea of being able to reproduce results, i.e. if I publish something, it should be possible for someone else to reproduce it, using the description of the methods used in the publication. As biological sciences have become increasingly reliant on computational methods, this has become a bigger and bigger issue, especially as the results of experiments become dependent on independently developed computational code, or use rather sophisticated computer packages that have a variety of settings that can affect output, and multiple versions. For further discussion on this issue, you might want to read (Iddo 2012) and (Brown 2012).\nI recently read a couple of different publications that really made me realize how big a problem this is. I want to spend some time showing what the problem is in these publications, and why we should be concerned about the current state of computational analytical reproducibility in life-sciences.\nIn both the articles mentioned below, I do not believe that I, or anyone not associated with the project, would be able to generate even approximately similar results based solely on the raw data and the description of methods provided. Ultimately, this is a failure of both those doing the analysis, and the reviewers who reviewed the work, and is a rather deplorable situation for a field that prides itself verification of results. This is why I’m saying these are bad bioinformatics methods sections.\nPuthanveettil et al., Synaptic Transcriptome\nPuthanveettil et al, 2013 had a paper out earlier titled “A strategy to capture and characterize the synaptic transcriptome” in PNAS. Although the primary development reported is a new method of characterizing RNA complexes that are carried by kinesin, much of the following analysis is bioinformatic in nature.\nFor example, they used BLAST searches to identify the RNA molecules, a cutoff value is reported in the results. However, functional characterization using Gene Ontology (GO) was carried out by “Bioinformatics analyses” (see the top of pg3 in the PDF). No mention of where the GO terms came from, which annotation source was used, or any software mentioned. Not in the results, discussion, or methods, or the supplemental methods. The microarray data analysis isn’t too badly described, but the 454 sequencing data processing isn’t really described at all.\nMy point is, that even given their raw data, I’m not sure I would be able to even approximate their results based on the methods reported in the methods section.\nGulsuner et al., Schizophrenia SNPs\nGulsuner et al published a paper in Cell in August 2013 titled “Spatial and Temporal Mapping of De Novo Mutations in Schizophrenia to a Fetal Prefrontal Cortical Network”. This one also looks really nice, they look for de novo mutations (i.e. new mutations in offspring not present in parents or siblings) that mess up genes that are in a heavily connected network, and also examine gene co-expression over brain development time-scales. Sounds really cool, and the results seem like they are legit, based on my reading of the manuscript. I was really impressed that they even used randomly generated networks to control the false discovery rate!\nHowever, almost all of the analysis again depends on a lot of different bioinformatic software. I do have to give the authors props, they actually give the full version of each tool used. But no mention of tool specific settings (which can generate vastly different results, see Exome Sequencing of the methods).\nThen there is this bombshell: “The predicted functional impact of each candidate de novo missense variant was assessed with in silico tools.” (near top of pg 525 of the PDF). Rrrreeeaaaalllly now. No actual quote of which tools were used, although the subsequent wording and references provided imply that they were PolyPhen2, SIFT, and the Grantham Method. But shouldn’t that have been stated up front? Along with any settings that were changed from default??\nThere is no raw data available, only their reported SNPs. Not even a list of all the SNPs that were potentially considered, so that I could at least go from those and re-run the later analysis. I have to take their word for it (although I am glad at least the SNPs they used in later analyses are reported).\nFinally, the random network generation. I’d like to be able to see that code, go over it, and see what exactly it was doing to verify it was done correctly. It likely was, based on the histograms provided, but still, these are where small errors creep in and result in invalid results.\nAs above, even if the raw data was available (didn’t see an SRA accession or any other download link), I’m not sure I could reproduce or verify the results.\nWhat to do??\nHow do we fix this problem? I think scripts and workflows used to run any type of bioinformatic analyses have to become first class research objects. And we have to teach scientists to write them and use them in a way that makes them first class research objects. So in the same way that a biologist might ask for verification of immunostaining, etc, bioinformaticians should ask that given known input, a script generates reasonable output.\nI know there has been discussion on this before, and disagreement, especially with the exploratory nature of research. However, once you’ve got something working right, you should be able to test it. Reviewers should be asking if it is testable, or the code should be available for others to test.\nI also think we as a community should do more to point out the problem. i.e. when we see it, point it out to others. I’ve done that here, but I don’t know how much should be formal. Maybe we need a new hashtag, #badbioinfomethodsection, and point links to papers that do this. Conversely, we should also point to examples when it is done right (#goodbioinfomethodsection??), and if you are bioinformatician or biologist who does a lot of coding, share your code, and at least supply it as supplemental materials. Oh, and maybe take a SoftwareCarpentry class, and look up git.\nPosted on August 16, 2013 at http://robertmflight.blogspot.com/2013/08/reproducible-methods-or-bad.html, raw markdown at https://github.com/rmflight/blogPosts/blob/master/reproducible_methods.md\n\n\n\nBrown, C Titus. 2012. “Anecdotal Science.” Living in an Ivory Basement. http://ivory.idyll.org/blog/anecdotal-science.html.\n\n\nIddo. 2012. “Can We Make Accountable Research Software?” Byte Size Biology. http://bytesizebio.net/2012/08/24/can-we-make-research-software-accountable/.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-22T21:47:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-07-11-r-interface-for-teaching/",
    "title": "R Interface for Teaching",
    "description": "What is the best interface for teaching a language like R?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-07-11",
    "categories": [
      "R",
      "teaching",
      "notebooks"
    ],
    "contents": "\n\nContents\niPython Notebook\nR Notebook\nRStudio as a Teaching Environment\nFeedback\nHard to Install\nMoving from Presentation to Coding\n\n\nKaitlin Thaney asked on Twitter last week about using Ramnath Vaidyanathan’s new interactive R notebook 1 2 for teaching.\nNow, to be clear up front, I am not trying to be mean to Ramnath, discredit his work, or the effort that went into that project. I think it is really cool, and has some rather interesting potential applications, but I don’t really think it is the right interface for teaching R. I would argue that the best interface for teaching R right now is RStudio. Keep reading to find out why.\niPython Notebook\nFirst, I believe Ramnath when he says he was inspired by the iPython Notebook that makes it so very nice to do interactive, reproducible Python coding. Software Carpentry has been very successfully using them for helping to teach Python to scientists.\nHowever, the iPython Notebook is an interesting beast for this purpose. You are able to mix markdown blocks and code blocks. In addition, it is extremely simple to break up your calculations into units of related code, and re-run those units as needed. This is particularly useful when writing new functions, because you can write the function definition, and a test that displays output in one block, and then the actual computations in subsequent blocks. It makes it very easy to keep re-running the same block of code over and over until it is correct, which allows one to interactively explore changes to functions. This is awesome for learning Python and prototyping functions.\nIn addition to being able to repeatedly write -> run -> modify in a loop, you can also insert prose describing what is going on in the form of markdown. This is a nice lightweight syntax that generates html. So it becomes relatively easy to document the why of something.\nR Notebook\nUnfortunately, the R notebook that Ramnath has put up is not quite the same beast. It is an Ace editor window coupled to an R process that knits the markdown and displays the resultant html. This is really cool, and I think will be useful in many other applications, but not for teaching in an interactive environment.\nRStudio as a Teaching Environment\nLets think. We want something that lets us repeatedly write -> run -> modify on small code blocks in R, but would be great if it was some kind of document that could be shared, and re-run.\nI would argue that the editor environment in RStudio when writing R markdown (Rmd) files is the solution. R code blocks behave much the same as in iPython notebook, in that they are colored differently, set apart, have syntax highlighting, and can be easily repeatedly run using the code chunk menu. Outside of code blocks is assumed to be markdown, making it easy to insert documentation and explanation. The code from the code blocks is sent to an attached R session, where objects can be further investigated if required, and results are displayed.\nThis infrastructure supplies an interactive back and forth between editor and execution environment, with the ability to easily group together units of code.\nIn addition, RStudio has git integration baked in, so it becomes easy to get started with some basic version control.\nFinally, RStudio is cross-platform, has tab completion among other standard IDE goodies, and its free.\nFeedback\nI’ve gotten some feedback on twitter about this, and I want to update this post to address it.\nHard to Install\nOne comment was that installing R, RStudio and necessary packages might be hard. True, it might be. However, I have done multiple installs of R, RStudio, Python, and iPython Notebook in both Linux and Windows, and I would argue that the level of difficulty is at least the same.\nMoving from Presentation to Coding\nI think this is always difficult, especially if you have a powerpoint, and your code is in another application. However, the latest dev version of RStudio (download) now includes the ability to view markdown based presentations in an attached window. This is probably one of the potentially nicest things for doing presentations that actually involve editing actual code.\nEdit: added download links for Rstudio preview\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-21T16:30:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-06-05-tim-hortons-density/",
    "title": "Tim Hortons Density",
    "description": "How far away are most Canadians from a Tim Hortons?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-06-05",
    "categories": [
      "R",
      "mapping",
      "tim-hortons"
    ],
    "contents": "\n\nContents\nSetup\nStatistics Canada Census Data\nDissemination Area Long. and Lat.\nRun queries on Foursquare\nLoad up the data and verify what we have.\nGenerate queries and run\nClean up the results\n\nVisualize Locations\nPut them on a map\nHow far??\n\nReplication\nCaveats\n\nPosted\nDisclaimer\n\nInspired by this post, I wanted to examine the locations and density of Tim Hortons restaurants in Canada. Using Stats Canada data, each census tract is queried on Foursquare for Tims locations.\nSetup\n\n\noptions(stringsAsFactors=F)\nrequire(timmysDensity)\nrequire(plyr)\nrequire(maps)\nrequire(ggplot2)\nrequire(geosphere)\n\n\n\nStatistics Canada Census Data\nThe actual Statistics Canada data at the dissemination block level can be downloaded from here. You will want to download the Excel format, read it, and then save it as either tab-delimited or CSV using a non-standard delimiter, I used a semi-colon (;).\n\n\ncensusData <- read.table(\"../timmysData/2011_92-151_XBB_XLSX.csv\", header=F, sep=\";\", quote=\"\")\ncensusData <- censusData[,1:17]\nnames(censusData) <- c(\"DBuid\", \"DBpop2011\", \"DBtdwell2011\", \"DBurdwell2011\", \"DBarea\", \"DB_ir2011\", \"DAuid\", \"DAlamx\", \"DAlamy\", \"DAlat\",\n                       \"DAlong\", \"PRuid\", \"PRname\", \"PRename\", \"PRfname\", \"PReabbr\", \"PRfabbr\")\ncensusData$DBpop2011 <- as.numeric(censusData$DBpop2011)\ncensusData$DBpop2011[is.na(censusData$DBpop2011)] <- 0\n\ncensusData$DBtdwell2011 <- as.numeric(censusData$DBtdwell2011)\ncensusData$DBtdwell2011[is.na(censusData$DBtdwell2011)] <- 0\n\n\n\nFrom this data we get block level:\npopulations (DBpop2011)\ntotal private dwellings (DBtdwell2011)\nprivale dwellings occupied by usual residents (DBurdwell2011)\nblock land area (DBarea)\ndissemination area id (DAuid)\nrepresentative point x coordinate in Lambert projection (DAlamx)\nrep. point y coordinate in Lambert projection (DAlamy)\nrep. point latitude (DAlat)\nrep. point longitude (DAlong)\nThis should be everything we need to do the investigation we want.\nDissemination Area Long. and Lat.\nWe need to find the unique dissemination areas, and get out their latitudes and longitudes for querying in other databases. Note that the longitude and latitude provided here actually are weighted representative locations based on population. However, given the size of them, I don’t think using them will be a problem for Foursquare. Because areas are what we have location data for, we will summarize everything at the area level, summing the population counts for all the blocks within an area.\n\n\nuniqAreas <- unique(censusData$DAuid)\n\nsummarizeArea <- function(areaID){\n  areaData <- censusData[(censusData$DAuid == areaID),]\n  outData <- data.frame(uid=areaID, lamx=areaData[1,\"DAlamx\"], lamy=areaData[1,\"DAlamy\"], lat=areaData[1,\"DAlat\"], long=areaData[1,\"DAlong\"], pop=sum(areaData[,\"DBpop2011\"]), dwell=sum(areaData[,\"DBtdwell2011\"]), prov=areaData[1, \"PRename\"])\n  return(outData)\n}\nareaData <- adply(uniqAreas, 1, summarizeArea)\n.sessionInfo <- sessionInfo()\n.timedate <- Sys.time()\nwrite.table(areaData, file=\"../timmysData/areaData.txt\", sep=\"\\t\", row.names=F, col.names=T)\nsave(areaData, .sessionInfo, .timedate, file=\"../timmysData/areaDataFile.RData\", compress=\"xz\")\n\n\n\nRun queries on Foursquare\nLoad up the data and verify what we have.\n\n\nload(\"../timmysData/areaDataFile.RData\")\nhead(areaData)\n\n\n\nGenerate queries and run\nFor each dissemination area (DA), we are going to use as the location for the query the latitude and longitude of each DA, as well as the search string “tim horton”.\nBecause Foursquare limits the number of userless requests to 5000 / hr. To make sure we stay under this limit, the runQueries function will only 5000 queries an hour.\n\n\nrunQueries(areaData, idFile=\"../timmysData/clientid.txt\", secretFile=\"../timmysData/clientsecret.txt\", outFile=\"../timmysData/timmysLocs2.txt\")\n\n\n\nClean up the results\nDue to the small size of the DAs, we have a lot of duplicate entries. Now lets remove all the duplicate entries.\n\n\ncleanUpResults(\"../timmysData/timmysLocs2.txt\")\n\n\n\nVisualize Locations\nFirst lets read in the data and make sure that we have Tims locations.\n\n\n# read in and clean up the data\ntimsLocs <- scan(file=\"../timmysData/timmysLocs2.txt\", what=character(), sep=\"\\n\")\ntimsLocs <- strsplit(timsLocs, \":\")\n\ntimsName <- sapply(timsLocs, function(x){x[1]})\ntimsLat <- sapply(timsLocs, function(x){x[2]})\ntimsLong <- sapply(timsLocs, function(x){x[3]})\n\nlocData <- data.frame(description=timsName, lat=as.numeric(timsLat), long=as.numeric(timsLong))\nhasNA <- is.na(locData[,\"lat\"]) | is.na(locData[,\"long\"])\nlocData <- locData[!(hasNA),]\n\ntimsStr <- c(\"tim hortons\", \"tim horton's\")\n\nhasTims <- (grepl(timsStr[1], locData$description, ignore.case=T)) | (grepl(timsStr[2], locData$description, ignore.case=T))\n\nlocData <- locData[hasTims,]\ntimsLocs <- locData\nrm(timsName, timsLat, timsLong, hasNA, locData, hasTims, timsStr)\n.timedate <- Sys.time()\n.sessionInfo <- sessionInfo()\nsave(timsLocs, .timedate, .sessionInfo, file=\"../timmysData/timsLocs.RData\", compress=\"xz\")\n\n\n\nPut them on a map\n\n\ndata(timsLocs)\ndata(areaDataFile)\ncanada <- map_data(\"world\", \"canada\")\n\np <- ggplot(legend=FALSE) +\n  geom_polygon( data=canada, aes(x=long, y=lat,group=group)) +\n  theme(panel.background = element_blank()) +\n  theme(panel.grid.major = element_blank()) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.text.x = element_blank(),axis.text.y = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  xlab(\"\") + ylab(\"\")\n\nsp <- timsLocs[1, c(\"lat\", \"long\")]\n\np2 <- p + geom_point(data=timsLocs[,c(\"lat\", \"long\")], aes(x=long, y=lat), colour=\"green\", size=1, alpha=0.5)\n\nprint(p2)\n\n\n\n\n\n\nHow far??\nAnd now lets also calculate the minimum distance of a given DA from Timmys locations.\n\n\nqueryLocs <- matrix(c(timsLocs$long, timsLocs$lat), nrow=nrow(timsLocs), ncol=2, byrow=F) # these are the tims locations\ndistLocs <- matrix(c(areaData$long, areaData$lat), nrow=nrow(areaData), ncol=2, byrow=F) # the census centers\nallDists <- apply(queryLocs, 1, function(x){\n  min(distHaversine(x, distLocs)) # only need the minimum value to determine \n})\n\n\n\nFrom the allDists variable above, we can determine that the maximum distance any census dissemination area (DA) is from a Tim Hortons is 51.5 km (31.9815 miles). This is based on distances calculated “as the crow flies”, but still, that is pretty close. Assuming roads, the furthest a Canadian should have to travel is less than an hour to get their Timmys fix.\n\n\ntotPopulation <- sum(areaData$pop, na.rm=T)\nlessDist <- seq(50, 51.6 * 1000, 50) # distances are in meters, so multiply by 1000 to get reasonable km\n\npercPop <- sapply(lessDist, function(inDist){\n  isLess <- allDists < inDist\n  sum(areaData$pop[isLess], na.rm=T) / totPopulation * 100\n})\n\nplotDistPerc <- data.frame(distance=lessDist, population=percPop, logDist=log10(lessDist))\nggplot(plotDistPerc, aes(x=logDist, y=population)) + geom_point() + xlab(\"Log10 Distance\") + ylab(\"% Population\")\n\n\n\n\n\n\nWhat gets really interesting, is how much of the population lives within a given distance of a Timmys. By summing up the percentage of the population within given distances. The plot above shows that 50% of the population is within 316.227766 meters of a Tim Hortons location.\nI guess Canadians really do like their Tim Hortons Coffee (and donuts!).\nReplication\nAll of the necessary processed data and code is available in the R package timmysDensity. You can install it using devtools. The original data files are linked in the relevant sections above.\n\n\nlibrary(devtools)\ninstall_github('timmysDensity', 'rmflight')\n\n\n\nCaveats\nI originally did this work based on a different set of data, that I have not been able to locate the original source for. I have not compared these results to that data to verify their accuracy. When I do so, I will update the package, vignette and blog post.\nPosted\nThis work exists as the vignette of timmysDensity, on my web-blog, and independently as the front page for the GitHub repo.\nDisclaimer\nTim Hortons was not involved in the creation or preparation of this work. I am not regularly updating the location information obtained from Foursquare, it is only valid for May 31, 2013. All code used in preparing these results was written by me, except in the case where code from other R packages was used. All opinions and conclusions are my own, and do not reflect the views of anyone else or any institution I may be associated with.\n\n\n\n",
    "preview": "posts/2013-06-05-tim-hortons-density/mapIt-1.png",
    "last_modified": "2021-03-13T21:03:30-05:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 960
  },
  {
    "path": "posts/2013-05-30-storing-package-data-in-custom-environments/",
    "title": "Storing Package Data in Custom Environments",
    "description": "How do you keep track of stuff for your own package without cluttering the users global space or setting a bunch of options?",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-05-30",
    "categories": [
      "R",
      "packages"
    ],
    "contents": "\nIf you do R package development, sometimes you want to be able to store variables specific to your package, without cluttering up the users workspace. One way to do this is by modifying the global options. This is done by packages grDevices and parallel. Sometimes this doesn’t seem to work quite right (see this issue for example.\nAnother way to do this is to create an environment within your package, that only package functions will be able to see, and therefore read from and modify. You get a space to put package specific stuff, the user can’t see it or modify it directly, and you just need to write functions that do the appropriate things to that environment (adding variables, reading them, etc). This sounds great in practice, but I wasn’t clear on how to do this, even after reading the help page on environments, the R documentation, or even Hadley’s excellent writeup. From all these sources, I could glean that one can create environments, name them, modify them, etc, but wasn’t sure how to work with this within a package.\nI checked out the knitcitations package to see how it was done. When I looked, I realized that it was pretty obvious in retrospect. In zzz.R, initialize the environment, assigning it to a variable. When you need to work with the variables inside, this variable will be accessible to your package, and you simply use the get and assign functions like you would if you were doing anything on the command line.\nTo make sure I had it figured out, I created a very tiny package to create a custom environment and functions for modifying it. Please feel free to examine, download, install (using devtools]) and see for yourself.\nI have at least two projects where I know I will use this, and I’m sure others might find it useful as well.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-21T16:30:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2013-05-10-writing-up-scientific-results-and-literate-programming/",
    "title": "Writing Up Scientific Results and Literate Programming",
    "description": "My thoughts on using literate programming to investigate and report scientific results",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2013-05-10",
    "categories": [
      "literate-programming",
      "academia",
      "notebooks",
      "reproducibility"
    ],
    "contents": "\n\nContents\nLiterate programming\nHow that changes writing\nSources\n\n\nAs an academic researcher, my primary purpose is to find some new insight, and subsequently communicate this insight to the general public. The process of doing this is traditionally thought to be:\nfrom observations of the world, generate a hypothesis\ndesign experiments to test hypothesis\nanalyse results of the experiments to determine if hypothesis correct\nwrite report to communicate results to others (academics and / or general public)\nAnd then repeat.\nThis is the way people envision it happening. And I would imagine that in some rare cases, this is what actually happens. However, I think many researchers would agree that this is not what normally happens. In the process of doing steps 3 and 4, your hypothesis in 1 will be modified, which modifies the experiments in 2, and so on and so forth. This makes the process of scientific discovery a very iterative process, often times right up to the report writing.\nFor some of this, it takes a long time to figure this out. I’ll never forget a professor during my PhD who suggested that you write the paper, and then figure out what experiments you should do to generate the results that would support or disprove the hypothesis you made in the paper. At the time I thought he was nuts, but when you start writing stuff, and looking at how all the steps of experiment and reporting can become intertwined, it doesn’t seem like a bad idea.\n\nI am not suggesting that one only do experiments that will support the publication, but writing out the paper at least gives you a framework for doing the experiments, and the initial analysis. One should always be willing to modify the publication / hypothesis based on what the experiments tell you.\nLiterate programming\nWhat does this have to do with literate programming? For those who don’t know, literate programming is a way to mix code and prose together in one document (in R we use knitr & sweave, python now has the iPython notebook as an option). This literate programming paradigm (combined with markdown as the markup language instead of latex thanks to knitr) is changing how I actually write my papers and do research in general.\nHow that changes writing\nAs I’ve previously described 12, RStudio makes the use of knitr and generation of literate documents using computations in R incredibly easy. Because my writing environment and programming environment are tightly coupled together, I can easily start writing what looks like a shareable, readable publication as soon as I start writing code. Couple this together with a CVS like git, and you have a way to follow the complete providence of a publication from start to finish.\nInstead of commenting my code to explain why I am doing something, I explain what I am doing in the prose, and then write the code to carry out that analysis. This changes my writing and coding style, and makes the interplay among the steps of writing scientific reports above much more explicit. I think it is a good thing, and will hopefully make my writing and research more productive.\nSources\nPublished 10.05.13 here\nThe source markdown for this document is available here\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-22T21:16:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-10-09-writing-papers-using-r-markdown/",
    "title": "Writing Papers Using R Markdown",
    "description": "How I used RMarkdown to write a manuscript",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2012-10-09",
    "categories": [
      "R",
      "open-science",
      "reproducibility",
      "literate-programming",
      "rmarkdown"
    ],
    "contents": "\n\nContents\nAdvantages\nTables and Figures\nCounting\nInserting\nMarkdown Tables\nFigures\n\nNumerical result formatting\nEcho and Include\nReferences\nHTML generation\nFunction source\nPost source\n\nI have been watching the activity in RStudio and knitr for a while, and have even been using Rmd (R markdown) files in my own work as a way to easily provide commentary on an actual dataset analysis. Yihui has proposed writing papers in markdown and posting them to a blog as a way to host a statistics journal, and lots of people are now using knitr as a way to create reproducible blog posts that include code (including yours truly).\nThe idea of writing a paper that actually includes the necessary code to perform the analysis, and is actually readable in its raw form, and that someone else could actually run was pretty appealing. Unfortunately, I had not had the time or opportunity to actually try it, until recently our group submitted a conference paper that included a lot of analysis in R that seemed like the perfect opportunity to try this. (I will link to the paper here when I hear more, or get clearance from my PI). Originally we wrote the paper in Microsoft(r) Word, but after submission I decided to see what it would have taken to write it as an Rmd document that could then generate markdown or html.\nIt turned out that it was not that hard, but it did force me to do some things differently. This is what I want to discuss here.\nAdvantages\nI actually found it much easier to have the text with the analysis (in contrast to having to be separate in a Word document), and upon doing the conversion, discovered some possible numerical errors that crept in because of having to copy numerical results separately (that is the nice thing about being able to insert variable directly into the text). In addition, the Word template for the submission didn’t play nice with automatic table and figure numbering, so our table and figure numbering got messed up in the submission. So overall, I’d say it worked out better with the Rmd file overall, even with the having to create functions to handle table and figure numbering properly myself (see below).\nTables and Figures\nAs I’m sure most of you know, Word (and other WYSIWYG editors) have ability to keep track of your object numbers, this is especially nice for keeping your figure and table numbers straight. Of course, there is no such ability built into a static text file, but I found it was easy to write a couple of functions for this. The way I came up with is to have a variable that contains a label for the figure or table, a function that increments the counter when new figures or tables are added, and a function that prints the associated number for a particular label. This does require a bit of forethought on the part of the writer, because you may have to add a table or figure label to the variable long before you actually create it, but as long as you use sane (i.e. descriptive) labels, it shouldn’t be a big deal. Let me show you what I mean.\nCounting\n\n\nincCount <- function(inObj, useName){\n  nObj <- length(inObj)\n  useNum <- max(inObj) + 1\n  inObj <- c(inObj, useNum)\n  names(inObj)[nObj+1] <- useName\n  inObj\n}\nfigCount <- c(\"_\"=0)\ntableCount <- c(\"_\"=0)\n\n\n\nThe incCount function is very simple, it takes an object, checks the maximum count, and then adds an incremental value with the supplied name. In this example, I initialized the figCount and tableCount objects with a non-sensical named value of zero.\nNow in the process of writing, I decide I’m going to need a table on the amount of time spent by post-docs writing blog posts in different years of their post-doc training. Lets call this t.blogPostDocs. Notice that this is a fairly descriptive name. We can assign it a number like so:\n\n\ntableCount <- incCount(tableCount, \"t.blogPostDocs\")\ntableCount\n\n\n             _ t.blogPostDocs \n             0              1 \n\nInserting\nSo now we have a variable with a named number we can refer to. But how do we insert it into the text? We are going to use another function that will let us insert either the text with a link, or just the text itself.\n\n\npasteLabel <- function(preText, inObj, objName, insLink=TRUE){\n  objNum <- inObj[objName]\n  \n  useText <- paste(preText, objNum, sep=\" \")\n  if (insLink){\n    useText <- paste(\"[\", useText, \"](#\", objName, \")\", sep=\"\")\n  }\n  useText\n}\n\n\n\nThis function allows us to insert the table number like so:\n\nr I(pasteLabel(\"Table\", tableCount, \"t.blogPostDocs\"))\n\nThis would be inserted into a normal inline code block. The I makes sure that the text will appear as normal text, and not get formatted as a code block. The default behavior is to insert as a relative link, thereby enabling the use of relative links to link where a table / figure is mentioned to its actual location. For example, we can insert the anchor link like so:\n<a id=\"t.blogPostDocs\"><\/a>\nMarkdown Tables\nFollowed by the actual table text. This brings up the subject of markdown tables. I also wrote a function (thanks to Yihui again) that transforms a normal R data.frame to a markdown table.\n\n\ntableCat <- function(inFrame){\n  outText <- paste(names(inFrame), collapse=\" | \")\n  outText <- c(outText, paste(rep(\"---\", ncol(inFrame)), collapse=\" | \"))\n  invisible(apply(inFrame, 1, function(inRow){\n    outText <<- c(outText, paste(inRow, collapse=\" | \"))\n  }))\n  return(outText)\n}\n\n\n\nLets see it in action.\n\n\npostDocBlogs <- data.frame(PD=c(\"p1\", \"p2\", \"p3\"), NBlog=c(4, 10, 2), Year=c(1, 4, 2))\npostDocBlogs\n\n\n  PD NBlog Year\n1 p1     4    1\n2 p2    10    4\n3 p3     2    2\n\npostDocInsert <- tableCat(postDocBlogs)\npostDocInsert\n\n\n[1] \"PD | NBlog | Year\" \"--- | --- | ---\"   \"p1 |  4 | 1\"      \n[4] \"p2 | 10 | 4\"       \"p3 |  2 | 2\"      \n\nTo actually insert it into the text, use a code chunk with results='asis' and echo=FALSE.\n\n\ncat(postDocInsert, sep=\"\\n\")\n\n\nPD\nNBlog\nYear\np1\n4\n1\np2\n10\n4\np3\n2\n2\n\nBefore inserting the table though, you might want an inline code with the table number and caption, like this:\nI(pasteLabel(\"Table\", tableCount, \"t.blogPostDocs\", FALSE)) This is the number of blog posts and year of training for post-docs.\nTable 1 This is the number of blog posts and year of training for post-docs.\nRemember for captions to set the insLink variable to FALSE so that you don’t generate a link from the caption.\nFigures\nOftentimes, you will have code that generates the figure, and then you want to insert the figure at a different point. This is accomplished by the judicious use of echo and include chunk options.\nFor example, we can create a ggplot2 figure and store it in a variable in one chunk, and then print it in a later chunk to actually insert it into the text body.\n\n\nplotData <- data.frame(x=rnorm(1000, 1, 5), y=rnorm(1000, 0, 2))\nplotKeep <- ggplot(plotData, aes(x=x, y=y)) + geom_point()\nfigCounts <- incCount(figCount, \"f.randomFigure\")\n\n\n\nAnd now we decide to actually insert it using print(plotKeep) with the option of echo=FALSE:\n\n\n\nFigure 1. A random figure.\nNumerical result formatting\nWhen R prints a number, it normally likes to do so with lots of digits. This is not probably what you want either in a table or when reporting a number in a sentence. You can control that by using the format function. When generating a new variable, the number of digits to display when printing will be saved, and when used on a variable directly, only the defined number of digits will display.\nEcho and Include\nThis brings up the issue of how to keep the code from appearing in the text body. I found depending on the particulars, either using echo=FALSE or include=FALSE would do the job. This is meant to be a paper, a reproducible one, but a paper nonetheless, and therefore the code should not end up in the text body.\nReferences\nOne thing I haven’t done yet is convert all the references. I am planning to try using the knitcitations package. I will probably post on that experience.\nHTML generation\nBecause I use RStudio, I set up a modified function For generating a full html version of the paper, changing the default RStudio markdown render options like so:\nhtmlOptions <- markdownHTMLOptions(defaults=TRUE)\nhtmlOptions <- htmlOptions[htmlOptions != \"hard_wrap\"]\nmarkdownToHTML(inputFile, outputFile, options = htmlOptions)\nThis should be added to a .Rprofile file either in your home directory or in the directory you start R in (this is especially useful for modification on a per project basis).\nI do this because when I write my documents, I want the source to be readable online. If this is a github hosted repo, that means being displayed in the github file browser, which does not do line wrapping. So I set up a 120 character line in my editor, and try very hard to stick to that.\nFunction source\nYou can find the previously mentioned functions in a github gist here.\nPost source\nThe source files for this blog post can be found at: Rmd, md, and html.\nPosted on October 9, 2012, at http://robertmflight.blogspot.com/2012/10/writing-papers-using-r-markdown.html\nEdit: added section on formatting numerical results\nEdit: added session info\n\nR version 4.0.0 (2020-04-24)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Pop!_OS 20.04 LTS\n\nMatrix products: default\nBLAS:   /software/R-4.0.0/lib/R/lib/libRblas.so\nLAPACK: /software/R-4.0.0/lib/R/lib/libRlapack.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] ggplot2_3.3.3\n\nloaded via a namespace (and not attached):\n [1] pillar_1.4.7      compiler_4.0.0    highr_0.8        \n [4] tools_4.0.0       digest_0.6.27     downlit_0.2.1    \n [7] evaluate_0.14     lifecycle_1.0.0   tibble_3.0.6     \n[10] gtable_0.3.0      pkgconfig_2.0.3   rlang_0.4.10     \n[13] DBI_1.1.1         distill_1.2       yaml_2.2.1       \n[16] xfun_0.21         withr_2.4.1       stringr_1.4.0    \n[19] dplyr_1.0.4       knitr_1.31        generics_0.1.0   \n[22] vctrs_0.3.6       grid_4.0.0        tidyselect_1.1.0 \n[25] glue_1.4.2        R6_2.5.0          fansi_0.4.2      \n[28] rmarkdown_2.6     purrr_0.3.4       farver_2.0.3     \n[31] magrittr_2.0.1    scales_1.1.1      ellipsis_0.3.1   \n[34] htmltools_0.5.1.1 assertthat_0.2.1  colorspace_2.0-0 \n[37] labeling_0.4.2    stringi_1.5.3     munsell_0.5.0    \n[40] crayon_1.4.1     \n\n\n\n\n",
    "preview": "posts/2012-10-09-writing-papers-using-r-markdown/writing-papers-using-rmarkdown_files/figure-html5/figureInsert-1.png",
    "last_modified": "2021-02-22T21:19:57-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2012-08-15-journal-club-2012-08-15/",
    "title": "Journal Club 2012-08-15",
    "description": "A summary of the paper Google Goes Cancer as was discussed in our journal club.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2012-08-15",
    "categories": [
      "journal-club"
    ],
    "contents": "\n\nContents\nPremise\nImplementation\nDid it work?\nIssues\nThings to try\n\nI just came back from our Bioinformatic group (a rather loose association of various researchers at UofL interested in and doing bioinformatics) journal club, where we discussed this recent paper:\nGoogle Goes Cancer: Improving Outcome Prediction for Cancer Patients by Network-Based Ranking of Marker Genes\nBesides the catchy title that makes one believe that perhaps Google is getting into cancer research (maybe they are and we don’t know it yet), there were some interesting aspects to this paper.\nPremise\nThe premise is that they can combine gene expression data and network data to find better associations between gene expression data and a particular disease endpoint. The way this is carried out is through the use of the TRANSFAC transcription factor - gene target database for the network, the correlation of the gene expression with the disease status as the importance of a gene with the disease, and the Google PageRank as the means to transfer the network knowledge to the gene expression data. They call their method NetRank.\nNote that the general idea had already been tried in this paper on GeneRank.\nImplementation\nRank the genes with disease status (poor or good prognosis) using a method (SAM, t-test, fold-change, correlation, NetRank). Pick n top genes, and develop a predictive model using a support vector machine. Wash, rinse, repeat several times to find the best set, varying the number of top genes, and the number of samples used in the training set.\nFor NetRank, the top genes were decided by using a sub-optimization based on varying d, the dampening factor in the PageRank algorithm that determines how much information can be transferred to other genes. The best value of d determined in this study was 0.3.\nAll other methods used just the 8000 genes that passed filtering, but NetRank used all the genes on the array, with those that were filtered out had their initial correlations set to 0, so that they were still in the network representation.\nMonte Carlo cross-validationDid it work?\nFrom the paper, it appears to have worked. Using a monte-carlo cross-validation, they were able to achieve over 70% prediction rates. And this was better than any of the other methods they used to associate genes with the disease, including SAM, t-test, fold-change, and raw correlations.\nNetRank feature selection performanceIssues\nAs we discussed the article, some questions did come up.\nWhat was the variation in d depending on the size of the training set?\nHow consistent were the genes that came out as biomarkers?\nIt would be nice to try this methodology on a series of independent, but related cancer datasets (ie breast or lung cancer) and see how consistent the lists are. This was done here.\nWhat happens if the genes that don’t pass filtering are removed from the network entirely?\nWere the problems reported with not-filtering genes due to having only two disease points (poor and good prognosis) to calculate a correlation of expression with?\nHow many iterations does it take to achieve convergence?\nThe list of genes they come up with are fairly well known cancer genes. We were kindof surprised that they didn’t seem to come up novel genes associated directly with pancreatic cancer.\nWhy is d so variable depending on the cancer examined?\nThings to try\nCould we improve on this by instead of taking just the top-ranked genes, look for the top ranked cliques, i.e. take the top gene, remove anything in its immediate neighborhood, and then go to the next one?\nWhat would happen if we used a directed network based on connected Reactome or KEGG pathways?\nFind this post online at: http://robertmflight.blogspot.com/2012/08/journal-club-150812.html\nAuthored using Markdown, and the R Markdown package. Published on 15.08.12\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-22T20:51:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "description": "Examples of messing with Affymetrix CDF data in Bioconductor.",
    "author": [
      {
        "name": "Robert M Flight",
        "url": {}
      }
    ],
    "date": "2012-07-13",
    "categories": [
      "R",
      "bioconductor",
      "bioinformatics",
      "cdf",
      "affymetrix",
      "microarray",
      "random-code-snippets"
    ],
    "contents": "\n\nContents\nWhat?\nWhy?\nWhy not somewhere else?\nHow?\nThe environment\nExample\nCustom CDF\n\n\n\n\nWhat?\nFor those who don’t know, CDF files are chip definition format files that define which probes on an Affymetrix microarray chip belong together, and are necessary to use any of the standard summarization methods such as RMA, and others.\nWhy?\nBecause we can, and because custom definitions have been shown to be quite useful. See the information over at Brainarray.\nWhy not somewhere else?\nA lot of times other people create custom CDF files based on their own criteria, and make it subsequently available for others to use (see the Brainarray for an example of what some are doing, as well as PlandbAffy)\nYou have a really nifty idea for a way to reorganize the probesets on an Affymetrix chip to perform a custom analysis, but you don’t want to go to the trouble of actually creating the CDF files and Bioconductor packages normally required to do the analysis, and yet you want to test and develop your analysis method.\nHow?\nIt turns out you are in luck. At least for AffyBatch objects in Bioconductor (created by calling ReadAffy), the CDF information is stored as an attached environment that can be easily hacked and modified to your hearts content. Environments in R are quite important and useful, and I wouldn’t have come up with this if I hadn’t been working in R for the past couple of years, but figured someone else might benefit from this knowledge.\nThe environment\nIn R, one can access an environment like so:\n\n\nget(\"objName\", envName) # get the value of object in the environment\nls(envName)\n\n\n\nWhat is also very cool, is that one can extract the objects in an environment to a list, and also create their own environment from a list using list2env. Using this methodology, we can create our own definition of probesets that can be used by standard Bioconductor routines to summarize the probes into probesets.\nA couple of disclaimers:\nI have only tried this on 3’ expression arrays\nThere might be a better way to do this, but I couldn’t find it (let me know in the comments)\nExample\n\n\nrequire(affy)\nrequire(estrogen)\nrequire(hgu95av2cdf)\n\ndatadir = system.file(\"extdata\", package=\"estrogen\")\n\npd = read.AnnotatedDataFrame(file.path(datadir, \"estrogen.txt\"), header=TRUE, sep=\"\", row.names=1)\npData(pd)\n\n\n             estrogen time.h\nlow10-1.cel    absent     10\nlow10-2.cel    absent     10\nhigh10-1.cel  present     10\nhigh10-2.cel  present     10\nlow48-1.cel    absent     48\nlow48-2.cel    absent     48\nhigh48-1.cel  present     48\nhigh48-2.cel  present     48\n\ncelDat = ReadAffy(filenames = rownames(pData(pd)), \n                  phenoData = pd,\n                  verbose=TRUE, celfile.path=datadir)\n\n\n1 reading /software/R_libs/R400/estrogen/extdata/low10-1.cel ...instantiating an AffyBatch (intensity a 409600x8 matrix)...done.\nReading in : /software/R_libs/R400/estrogen/extdata/low10-1.cel\nReading in : /software/R_libs/R400/estrogen/extdata/low10-2.cel\nReading in : /software/R_libs/R400/estrogen/extdata/high10-1.cel\nReading in : /software/R_libs/R400/estrogen/extdata/high10-2.cel\nReading in : /software/R_libs/R400/estrogen/extdata/low48-1.cel\nReading in : /software/R_libs/R400/estrogen/extdata/low48-2.cel\nReading in : /software/R_libs/R400/estrogen/extdata/high48-1.cel\nReading in : /software/R_libs/R400/estrogen/extdata/high48-2.cel\n\nThis loads up the data, reads in the raw data, and gets it ready for us to use. Now, lets see what is in the actual CDF environment.\n\n\ntopProbes <- head(ls(hgu95av2cdf)) # get a list of probesets\ntopProbes\n\n\n[1] \"100_g_at\"  \"1000_at\"   \"1001_at\"   \"1002_f_at\" \"1003_s_at\"\n[6] \"1004_at\"  \n\nexSet <- get(topProbes[1], hgu95av2cdf)\nexSet\n\n\n          pm     mm\n [1,] 175218 175858\n [2,] 356689 357329\n [3,] 227696 228336\n [4,] 237919 238559\n [5,] 275173 275813\n [6,] 203444 204084\n [7,] 357984 358624\n [8,] 368524 369164\n [9,] 285352 285992\n[10,] 304510 305150\n[11,] 159937 160577\n[12,] 223929 224569\n[13,] 282764 283404\n[14,] 270003 270643\n[15,] 303343 303983\n[16,] 389048 389688\n\nWe can see here that the first probe set 100_g_at has 16 perfect-match and mis-match probes in associated with it.\nLets summarize the original data using RMA.\n\n\nrma1 <- exprs(rma(celDat))\n\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma1)\n\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel\n100_g_at     9.642896    9.741496     9.537036     9.353625\n1000_at     10.398169   10.254362    10.003971     9.903528\n1001_at      5.717613    5.881008     5.859563     5.954028\n1002_f_at    5.512596    5.801807     5.571065     5.608132\n1003_s_at    7.783927    8.007975     8.037999     7.835120\n1004_at      7.289162    7.603670     7.488539     7.771506\n          low48-1.cel low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.591697    9.570590     9.475796     9.530655\n1000_at     10.374866   10.033520    10.345066     9.863321\n1001_at      5.960540    6.020889     5.981080     6.285192\n1002_f_at    5.390064    5.494511     5.508104     5.630107\n1003_s_at    7.926487    8.138870     7.994937     8.233338\n1004_at      7.521789    7.599544     7.456149     7.675171\n\nNow lets get the data as a list, and then create a new environment to be used for summarization.\n\n\nallSets <- ls(hgu95av2cdf)\nallSetDat <- mget(allSets, hgu95av2cdf)\n\nallSetDat[1]\n\n\n$`100_g_at`\n          pm     mm\n [1,] 175218 175858\n [2,] 356689 357329\n [3,] 227696 228336\n [4,] 237919 238559\n [5,] 275173 275813\n [6,] 203444 204084\n [7,] 357984 358624\n [8,] 368524 369164\n [9,] 285352 285992\n[10,] 304510 305150\n[11,] 159937 160577\n[12,] 223929 224569\n[13,] 282764 283404\n[14,] 270003 270643\n[15,] 303343 303983\n[16,] 389048 389688\n\nhgu2 <- list2env(allSetDat)\ncelDat@cdfName <- \"hgu2\"\n\nrma2 <- exprs(rma(celDat))\n\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma2)\n\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel\n100_g_at     9.642896    9.741496     9.537036     9.353625\n1000_at     10.398169   10.254362    10.003971     9.903528\n1001_at      5.717613    5.881008     5.859563     5.954028\n1002_f_at    5.512596    5.801807     5.571065     5.608132\n1003_s_at    7.783927    8.007975     8.037999     7.835120\n1004_at      7.289162    7.603670     7.488539     7.771506\n          low48-1.cel low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.591697    9.570590     9.475796     9.530655\n1000_at     10.374866   10.033520    10.345066     9.863321\n1001_at      5.960540    6.020889     5.981080     6.285192\n1002_f_at    5.390064    5.494511     5.508104     5.630107\n1003_s_at    7.926487    8.138870     7.994937     8.233338\n1004_at      7.521789    7.599544     7.456149     7.675171\n\nWhat about removing the MM columns? RMA only uses the PM, so it should still work.\n\n\nallSetDat <- lapply(allSetDat, function(x){\n  x[,1, drop=F]\n})\n\nallSetDat[1]\n\n\n$`100_g_at`\n          pm\n [1,] 175218\n [2,] 356689\n [3,] 227696\n [4,] 237919\n [5,] 275173\n [6,] 203444\n [7,] 357984\n [8,] 368524\n [9,] 285352\n[10,] 304510\n[11,] 159937\n[12,] 223929\n[13,] 282764\n[14,] 270003\n[15,] 303343\n[16,] 389048\n\nhgu3 <- list2env(allSetDat)\ncelDat@cdfName <- \"hgu3\"\nrma3 <-exprs(rma(celDat))\n\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma3)\n\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel\n100_g_at     9.642896    9.741496     9.537036     9.353625\n1000_at     10.398169   10.254362    10.003971     9.903528\n1001_at      5.717613    5.881008     5.859563     5.954028\n1002_f_at    5.512596    5.801807     5.571065     5.608132\n1003_s_at    7.783927    8.007975     8.037999     7.835120\n1004_at      7.289162    7.603670     7.488539     7.771506\n          low48-1.cel low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.591697    9.570590     9.475796     9.530655\n1000_at     10.374866   10.033520    10.345066     9.863321\n1001_at      5.960540    6.020889     5.981080     6.285192\n1002_f_at    5.390064    5.494511     5.508104     5.630107\n1003_s_at    7.926487    8.138870     7.994937     8.233338\n1004_at      7.521789    7.599544     7.456149     7.675171\n\nWhat if we only want to use the first 5 probesets?\n\n\nallSetDat <- allSetDat[1:5]\nhgu4 <- list2env(allSetDat)\ncelDat@cdfName <- \"hgu4\"\ncelDat\n\n\nAffyBatch object\nsize of arrays=640x640 features (21 kb)\ncdf=hgu4 (5 affyids)\nnumber of samples=8\nnumber of genes=5\nannotation=hgu95av2\nnotes=\n\nrma4 <- exprs(rma(celDat))\n\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nrma4\n\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel\n100_g_at     9.463007    9.554665     9.449050     9.401976\n1000_at     10.182753   10.009785    10.009785     9.970396\n1001_at      5.943840    6.005177     5.944015     6.089531\n1002_f_at    5.787166    5.846225     5.816964     5.814798\n1003_s_at    7.750877    7.769401     7.913021     7.864052\n          low48-1.cel low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.447562    9.457986     9.401366     9.431078\n1000_at     10.102424   10.009785    10.197065     9.889555\n1001_at      6.237329    6.147957     6.189200     6.206669\n1002_f_at    5.763175    5.763175     5.740757     5.755085\n1003_s_at    7.860778    7.917565     7.862614     7.928691\n\ndim(rma4)\n\n\n[1] 5 8\n\nCustom CDF\nTo generate our custom CDF, we are going to set our own names, and take random probes from all of the probes on the chip. The actual criteria of which probes should be together can be made using any method the author chooses.\n\n\nmaxIndx <- 640*640\n\ncustomCDF <- lapply(seq(1,100), function(x){\n  tmp <- matrix(sample(maxIndx, 20), nrow=20, ncol=1)\n  colnames(tmp) <- \"pm\"\n  return(tmp)\n})\n\nnames(customCDF) <- seq(1, 100)\n\nhgu5 <- list2env(customCDF)\ncelDat@cdfName <- \"hgu5\"\nrma5 <- exprs(rma(celDat))\n\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma5)\n\n\n    low10-1.cel low10-2.cel high10-1.cel high10-2.cel low48-1.cel\n1      6.557239    6.522249     6.639682     6.683450    6.862754\n10     7.145256    7.166927     7.124289     7.227240    7.055032\n100    6.494146    6.388879     6.349202     6.605702    6.499393\n11     6.355700    6.431657     6.108054     6.213595    6.119581\n12     5.602336    5.523976     5.370915     5.692428    5.512562\n13     6.320948    6.211828     6.338067     6.178676    6.478441\n    low48-2.cel high48-1.cel high48-2.cel\n1      7.084571     6.742868     7.289879\n10     6.908141     7.164122     7.279369\n100    6.455822     6.475354     6.514070\n11     6.455912     6.136583     6.310523\n12     5.590132     5.515823     5.544244\n13     6.280520     6.116337     6.348493\n\nI hope this information is useful to someone else. I know it made my life a lot easier.\n\n\nsessionInfo()\n\n\nR version 4.0.0 (2020-04-24)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Pop!_OS 20.04 LTS\n\nMatrix products: default\nBLAS:   /software/R-4.0.0/lib/R/lib/libRblas.so\nLAPACK: /software/R-4.0.0/lib/R/lib/libRlapack.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets \n[7] methods   base     \n\nother attached packages:\n[1] hgu95av2cdf_2.18.0  estrogen_1.34.0     affy_1.66.0        \n[4] Biobase_2.48.0      BiocGenerics_0.34.0\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.6            bslib_0.2.4           compiler_4.0.0       \n [4] BiocManager_1.30.10   jquerylib_0.1.3       tools_4.0.0          \n [7] zlibbioc_1.34.0       bit_4.0.4             digest_0.6.27        \n[10] downlit_0.2.1         memoise_2.0.0         jsonlite_1.7.2       \n[13] evaluate_0.14         RSQLite_2.2.3         preprocessCore_1.50.0\n[16] rlang_0.4.10          DBI_1.1.1             distill_1.2          \n[19] yaml_2.2.1            xfun_0.21             fastmap_1.1.0        \n[22] stringr_1.4.0         knitr_1.31            IRanges_2.22.2       \n[25] S4Vectors_0.26.1      vctrs_0.3.6           sass_0.3.1           \n[28] stats4_4.0.0          bit64_4.0.5           R6_2.5.0             \n[31] fansi_0.4.2           AnnotationDbi_1.50.3  rmarkdown_2.7        \n[34] blob_1.2.1            magrittr_2.0.1        htmltools_0.5.1.1    \n[37] stringi_1.5.3         cachem_1.0.4          affyio_1.58.0        \n\nOriginally published 2013/07/13, moved to http://rmflight.github.io on 2013/12/04.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-05T10:22:11-05:00",
    "input_file": {}
  }
]
