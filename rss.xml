<?xml version="1.0" encoding="ISO-8859-1" ?><rss version="2.0">
  <channel>
    <title>Deciphering life: One bit at a time</title>
    <link>http://rmflight.github.io</link>
    <description>Robert M Flight's personal blog</description>
    <category>R</category>
    <category>data</category>
    <category>bioinformatics</category>
    <category>open science</category>
    <item><title>PubmedCommons API</title><link>http://rmflight.github.io/posts/2013/10/pubmedComonsAPI.html</link><category>openscience</category><category>pubmedcommons</category><description>&lt;h1&gt;PubmedCommons API&lt;/h1&gt;

&lt;p&gt;The announcements are out, Pubmed is introducing a commenting system &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmedcommons/&quot;&gt;pubmedcommons&lt;/a&gt;, theoretically providing a single location for true post-publication peer review. This is a really good idea, as NCBI is likely to be around for a lot longer than a given publisher, and the requirement for all NIH funded research to be deposited into Pubmed.&lt;/p&gt;

&lt;p&gt;There are some detractors, and they may have some valid points &lt;a href=&quot;https://twitter.com/caseybergman/status/392729744640577537&quot;&gt;link&lt;/a&gt;. However, the alternative, &lt;a href=&quot;https://pubpeer.com/&quot;&gt;pubpeer&lt;/a&gt;, I had not heard about. &lt;a href=&quot;http://f1000.com/&quot;&gt;F1000&lt;/a&gt; puts the comments in one location, in a way that is no better than a single publisher site.&lt;/p&gt;

&lt;p&gt;What would truly let the pubmedcommons proliferate would be if it became the equivalent of &lt;a href=&quot;disqus.com&quot;&gt;disqus&lt;/a&gt;, but for scientific articles. For those who don&amp;#39;t know, disqus is essentially a remote commenting platform that provides commenting on other websites. For example, &lt;a href=&quot;http://rmflight.github.io&quot;&gt;my blog&lt;/a&gt; is a static site, and comments are provided by disqus. This is enabled with an account on disqus and a javascript snippet on my website.&lt;/p&gt;

&lt;p&gt;It would be neat if pubmedcommons became a similar platform for scientific articles, with a little bit more flexibility. i.e. someone would be able to include some javascript, with relevant pubmed IDs, and the comments for that article would be displayed, and others would be able to make comments on that site.&lt;/p&gt;

&lt;h3&gt;Let me illustrate with an example.&lt;/h3&gt;

&lt;p&gt;Imagine I publish a paper in PLOS One (or &lt;strong&gt;any&lt;/strong&gt; other journal, for that matter). Upon acceptance, the paper also gets a pubmed id. This instantly creates a forum for comments using pubmedcommons. In addition, PLOS One adds a little bit of javascript to their page for the paper that enables the same commenting system. Therefore, any comments made on the paper at pubmed or PLOS One appear in both places (maybe with a tag for where they originated?). In addition, I post a copy of the paper on my own blog, and add the same javascript, and readers will see the comments made at pubmed, PLOS One, and can comment on the paper on my own blog. In addition, someone else could do the same to display the comments on their own site, etc, etc. &lt;/p&gt;

&lt;p&gt;I think the above example might have the potential to revolutionize post-pub peer review, because the actual comments are hosted in a central location, but are accessible in many different locations where the publication might exist. &lt;/p&gt;

&lt;p&gt;Now, it could still go awry. They could take too long to open things up, they might make the policies regarding comments too restrictive, they may police the comments too heavily, etc, etc. But&lt;/p&gt;
</description></item>
    <item><title>Open VS Closed Analysis Languages</title><link>http://rmflight.github.io/posts/2013/10/openvsclosed.html</link><category>openscience</category><category>R</category><category>python</category><description>&lt;h1&gt;Open VS Closed Analysis Languages&lt;/h1&gt;

&lt;h2&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;I think data scientists should choose to learn &lt;strong&gt;open&lt;/strong&gt; languages such as &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt; because they are &lt;strong&gt;open&lt;/strong&gt; in the sense that anyone can obtain them, use them and modify them for free, and this has lead to large, robust groups of users, making it more likely that packages exist that you can use, and others can easily build on your own work.&lt;/p&gt;

&lt;h2&gt;Why the debate?&lt;/h2&gt;

&lt;p&gt;This was sparked by a comment on twitter suggesting that data scientists and analysts need to be &lt;em&gt;polyglots&lt;/em&gt;, that they should know more than &lt;strong&gt;one&lt;/strong&gt; programming language or analysis framework (the full conversation of tweets can be found &lt;a href=&quot;https://twitter.com/rmflight/status/387910932250517504&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;Data Scientists need to be Polyglots - know 2 or more of &lt;a href=&quot;https://twitter.com/search?q=%23python&amp;amp;src=hash&quot;&gt;#python&lt;/a&gt; &lt;a href=&quot;https://twitter.com/search?q=%23rstats&amp;amp;src=hash&quot;&gt;#rstats&lt;/a&gt; &lt;a href=&quot;https://twitter.com/search?q=%23sas&amp;amp;src=hash&quot;&gt;#sas&lt;/a&gt; &lt;a href=&quot;https://twitter.com/search?q=%23spss&amp;amp;src=hash&quot;&gt;#spss&lt;/a&gt; &lt;a href=&quot;https://twitter.com/search?q=%23matlab&amp;amp;src=hash&quot;&gt;#matlab&lt;/a&gt; &lt;a href=&quot;https://twitter.com/search?q=%23julia&amp;amp;src=hash&quot;&gt;#julia&lt;/a&gt; &lt;a href=&quot;https://twitter.com/search?q=%23octave&amp;amp;src=hash&quot;&gt;#octave&lt;/a&gt; &lt;a href=&quot;http://t.co/LtzIOzZ4XH&quot;&gt;http://t.co/LtzIOzZ4XH&lt;/a&gt;&lt;/p&gt;&amp;mdash; Gregory Piatetsky (@kdnuggets) &lt;a href=&quot;https://twitter.com/kdnuggets/statuses/387706109509005312&quot;&gt;October 8, 2013&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src=&quot;http://rmflight.github.io//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;The commenter suggested knowing at least two of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;R&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Matlab&lt;/li&gt;
&lt;li&gt;SPSS&lt;/li&gt;
&lt;li&gt;SAS&lt;/li&gt;
&lt;li&gt;Julia&lt;/li&gt;
&lt;li&gt;Octave&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My comment back was that one should really evaluate whether &lt;code&gt;SPSS&lt;/code&gt;, &lt;code&gt;SAS&lt;/code&gt; and &lt;code&gt;Matlab&lt;/code&gt; should be on this list, as they are &lt;em&gt;closed&lt;/em&gt; languages, not open, or free.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://twitter.com/kdnuggets&quot;&gt;@kdnuggets&lt;/a&gt; &lt;a href=&quot;https://twitter.com/data_nerd&quot;&gt;@data_nerd&lt;/a&gt; I would remove &lt;a href=&quot;https://twitter.com/search?q=%23matlab&amp;amp;src=hash&quot;&gt;#matlab&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/search?q=%23sas&amp;amp;src=hash&quot;&gt;#sas&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/search?q=%23spss&amp;amp;src=hash&quot;&gt;#spss&lt;/a&gt; off the list, just because they are proprietary languages&lt;/p&gt;&amp;mdash; Robert M Flight (@rmflight) &lt;a href=&quot;https://twitter.com/rmflight/statuses/387906638713081856&quot;&gt;October 9, 2013&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src=&quot;http://rmflight.github.io//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;I want to expand on &lt;em&gt;why&lt;/em&gt; I made that comment. Let me be forthright, I have not used &lt;code&gt;SPSS&lt;/code&gt;, nor &lt;code&gt;SAS&lt;/code&gt;, but I have programmed in &lt;code&gt;MatLab&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; extensively, and dabbled in &lt;code&gt;Python&lt;/code&gt;. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I also think it is a good thing for data scientists to know more than one language.&lt;/strong&gt; Just to be clear, I am &lt;strong&gt;NOT&lt;/strong&gt; arguing that point.&lt;/p&gt;

&lt;h2&gt;Closed&lt;/h2&gt;

&lt;p&gt;What is a closed analysis language? I would say that there are three types of closed languages:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;those that are not free (but still may their source code openly available)&lt;/li&gt;
&lt;li&gt;those where the underlying engine is closed source&lt;/li&gt;
&lt;li&gt;those where one cannot write their own functions to expand on what is already available&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now, &lt;code&gt;MatLab&lt;/code&gt; fits the first two categories. It is rather expensive to get a license for, the license &lt;strong&gt;can&lt;/strong&gt; be restrictive (I know they have had a lot of abuse of licenses in the past, and they are trying to avoid that), and you are not expected to poke around in the internals of the &lt;code&gt;MatLab&lt;/code&gt; engine. Oh, and if you want more than the base engine, expect to pay heavily for add-on packages.&lt;/p&gt;

&lt;p&gt;However, it is possible to write add-on&amp;#39;s for &lt;code&gt;MatLab&lt;/code&gt;. I have previously &lt;a href=&quot;http://www.mathworks.com/matlabcentral/fileexchange/authors/19294&quot;&gt;written a few&lt;/a&gt;.&lt;/p&gt;

&lt;h3&gt;Closed problem: Checking results&lt;/h3&gt;

&lt;p&gt;So why are closed languages a problem? A closed language that does not make it &lt;strong&gt;possible&lt;/strong&gt; to examine the underlying functionality of the analysis engine has two problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;surety that calculations are done correctly&lt;/li&gt;
&lt;li&gt;the ability of others to run and check results&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Both of these are issues that are really important in &lt;strong&gt;science&lt;/strong&gt;. I would consider a &lt;strong&gt;data scientist&lt;/strong&gt; to be doing actual &lt;strong&gt;science&lt;/strong&gt;, so others should be able to scrutinize their work. The best scrutiny, is for others to be able to actually run their code. If I can&amp;#39;t run your code, then how do I know what you did is right?? If I can&amp;#39;t afford a copy of &lt;code&gt;MatLab&lt;/code&gt; to run your code (assuming you made it available, you did provide the source for the analysis, right?), that is a bad thing.&lt;/p&gt;

&lt;h3&gt;Closed problem: Re-using code&lt;/h3&gt;

&lt;p&gt;Of course, the other problem is that with a closed language you have made it impossible for others to easily make use of your analysis. Sure, they could code it up in another language, but unless it is the be-all and end-all of analysis methods, I&amp;#39;m not going to bother. I don&amp;#39;t have a license for &lt;code&gt;MatLab&lt;/code&gt;, or &lt;code&gt;SPSS&lt;/code&gt;, or &lt;code&gt;SAS&lt;/code&gt;, and I can&amp;#39;t afford it; therefore I&amp;#39;m not going to use your method / code nor give you a citation or credit.&lt;/p&gt;

&lt;h2&gt;Solution: Open languages&lt;/h2&gt;

&lt;p&gt;Languages like &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;python&lt;/code&gt;, they don&amp;#39;t have these problems. If I wonder how a function in the base distribution of &lt;code&gt;R&lt;/code&gt; or &lt;code&gt;python&lt;/code&gt; works, I can go look at the source. If I find a bug, I can suggest a fix, or fix it myself and tell others about it. In addition, if I write code to do an analysis, I can make it available and know that others &lt;strong&gt;should&lt;/strong&gt; have the ability to examine it, including re-running it, in addition to using it for themselves, if it is licensed appropriately. This is the way science should work.&lt;/p&gt;

&lt;h2&gt;What should you use?&lt;/h2&gt;

&lt;p&gt;Some would argue that you should use &lt;code&gt;MatLab&lt;/code&gt;, &lt;code&gt;SAS&lt;/code&gt;, and &lt;code&gt;SPSS&lt;/code&gt; because they have been around for a while, and are the standard. I would argue that you should not use them because they are controlled by single corporate entities, who are only interested in what will get people to buy their product and use it. You should use software that &lt;a href=&quot;http://r4stats.com/articles/popularity/&quot;&gt;others are using&lt;/a&gt;, and that others will be able to use, regardless of income.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;R&lt;/code&gt; is being used in lots of different places, by lots of different people for statistics, bioinformatics, visualization, and as a general functional language. &lt;code&gt;Python&lt;/code&gt; is a great general purpose language that provides a lot of &lt;strong&gt;functional&lt;/strong&gt; glue for doing lots of different things. &lt;/p&gt;
</description></item>
    <item><title>Pre-calculating large tables of values</title><link>http://rmflight.github.io/posts/2013/10/precalcLookup.html</link><category>R</category><category>precalculate</category><category>hpc</category><description>&lt;h1&gt;Pre-calculating large tables of values&lt;/h1&gt;

&lt;p&gt;I&amp;#39;m currently working on a project where we want to know, based on a euclidian distance measure, what is the probability that the value is a match to the another value. &lt;em&gt;i.e.&lt;/em&gt; given an actual value, and a theoretical value from calculation, what is the probability that they are the same? This can be calculated using a &lt;strong&gt;chi-square&lt;/strong&gt; distribution with one degree-of-freedom, easily enough by considering how much of the chi-cdf we are taking up.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pMatch &amp;lt;- 1 - pchisq(distVal, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The catch is, we want to do this a whole lot of times, in &lt;code&gt;c++&lt;/code&gt;. We could use the &lt;code&gt;boost&lt;/code&gt; library to calculate the &lt;strong&gt;chi-square&lt;/strong&gt; each time we need it. Or we could generate a lookup table that is able to find the p-value simply based on the distance. This is especially attractive if we have a limit past which we consider the probability of a match as being zero, and if we use enough decimal points that we don&amp;#39;t suffer too much in precision.&lt;/p&gt;

&lt;p&gt;Although our goal is to implement this in &lt;strong&gt;c++&lt;/strong&gt;, I also want to prototype, demonstrate and evaluate the approach in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;R&lt;/h2&gt;

&lt;h3&gt;Random number set&lt;/h3&gt;

&lt;p&gt;We are going to consider 25 (5 standard deviations squared) as our cutoff for saying the probability is zero. So to make sure we are doing all calculations using the exact same thing, we will pre-generate the values for testing on &lt;strong&gt;real&lt;/strong&gt; data, in this case a set of 1000000 random numbers from zero to 25.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;nPoint &amp;lt;- 1e+06
randomData &amp;lt;- abs(rnorm(nPoint, mean = 5, sd = 5))  # take absolute so we have only positive values
randomData[randomData &amp;gt; 25] &amp;lt;- 25
hist(randomData, 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog//researchBlog/img/genRandomData.png&quot; alt=&quot;plot of chunk genRandomData&quot;/&gt; &lt;/p&gt;

&lt;h3&gt;The R way&lt;/h3&gt;

&lt;p&gt;Of course, the way to calculate &lt;strong&gt;p-values&lt;/strong&gt; for all this data in &lt;code&gt;R&lt;/code&gt; is to do the whole vector at once. How long does that take?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;bTime &amp;lt;- Sys.time()
actPVals &amp;lt;- 1 - pchisq(randomData, 1)
eTime &amp;lt;- Sys.time()
rwayDiff &amp;lt;- difftime(eTime, bTime)
rwayDiff
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Time difference of 0.777 secs
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Naive way&lt;/h3&gt;

&lt;p&gt;A naive way to do this is to do it piecemeal, in a for loop. I will pre-allocate the result vector so we don&amp;#39;t take a hit on memory allocation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;naiveRes &amp;lt;- numeric(nPoint)
bTime &amp;lt;- Sys.time()
for (iP in 1:nPoint) {
    naiveRes[iP] &amp;lt;- 1 - pchisq(randomData[iP], 1)
}
eTime &amp;lt;- Sys.time()
naiveDiff &amp;lt;- difftime(eTime, bTime)
naiveDiff
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Time difference of 7.921 secs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So this takes almost 10 times longer. Which of course, is why you are encouraged to do vectorized calculations whenever possible in &lt;code&gt;R&lt;/code&gt;, but you already knew that, didn&amp;#39;t you?&lt;/p&gt;

&lt;h3&gt;Lookup table&lt;/h3&gt;

&lt;p&gt;What if we now store a set of p-value results in a table, doing it in such a way as to use the actual distance as an index into the table?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;nDivision &amp;lt;- 10000
dof &amp;lt;- 1
nSD &amp;lt;- 25

nElements &amp;lt;- nSD * nDivision

chiVals &amp;lt;- seq(0, nElements, 1)/nDivision  # the chi-squared values (or distances), also used as indices when multiplied by 10000

pTable &amp;lt;- 1 - pchisq(chiVals, 1)  # the actual chi-square p-values for those distances
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To find a value, we just multiply the distance by 10000 (the number of divisions), and add 1 (because &lt;code&gt;R&lt;/code&gt; uses 1 based indexing instead of zero).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;testVal &amp;lt;- sample(chiVals, 1)  # grab a value from the chiVals previously generated
pTable[(testVal * nDivision) + 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.471e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;1 - pchisq(testVal, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 2.471e-05
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;How long does this take compared to the other approaches?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;tableRes &amp;lt;- numeric(nPoint)
bTime &amp;lt;- Sys.time()
for (iP in 1:nPoint) {
    tableRes[iP] &amp;lt;- pTable[(randomData[iP] * nDivision) + 1]
}
eTime &amp;lt;- Sys.time()
tableDiff &amp;lt;- difftime(eTime, bTime)
tableDiff
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## Time difference of 4.557 secs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So somewhere in-between the two. So not as good as doing a vectorized call, but better than making a call each time. Which is actually what I expected. What about any loss in precision of the values returned?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;tableRawPrecision &amp;lt;- abs(tableRes - actPVals)/actPVals * 100

precTable &amp;lt;- data.frame(org = actPVals, table = tableRes, percError = tableRawPrecision)
ggplot(precTable, aes(x = org, y = table)) + geom_point()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog//researchBlog/img/tablePres1.png&quot; alt=&quot;plot of chunk tablePres&quot;/&gt; &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;ggplot(precTable, aes(x = org, y = percError)) + geom_point()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog//researchBlog/img/tablePres2.png&quot; alt=&quot;plot of chunk tablePres&quot;/&gt; &lt;/p&gt;

&lt;p&gt;So, according to this, we are only introducing error at 0.782%, which isn&amp;#39;t much. And the values look like the are well correlated, so we should be good. &lt;/p&gt;

&lt;p&gt;Now, how do these approaches compare when using &lt;code&gt;c++&lt;/code&gt;?&lt;/p&gt;

&lt;h2&gt;C++&lt;/h2&gt;

&lt;p&gt;So it&amp;#39;s a fair comparison, the code below actually writes the &lt;code&gt;c++&lt;/code&gt; program we are going to use, with the random numbers for the &lt;strong&gt;p-value&lt;/strong&gt; calculation stored as part of the code file. &lt;/p&gt;

&lt;p&gt;A couple of notes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;To be fair, both versions of the code have the set of random numbers and the lookup table as &lt;code&gt;float&lt;/code&gt; variables, so that there is no difference in each for memory allocation.&lt;/li&gt;
&lt;li&gt;Neither one stores the results of the calculation, we don&amp;#39;t need it for this demonstration.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Raw calculations&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;cppRaw &amp;lt;- c(&amp;quot;#include &amp;lt;iostream&amp;gt;&amp;quot;, &amp;quot;#include &amp;lt;boost/math/distributions/chi_squared.hpp&amp;gt;&amp;quot;, 
    &amp;quot;int nVal = 1000000;&amp;quot;, &amp;quot;double dof = 1.0;&amp;quot;, &amp;quot;int i;&amp;quot;, paste(&amp;quot;float randVals[1000000] = {&amp;quot;, 
        paste(as.character(randomData), sep = &amp;quot;&amp;quot;, collapse = &amp;quot;, &amp;quot;), &amp;quot;};&amp;quot;, sep = &amp;quot;&amp;quot;, 
        collapse = &amp;quot;&amp;quot;), paste(&amp;quot;float pTable[250001] = {&amp;quot;, paste(as.character(pTable), 
        sep = &amp;quot;&amp;quot;, collapse = &amp;quot;, &amp;quot;), &amp;quot;};&amp;quot;, sep = &amp;quot;&amp;quot;, collapse = &amp;quot;&amp;quot;), &amp;quot;int main() {&amp;quot;, 
    &amp;quot;using boost::math::chi_squared_distribution;&amp;quot;, &amp;quot;chi_squared_distribution&amp;lt;&amp;gt; myChi(dof);&amp;quot;, 
    &amp;quot;for (i = 0; i &amp;lt; nVal; i++){&amp;quot;, &amp;quot;1 - cdf(myChi, randVals[i]);&amp;quot;, &amp;quot;};&amp;quot;, &amp;quot;return(0);&amp;quot;, 
    &amp;quot;};&amp;quot;)
cat(cppRaw, sep = &amp;quot;\n&amp;quot;, file = &amp;quot;cppRaw.cpp&amp;quot;)

system(&amp;quot;g++ cppRaw.cpp -o cppRaw.out&amp;quot;)
system(&amp;quot;time ./cppRaw.out&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;real  0m0.642s
user    0m0.631s
sys 0m0.001s
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;cppLookup &amp;lt;- c(&amp;quot;#include &amp;lt;iostream&amp;gt;&amp;quot;, &amp;quot;#include &amp;lt;boost/math/distributions/chi_squared.hpp&amp;gt;&amp;quot;, 
    &amp;quot;int nVal = 1000000;&amp;quot;, &amp;quot;double dof = 1.0;&amp;quot;, &amp;quot;int i;&amp;quot;, paste(&amp;quot;float randVals[1000000] = {&amp;quot;, 
        paste(as.character(randomData), sep = &amp;quot;&amp;quot;, collapse = &amp;quot;, &amp;quot;), &amp;quot;};&amp;quot;, sep = &amp;quot;&amp;quot;, 
        collapse = &amp;quot;&amp;quot;), paste(&amp;quot;float pTable[250001] = {&amp;quot;, paste(as.character(pTable), 
        sep = &amp;quot;&amp;quot;, collapse = &amp;quot;, &amp;quot;), &amp;quot;};&amp;quot;, sep = &amp;quot;&amp;quot;, collapse = &amp;quot;&amp;quot;), &amp;quot;int main() {&amp;quot;, 
    &amp;quot;using boost::math::chi_squared_distribution;&amp;quot;, &amp;quot;chi_squared_distribution&amp;lt;&amp;gt; myChi(dof);&amp;quot;, 
    &amp;quot;for (i = 0; i &amp;lt; nVal; i++){&amp;quot;, &amp;quot;pTable[(int(randVals[i] * nVal))];&amp;quot;, &amp;quot;};&amp;quot;, 
    &amp;quot;return(0);&amp;quot;, &amp;quot;};&amp;quot;)
cat(cppLookup, sep = &amp;quot;\n&amp;quot;, file = &amp;quot;cppLookup.cpp&amp;quot;)
system(&amp;quot;g++ cppLookup.cpp -o cppLookup.out&amp;quot;)
system(&amp;quot;time ./cppLookup.out&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;real  0m0.009s
user    0m0.003s
sys 0m0.001s
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So bypassing &lt;code&gt;boost&lt;/code&gt; in this case is a good thing, we get some extra speed, and reduce a dependency. We have to generate the lookup table first, but the &lt;code&gt;cpp&lt;/code&gt; file can be generated once, with a static variable in a class that is initialized to the lookup values. We do have some error, but in our case we can live with it, as the relative rankings should still be pretty good.&lt;/p&gt;
</description></item>
    <item><title>Open VS Closed</title><link>http://rmflight.github.io/posts/2013/10/openvsclosed.html</link><category>openlanguage</category><description>&lt;h1&gt;Open VS Closed&lt;/h1&gt;

&lt;p&gt;This was sparked by a comment on twitter suggesting that data scientists and analysts need to be &lt;em&gt;polyglots&lt;/em&gt;, that they should know more than &lt;strong&gt;one&lt;/strong&gt; programming language or analysis framework. The commenter suggested knowing at least two of:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;R&lt;/li&gt;
&lt;li&gt;Python&lt;/li&gt;
&lt;li&gt;Matlab&lt;/li&gt;
&lt;li&gt;SPSS&lt;/li&gt;
&lt;li&gt;SAS&lt;/li&gt;
&lt;li&gt;Julia&lt;/li&gt;
&lt;li&gt;Octave&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My comment back was that one should really evaluate whether SPSS, SAS and Matlab should be on this list, as they are &lt;em&gt;closed&lt;/em&gt; languages, not open source. I want to expand on &lt;em&gt;why&lt;/em&gt; I made that comment. Let me be forthright, I have not used SPSS, nor SAS, but I have programmed in MatLab and R extensively.&lt;/p&gt;
</description></item>
    <item><title>this is a test blog post</title><link>http://rmflight.github.io/posts/2013/09/testPost.html</link><category>testing</category><description>&lt;h1&gt;this is a test blog post&lt;/h1&gt;

&lt;p&gt;This is me seeing what happens when I make a test post&lt;/p&gt;
</description></item>
    <item><title>Portable, personal packages</title><link>http://rmflight.github.io/posts/2013/09/github_packages.html</link><category>R</category><category>packages</category><description>&lt;h1&gt;Portable, personal packages&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www.programmingr.com/&quot;&gt;ProgrammingR&lt;/a&gt; had an interesting &lt;a href=&quot;http://www.programmingr.com/content/creating-personal-portable-r-code-library-github/&quot;&gt;post&lt;/a&gt; recently about keeping a set of &lt;code&gt;R&lt;/code&gt; functions that are used often as a &lt;a href=&quot;https://gist.github.com/discover&quot;&gt;&lt;code&gt;gist&lt;/code&gt;&lt;/a&gt; on &lt;a href=&quot;https://github.com&quot;&gt;Github&lt;/a&gt;, and &lt;code&gt;source&lt;/code&gt;ing that file at the beginning of &lt;code&gt;R&lt;/code&gt; analysis scripts. There is nothing inherently wrong with this, but it does end up cluttering the user workspace, and there is no real documentation on the functions, and no good way to implement &lt;a href=&quot;http://cran.r-project.org/web/packages/testthat/index.html&quot;&gt;unit testing&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;However, the best way to have &lt;strong&gt;sets&lt;/strong&gt; of &lt;code&gt;R&lt;/code&gt; functions is as a &lt;a href=&quot;http://adv-r.had.co.nz/Package-basics.html&quot;&gt;&lt;code&gt;package&lt;/code&gt;&lt;/a&gt;, that can then be installed and loaded by anyone. Normally packages are compiled and hosted on &lt;a href=&quot;http://cran.r-project.org/&quot;&gt;&lt;code&gt;CRAN&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;http://r-forge.r-project.org/&quot;&gt;&lt;code&gt;R-forge&lt;/code&gt;&lt;/a&gt;, or &lt;a href=&quot;http://bioconductor.org&quot;&gt;&lt;code&gt;Bioconductor&lt;/code&gt;&lt;/a&gt;. However, Github is becoming a common place to host packages, and thanks to Hadley Wickham&amp;#39;s &lt;a href=&quot;http://www.rdocumentation.org/packages/devtools/functions/install_github&quot;&gt;&lt;code&gt;install_github&lt;/code&gt;&lt;/a&gt; function in the &lt;a href=&quot;http://www.rdocumentation.org/packages/devtools&quot;&gt;&lt;code&gt;devtools&lt;/code&gt; package&lt;/a&gt;, it is rather easy to install a package directly from Github. This does require that you have &lt;a href=&quot;http://cran.r-project.org/bin/windows/Rtools/&quot;&gt;r-tools&lt;/a&gt; installed if you are using Windows (I know that can take a bit of work, but it&amp;#39;s not impossible), and do some extra work to create a &lt;a href=&quot;http://adv-r.had.co.nz/Package-basics.html&quot;&gt;proper package&lt;/a&gt;, but the overhead is probably worth it if you are using these functions all the time. &lt;/p&gt;

&lt;p&gt;Once you have the package created and hosted on Github, it is simple to install it once, and load it when needed. If there is a particular version of the package that is required, it is even possible to tell &lt;code&gt;install_github&lt;/code&gt; to install a particular version based on the &lt;em&gt;commit&lt;/em&gt;, or a &lt;em&gt;tag&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Some examples of this type of package can be found on Github: &lt;a href=&quot;https://github.com/kbroman/broman&quot;&gt;1&lt;/a&gt; &lt;a href=&quot;https://github.com/childsish/lhc-R&quot;&gt;2&lt;/a&gt; &lt;a href=&quot;https://github.com/juba/r-perso&quot;&gt;3&lt;/a&gt;&lt;/p&gt;
</description></item>
    <item><title>K-12 Wants Scientists!!</title><link>http://rmflight.github.io/posts/2013/09/postdocAlternatives.html</link><category>postdoc</category><category>academia</category><category>science</category><description>&lt;h1&gt;K-12 Wants Scientists!!&lt;/h1&gt;

&lt;p&gt;It seems that the PostDoc committee here at UK has an interest in providing some information on alternative careers for PostDocs outside of academia. We all know that there are not enough PI slots at universities for all of the PhDs who are going on to do PostDocs, and many will end up in &lt;em&gt;alternative&lt;/em&gt; (I use that term loosely) careers.&lt;/p&gt;

&lt;p&gt;Yesterday (2013-09-18), Scott Diamond gave a &lt;em&gt;seminar&lt;/em&gt; to PostDocs in the Medical Center at UK on getting involved in teaching science at the K-12 and college level. Scott was previously a professor here at UK, who got involved in science teaching at local schools, saw how miserable many students were in science classes, and wanted to change that. So he quit academia, got his teaching certification, and is now teaching science at a school for high-risk youth, &lt;a href=&quot;http://www.tlc.fcps.net/&quot;&gt;The Learning Center&lt;/a&gt;, here in Fayette County. &lt;/p&gt;

&lt;p&gt;This school takes 180 of the kids who are extremely truant, i.e. they like to skip school. Of course, we know that poverty is a big factor in the decision to skip school, but so is boredom from lack of engagement with the material. At The Learning Center they have been finding that if you find ways to make the material engaging (especially science), then the kids will &lt;em&gt;want&lt;/em&gt; to be at school, learning, and they will still do well on the standard testing. Scott mentioned that these kids in regular school were truant at least 50% of the time, and at The Learning Center they have an attendance rate of 95%.&lt;/p&gt;

&lt;p&gt;As scientists, we are out to make discoveries. We look at something happening in the natural world, make hypotheses, and test them. Even in bioinformatics, the goal is to use computers to analyze biological data so we can understand better how these biological systems work, and make better hypotheses and test them. This is not how science is taught in K-12 right now here in the US. In many school systems, it is merely recitation of science facts, with no actual &amp;ldquo;investigational&amp;rdquo; science going on. &lt;/p&gt;

&lt;p&gt;Scott (and others) are working to change this. But to do it right, they need trained scientists in the system. So if you are thinking about doing something other than academia following your PostDoc, and want to help kids be truly interested in science and how it works, maybe you should consider going into teaching K-12??&lt;/p&gt;

&lt;p&gt;If you want more info about The Learning Center, about their model, or about getting into teaching at this level, you can reach Scott at &lt;code&gt;scott&lt;/code&gt; dot &lt;code&gt;diamond&lt;/code&gt; near &lt;code&gt;uky&lt;/code&gt; circular shape &lt;code&gt;edu&lt;/code&gt;.&lt;/p&gt;
</description></item>
    <item><title>R, RStudio, and release and dev Bioconductor</title><link>http://rmflight.github.io/posts/2013/09/bioconductorDevLibs.html</link><category>R</category><category>bioconductor</category><category>rstudio</category><description>&lt;h1&gt;R, RStudio, and release and dev Bioconductor&lt;/h1&gt;

&lt;p&gt;I have one &lt;a href=&quot;http://bioconductor.org/packages/release/bioc/html/categoryCompare.html&quot;&gt;&lt;code&gt;Bioconductor&lt;/code&gt; package&lt;/a&gt; that I am currently responsible for. Each &lt;a href=&quot;http://master.bioconductor.org/developers/release-schedule/&quot;&gt;bi-annual release&lt;/a&gt; of &lt;code&gt;Bioconductor&lt;/code&gt; requires testing and squashing errors, warnings and bugs in a given package. Doing this means being able to work with multiple versions of &lt;code&gt;R&lt;/code&gt; and multiple versions of &lt;code&gt;Bioconductor&lt;/code&gt; libraries on a single system (assuming that you do production work and development on the same machine, right?).&lt;/p&gt;

&lt;p&gt;I really, really like &lt;a href=&quot;http://rstudio.org&quot;&gt;&lt;code&gt;RStudio&lt;/code&gt;&lt;/a&gt; as my working &lt;code&gt;R&lt;/code&gt; environment, as some of you have read before. So how do we get &lt;code&gt;RStudio&lt;/code&gt; on our Linux system to respect which version of &lt;code&gt;R&lt;/code&gt; and libraries we want to use?&lt;/p&gt;

&lt;h2&gt;Setup&lt;/h2&gt;

&lt;p&gt;This assumes that you have your &lt;code&gt;R&lt;/code&gt; installs set somewhere properly, and a &lt;strong&gt;normal&lt;/strong&gt; library for production level packages. You should install whichever &lt;code&gt;Bioconductor&lt;/code&gt; packages you want into the &lt;strong&gt;normal&lt;/strong&gt; library, and then make a copy of that. This copy will be your &lt;strong&gt;development&lt;/strong&gt; library.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp -R productionLibrary developmentLibrary
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I also assume that you are using a local (i.e. sits in your &lt;em&gt;home&lt;/em&gt; directory) &lt;code&gt;.Renviron&lt;/code&gt; file to control where &lt;code&gt;R&lt;/code&gt; installs the packages.&lt;/p&gt;

&lt;h2&gt;Changing Versions&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;RStudio&lt;/code&gt; really needs the environment variable &lt;code&gt;RSTUDIO_WHICH_R&lt;/code&gt; set to know where &lt;code&gt;R&lt;/code&gt; is, and &lt;code&gt;R&lt;/code&gt; looks at &lt;code&gt;R_LIBS&lt;/code&gt; in the &lt;code&gt;.Renviron&lt;/code&gt; file. So I simply create two shell scripts that get sourced.&lt;/p&gt;

&lt;h3&gt;useRDev.sh&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
export RSTUDIO_WHICH_R=/pathToDevR/bin/R
echo &amp;quot;R_LIBS=pathtoDevLibs&amp;quot; &amp;gt; .Renviron
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then I can simply do &lt;code&gt;source useRDev.sh&lt;/code&gt; when I need to use the development &lt;code&gt;R&lt;/code&gt; and library. &lt;strong&gt;Note&lt;/strong&gt; that you will need to start &lt;code&gt;RStudio&lt;/code&gt; from the shell for it to respect this environment variable. &lt;code&gt;RStudio&lt;/code&gt; generally seems to install to &lt;code&gt;/usr/lib/rstudio/bin/rstudio&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;resetR.sh&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/sh
export RSTUDIO_WHICH_R=/pathtoReleaseR/bin/R
echo &amp;quot;R_LIBS=pathtoReleaseLibs&amp;quot; &amp;gt; .Renviron
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This resets my variables by doint &lt;code&gt;source resetR.sh&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Bioconductor Dev Version&lt;/h2&gt;

&lt;p&gt;To setup &lt;code&gt;Bioconductor&lt;/code&gt; to use the develoment version, simply:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;source useRDev.sh
rstudio

# once in RStudio
library(BiocInstaller)
useDev()
biocLite(ask=F) # this will update all the installed bioconductor packages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I know this is not the most ideal situation, because I am rewriting over files, but it is working for me, and I thought it might help somone else.&lt;/p&gt;
</description></item>
    <item><title>Difference in posted date vs sessionInfo()</title><link>http://rmflight.github.io/posts/2013/09/movingDates.html</link><category>moving</category><description>&lt;h1&gt;Difference in posted date vs sessionInfo()&lt;/h1&gt;

&lt;p&gt;If you are a newcomer to my weblog, you may notice that some posts that are &lt;code&gt;R&lt;/code&gt; tutorials generally include the output of &lt;code&gt;Sys.time()&lt;/code&gt; at the end. If you look closeley at that time and the &lt;strong&gt;Posted on&lt;/strong&gt; date, you may notice that some posts show disagreement between them. This is because I decided to move &lt;em&gt;all&lt;/em&gt; of my old blog posts from &lt;em&gt;blogspot&lt;/em&gt; to here, and keep the original posted dates.&lt;/p&gt;
</description></item>
    <item><title>Adding things to the site</title><link>http://rmflight.github.io/posts/2013/09/adding_things.html</link><category>Samatha</category><category>twitter</category><description>&lt;h1&gt;Adding things to the site&lt;/h1&gt;

&lt;p&gt;In addition to working with David on &lt;a href=&quot;https://github.com/DASpringate/samatha&quot;&gt;&lt;code&gt;samatha&lt;/code&gt;&lt;/a&gt; and testing, I took some time today to add my &lt;a href=&quot;https://twitter.com/rmflight&quot;&gt;twitter&lt;/a&gt; feed and &lt;a href=&quot;http://disqus.com&quot;&gt;disqus&lt;/a&gt; comments. Those were the major elements I wanted to add to the blog. If you visit often, I would expect to see changes for the next little while as I get stuff worked out. Note that I don&amp;#39;t think I have RSS figured out yet, will work on that sometime soon.&lt;/p&gt;
</description></item>
    <item><title>Moved!</title><link>http://rmflight.github.io/posts/2013/09/Moved.html</link><category>moving</category><description>&lt;h1&gt;Moved!&lt;/h1&gt;

&lt;p&gt;OK, if you can see this, then it is official, my blog has moved to static hosting on github pages. We&amp;#39;ll see how this works out!&lt;/p&gt;
</description></item>
    <item><title>Test post</title><link>http://rmflight.github.io/posts/2013/09/Test_post.html</link><category>R</category><category>setup</category><category>Samatha</category><description>&lt;h1&gt;Test post&lt;/h1&gt;

&lt;p&gt;Here is a summary of the &lt;code&gt;cars&lt;/code&gt; dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;summary(cars)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##      speed           dist    
##  Min.   : 4.0   Min.   :  2  
##  1st Qu.:12.0   1st Qu.: 26  
##  Median :15.0   Median : 36  
##  Mean   :15.4   Mean   : 43  
##  3rd Qu.:19.0   3rd Qu.: 56  
##  Max.   :25.0   Max.   :120
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And here is a graph:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;plot(cars)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog//researchBlog/img/unnamed-chunk-2.png&quot; alt=&quot;plot of chunk unnamed-chunk-2&quot;/&gt; &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;library(ggplot2)
ggplot(cars, aes(x = speed, y = dist)) + geom_point()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog//researchBlog/img/gCars.png&quot; alt=&quot;plot of chunk gCars&quot;/&gt; &lt;/p&gt;

&lt;p&gt;Adding some text to test something.&lt;/p&gt;
</description></item>
    <item><title>Using Samatha</title><link>http://rmflight.github.io/posts/2013/09/Using_samatha.html</link><category>Samatha</category><category>R</category><category>tutorial</category><description>&lt;h1&gt;Using Samatha&lt;/h1&gt;

&lt;p&gt;So I decided to try out David Springates Samatha package for statically building this blog. If you decide to use it, be warned, right now it is a little rough around the edges, and needs some work. David created his own blog using it, and has been using that as a test case. Unfortunately, that has meant that there are bugs that haven&amp;#39;t been caught. So we&amp;#39;ll hopefully be finding bugs and squashing them.&lt;/p&gt;

&lt;h2&gt;Installing &amp;amp; Using&lt;/h2&gt;

&lt;p&gt;Right now (Sept 6, 2013) the easiest way to use it is by cloning the repo, and then using &lt;code&gt;devtools&lt;/code&gt; to load the functions:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Outside of &lt;code&gt;R&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/DASpringate/samatha.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;in &lt;code&gt;R&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;# navigate to samatha directory
library(devtools)
load_all(&amp;quot;.&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Setting Up&lt;/h2&gt;

&lt;p&gt;To set up a site, David has helpfully created two functions, &lt;code&gt;skeleton&lt;/code&gt; and &lt;code&gt;setup_example_site&lt;/code&gt; to essentially instantiate a fully functional site with a first post, etc.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;site &amp;lt;- &amp;quot;dirToHoldBlog&amp;quot;
skeleton(site)
setup_example_site(site)

samatha(site, rss = FALSE, initial = T)
samatha(site, rss = FALSE, initial = T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is nice, because some of the way the site works were not quite intuitive to me, and it wasn&amp;#39;t clear from his own setup where everything went. So starting from this site, it gets a little clearer how everything works, and then subsequently start modifying things.&lt;/p&gt;

&lt;h2&gt;Posts&lt;/h2&gt;

&lt;p&gt;Posts are kept in the &lt;code&gt;template/posts&lt;/code&gt; directory.&lt;/p&gt;

&lt;h2&gt;Changes&lt;/h2&gt;

&lt;p&gt;The first things you will definitely want to change are in the &lt;code&gt;template/layouts/default_template.R&lt;/code&gt; to reflect the title of your actual blog, and the links.&lt;/p&gt;

&lt;h2&gt;Themes&lt;/h2&gt;

&lt;p&gt;Haven&amp;#39;t quite figured this out yet. Will update when I do.&lt;/p&gt;

&lt;h2&gt;Bugs&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;I haven&amp;#39;t got the &lt;code&gt;RSS&lt;/code&gt; properly working yet, so I don&amp;#39;t know if that is really a bug, but I get an error if I don&amp;#39;t call &lt;code&gt;samatha&lt;/code&gt; with &lt;code&gt;rss=FALSE&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The index page doesn&amp;#39;t update properly when you just have a new post, or change the name of a post. My current workaround is to change something minor on the index page when I want the new post to show up.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description></item>
    <item><title>Reproducible Methods</title><link>http://rmflight.github.io/posts/2013/08/reproducible_methods.html</link><category>openscience</category><category>reproducibility</category><description>&lt;h1&gt;Reproducible Methods&lt;/h1&gt;

&lt;p&gt;Science is built on the whole idea of being able to reproduce results, i.e. if I publish something, it should be possible for someone else to reproduce it, using the description of the methods used in the publication. As biological sciences have become increasingly reliant on computational methods, this has become a bigger and bigger issue, especially as the results of experiments become dependent on independently developed computational code, or use rather sophisticated computer packages that have a variety of settings that can affect output, and multiple versions. For further discussion on this issue, you might want to read &lt;a href=&quot;http://bytesizebio.net/index.php/2012/08/24/can-we-make-research-software-accountable/&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;http://ivory.idyll.org/blog/anecdotal-science.html&quot;&gt;2&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I recently read a couple of different publications that really made me realize how big a problem this is. I want to spend some time showing what the problem is in these publications, and why we should be concerned about the current state of computational analytical reproducibility in life-sciences.&lt;/p&gt;

&lt;p&gt;In both the articles mentioned below, I do not believe that I, or anyone not associated with the project, would be able to generate even approximately similar results based solely on the raw data and the description of methods provided. Ultimately, this is a failure of both those doing the analysis, and the reviewers who reviewed the work, and is a rather deplorable situation for a field that prides itself verification of results. This is why I&amp;#39;m saying these are &lt;strong&gt;bad bioinformatics methods sections&lt;/strong&gt;.&lt;/p&gt;

&lt;h2&gt;Puthanveettil et al., Synaptic Transcriptome&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://doi.org/10.1073/pnas.1304422110&quot;&gt;Puthanveettil et al, 2013&lt;/a&gt; had a paper out earlier titled &lt;a href=&quot;http://doi.org/10.1073/pnas.1304422110&quot;&gt;&amp;ldquo;A strategy to capture and characterize the synaptic transcriptome&amp;rdquo;&lt;/a&gt; in PNAS. Although the primary development reported is a new method of characterizing RNA complexes that are carried by kinesin, much of the following analysis is bioinformatic in nature.&lt;/p&gt;

&lt;p&gt;For example, they used BLAST searches to identify the RNA molecules, a cutoff value is reported in the results. However, functional characterization using Gene Ontology (GO) was carried out by &amp;ldquo;Bioinformatics analyses&amp;rdquo; (see the top of pg3 in the PDF). No mention of where the GO terms came from, which annotation source was used, or any software mentioned. Not in the results, discussion, or methods, or the supplemental methods. The microarray data analysis isn&amp;#39;t too badly described, but the 454 sequencing data processing isn&amp;#39;t really described at all.&lt;/p&gt;

&lt;p&gt;My point is, that even given their raw data, I&amp;#39;m not sure I would be able to even approximate their results based on the methods reported in the methods section.&lt;/p&gt;

&lt;h2&gt;Gulsuner et al., Schizophrenia SNPs&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://dx.doi.org/10.1016/j.cell.2013.06.049&quot;&gt;Gulsuner et al&lt;/a&gt; published a paper in Cell in August 2013 titled &lt;a href=&quot;http://dx.doi.org/10.1016/j.cell.2013.06.049&quot;&gt;&amp;ldquo;Spatial and Temporal Mapping of De Novo Mutations in Schizophrenia to a Fetal Prefrontal Cortical Network&amp;rdquo;&lt;/a&gt;. This one also looks really nice, they look for &lt;em&gt;de novo&lt;/em&gt; mutations (i.e. new mutations in offspring not present in parents or siblings) that mess up genes that are in a heavily connected network, and also examine gene co-expression over brain development time-scales. Sounds really cool, and the results seem like they are legit, based on my reading of the manuscript. I was really impressed that they even used &lt;strong&gt;randomly generated&lt;/strong&gt; networks to control the false discovery rate!&lt;/p&gt;

&lt;p&gt;However, almost all of the analysis again depends on a lot of different bioinformatic software. I do have to give the authors props, they actually give the &lt;strong&gt;full&lt;/strong&gt; version of each tool used. But no mention of tool specific settings (which can generate vastly different results, see &lt;strong&gt;Exome Sequencing&lt;/strong&gt; of the methods).&lt;/p&gt;

&lt;p&gt;Then there is this bombshell: &amp;ldquo;The predicted functional impact of each candidate de novo missense variant was assessed with in silico tools.&amp;rdquo; (near top of pg 525 of the PDF). Rrrreeeaaaalllly now. No actual quote of which tools were used, although the subsequent wording and references provided imply that they were &lt;a href=&quot;http://www.nature.com/nmeth/journal/v7/n4/full/nmeth0410-248.html&quot;&gt;PolyPhen2&lt;/a&gt;, &lt;a href=&quot;http://sift.jcvi.org/&quot;&gt;SIFT&lt;/a&gt;, and the &lt;a href=&quot;http://www.sciencemag.org/content/185/4154/862.long&quot;&gt;Grantham Method&lt;/a&gt;. But shouldn&amp;#39;t that have been stated up front? Along with any settings that were changed from default??&lt;/p&gt;

&lt;p&gt;There is no raw data available, only their reported SNPs. Not even a list of &lt;strong&gt;all&lt;/strong&gt; the SNPs that were potentially considered, so that I could at least go from those and re-run the later analysis. I have to take their word for it (although I am glad at least the SNPs they used in later analyses are reported). &lt;/p&gt;

&lt;p&gt;Finally, the random network generation. I&amp;#39;d like to be able to see that code, go over it, and see what exactly it was doing to verify it was done correctly. It likely was, based on the histograms provided, but still, these are where small errors creep in and result in invalid results.&lt;/p&gt;

&lt;p&gt;As above, even if the raw data was available (didn&amp;#39;t see an SRA accession or any other download link), I&amp;#39;m not sure I could reproduce or verify the results.&lt;/p&gt;

&lt;h2&gt;What to do??&lt;/h2&gt;

&lt;p&gt;How do we fix this problem? I think scripts and workflows used to run any type of bioinformatic analyses have to become first class research objects. And we have to teach scientists to write them and use them in a way that makes them first class research objects. So in the same way that a biologist might ask for verification of immunostaining, etc, bioinformaticians should ask that given known input, a script generates &lt;em&gt;reasonable&lt;/em&gt; output. &lt;/p&gt;

&lt;p&gt;I know there has been discussion on this before, and disagreement, especially with the exploratory nature of research. However, once you&amp;#39;ve got something working &lt;em&gt;right&lt;/em&gt;, you should be able to &lt;em&gt;test&lt;/em&gt; it. Reviewers should be asking if it is testable, or the code should be available for others to test.&lt;/p&gt;

&lt;p&gt;I also think we as a community should do more to point out the problem. i.e. when we see it, point it out to others. I&amp;#39;ve done that here, but I don&amp;#39;t know how much should be formal. Maybe we need a new hashtag, #badbioinfomethodsection, and point links to papers that do this. Conversely, we should also point to examples when it is done right (#goodbioinfomethodsection??), and if you are bioinformatician or biologist who does a lot of coding, share your code, and at least supply it as supplemental materials. Oh, and maybe take a &lt;a href=&quot;http://softwarecarpentry.org&quot;&gt;SoftwareCarpentry&lt;/a&gt; class, and look up &lt;a href=&quot;http://git-scm.com/&quot;&gt;git&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Posted on August 16, 2013 at &lt;a href=&quot;http://robertmflight.blogspot.com/2013/08/reproducible-methods-or-bad.html&quot;&gt;http://robertmflight.blogspot.com/2013/08/reproducible-methods-or-bad.html&lt;/a&gt;, raw markdown at &lt;a href=&quot;https://github.com/rmflight/blogPosts/blob/master/reproducible_methods.md&quot;&gt;https://github.com/rmflight/blogPosts/blob/master/reproducible_methods.md&lt;/a&gt;&lt;/p&gt;
</description></item>
    <item><title>R Interface for Teaching</title><link>http://rmflight.github.io/posts/2013/07/rTeachingApp.html</link><category>R</category><category>teaching</category><category>notebook</category><description>&lt;h1&gt;R Interface for Teaching&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/intent/user?screen_name=kaythaney&amp;amp;tw_i=354635159447941120&amp;amp;tw_p=tweetembed&quot;&gt;Kaitlin Thaney&lt;/a&gt; asked on Twitter last week about using &lt;a href=&quot;https://twitter.com/intent/user?screen_name=ramnath_vaidya&amp;amp;tw_i=354599459868508160&amp;amp;tw_p=tweetembed&quot;&gt;Ramnath Vaidyanathan&amp;#39;s&lt;/a&gt; new &lt;code&gt;interactive R notebook&lt;/code&gt; &lt;a href=&quot;http://ramnathv.github.io/rNotebook/&quot;&gt;1&lt;/a&gt; &lt;a href=&quot;https://github.com/ramnathv/rNotebook&quot;&gt;2&lt;/a&gt; for teaching.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p&gt;Liking the look of interactive R notebook by &lt;a href=&quot;https://twitter.com/ramnath_vaidya&quot;&gt;@ramnath_vaidya&lt;/a&gt;. Any success stories in using to teach? &lt;a href=&quot;http://t.co/wmVuFM2Rst&quot;&gt;http://t.co/wmVuFM2Rst&lt;/a&gt; (HT &lt;a href=&quot;https://twitter.com/_inundata&quot;&gt;@_inundata&lt;/a&gt;)&lt;/p&gt;&amp;mdash; Kaitlin Thaney (@kaythaney) &lt;a href=&quot;https://twitter.com/kaythaney/statuses/354635159447941120&quot;&gt;July 9, 2013&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src=&quot;http://rmflight.github.io//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;Now, to be clear up front, I am &lt;strong&gt;not&lt;/strong&gt; trying to be mean to Ramnath, discredit his work, or the effort that went into that project. I think it is really cool, and has some rather interesting potential applications, but I don&amp;#39;t really think it is the right interface for teaching &lt;code&gt;R&lt;/code&gt;. I would argue that the best interface for teaching &lt;code&gt;R&lt;/code&gt; right now is &lt;a href=&quot;http://www.rstudio.com/&quot;&gt;RStudio&lt;/a&gt;. Keep reading to find out why.&lt;/p&gt;

&lt;h2&gt;iPython Notebook&lt;/h2&gt;

&lt;p&gt;First, I believe Ramnath when he says he was inspired by the &lt;a href=&quot;http://ipython.org/notebook.html&quot;&gt;&lt;code&gt;iPython Notebook&lt;/code&gt;&lt;/a&gt; that makes it so very nice to do interactive, reproducible Python coding. Software Carpentry has been very successfully using them for helping to teach Python to scientists.&lt;/p&gt;

&lt;p&gt;However, the iPython Notebook is an interesting beast for this purpose. You are able to mix &lt;code&gt;markdown&lt;/code&gt; blocks and &lt;code&gt;code&lt;/code&gt; blocks. In addition, it is extremely simple to break up your calculations into &lt;strong&gt;units&lt;/strong&gt; of related code, and re-run those units as needed. This is particularly useful when writing new functions, because you can write the function definition, and a test that displays output in one block, and then the actual computations in subsequent blocks. It makes it very easy to keep re-running the same block of code over and over until it is correct, which allows one to interactively explore changes to functions. This is &lt;strong&gt;awesome&lt;/strong&gt; for learning Python and prototyping functions.&lt;/p&gt;

&lt;p&gt;In addition to being able to repeatedly &lt;code&gt;write -&amp;gt; run -&amp;gt; modify&lt;/code&gt; in a loop, you can also insert prose describing what is going on in the form of &lt;code&gt;markdown&lt;/code&gt;. This is a nice lightweight syntax that generates html. So it becomes relatively easy to document the &lt;em&gt;why&lt;/em&gt; of something.&lt;/p&gt;

&lt;h2&gt;R Notebook&lt;/h2&gt;

&lt;p&gt;Unfortunately, the &lt;code&gt;R notebook&lt;/code&gt; that Ramnath has put up is not quite the same beast. It is an &lt;a href=&quot;http://ajaxorg.github.io/ace/#nav=about&quot;&gt;Ace editor&lt;/a&gt; window coupled to an R process that knits the markdown and displays the resultant html. This is really cool, and I think will be useful in many other applications, but &lt;strong&gt;not&lt;/strong&gt; for teaching in an interactive environment. &lt;/p&gt;

&lt;h2&gt;RStudio as a Teaching Environment&lt;/h2&gt;

&lt;p&gt;Lets think. We want something that lets us repeatedly &lt;code&gt;write -&amp;gt; run -&amp;gt; modify&lt;/code&gt; &lt;strong&gt;on small code blocks&lt;/strong&gt; in &lt;code&gt;R&lt;/code&gt;, but would be great if it was some kind of document that could be shared, and re-run.&lt;/p&gt;

&lt;p&gt;I would argue that the editor environment in &lt;a href=&quot;http://www.rstudio.com/&quot;&gt;RStudio&lt;/a&gt; when writing &lt;a href=&quot;http://www.rstudio.com/ide/docs/authoring/using_markdown&quot;&gt;R markdown (Rmd)&lt;/a&gt; files is the solution. &lt;code&gt;R&lt;/code&gt; code blocks behave much the same as in iPython notebook, in that they are colored differently, set apart, have syntax highlighting, and can be easily repeatedly run using the &lt;code&gt;code chunk&lt;/code&gt; menu. Outside of code blocks is assumed to be markdown, making it easy to insert documentation and explanation. The code from the code blocks is sent to an attached &lt;code&gt;R&lt;/code&gt; session, where objects can be further investigated if required, and results are displayed.&lt;/p&gt;

&lt;p&gt;This infrastructure supplies an interactive back and forth between editor and execution environment, with the ability to easily group together units of code.&lt;/p&gt;

&lt;p&gt;In addition, RStudio has git integration baked in, so it becomes easy to get started with some basic version control.&lt;/p&gt;

&lt;p&gt;Finally, RStudio is cross-platform, has tab completion among other standard IDE goodies, and its free.&lt;/p&gt;

&lt;h2&gt;Feedback&lt;/h2&gt;

&lt;p&gt;I&amp;#39;ve gotten some feedback on twitter about this, and I want to update this post to address it.&lt;/p&gt;

&lt;h3&gt;Hard to Install&lt;/h3&gt;

&lt;p&gt;One comment was that installing R, RStudio and necessary packages might be hard. True, it might be. However, I have done multiple installs of R, RStudio, Python, and iPython Notebook in both Linux and Windows, and I would argue that the level of difficulty is at least the same.&lt;/p&gt;

&lt;h3&gt;Moving from Presentation to Coding&lt;/h3&gt;

&lt;p&gt;I think this is always difficult, especially if you have a powerpoint, and your code is in another application. However, the latest dev version of RStudio (&lt;a href=&quot;http://www.rstudio.com/ide/download/preview&quot;&gt;download&lt;/a&gt;) now includes the ability to view markdown based presentations in an &lt;a href=&quot;http://www.rstudio.com/ide/docs/presentations/overview&quot;&gt;attached window&lt;/a&gt;. This is probably one of the potentially nicest things for doing presentations that actually involve editing actual code.&lt;/p&gt;

&lt;p&gt;Edit: added download links for Rstudio preview&lt;/p&gt;
</description></item>
    <item><title>Tim Hortons Density</title><link>http://rmflight.github.io/posts/2013/06/timmysDensity.html</link><category>R</category><category>timhortons</category><category>mapping</category><description>&lt;h1&gt;Tim Hortons Density&lt;/h1&gt;

&lt;p&gt;Inspired by this &lt;a href=&quot;http://www.ifweassume.com/2012/10/the-united-states-of-starbucks.html&quot;&gt;post&lt;/a&gt;, I wanted to examine the locations and density of Tim Hortons restaurants in Canada. Using Stats Canada data, each census tract is queried on Foursquare for Tims locations.&lt;/p&gt;

&lt;h2&gt;Setup&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;options(stringsAsFactors = F)
require(timmysDensity)
require(plyr)
require(maps)
require(ggplot2)
require(geosphere)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Statistics Canada Census Data&lt;/h2&gt;

&lt;p&gt;The actual Statistics Canada data at the dissemination block level can be downloaded from &lt;a href=&quot;http://www.data.gc.ca/default.asp?lang=En&amp;amp;n=5175A6F0-1&amp;amp;xsl=datacataloguerecord&amp;amp;metaxsl=datacataloguerecord&amp;amp;formid=C87D5FDD-00E6-41A0-B5BA-E8E41B521ED0&quot;&gt;here&lt;/a&gt;. You will want to download the Excel format, read it, and then save it as either tab-delimited or CSV using a non-standard delimiter, I used a semi-colon (;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;censusData &amp;lt;- read.table(&amp;quot;../timmysData/2011_92-151_XBB_XLSX.csv&amp;quot;, header = F, 
    sep = &amp;quot;;&amp;quot;, quote = &amp;quot;&amp;quot;)
censusData &amp;lt;- censusData[, 1:17]
names(censusData) &amp;lt;- c(&amp;quot;DBuid&amp;quot;, &amp;quot;DBpop2011&amp;quot;, &amp;quot;DBtdwell2011&amp;quot;, &amp;quot;DBurdwell2011&amp;quot;, 
    &amp;quot;DBarea&amp;quot;, &amp;quot;DB_ir2011&amp;quot;, &amp;quot;DAuid&amp;quot;, &amp;quot;DAlamx&amp;quot;, &amp;quot;DAlamy&amp;quot;, &amp;quot;DAlat&amp;quot;, &amp;quot;DAlong&amp;quot;, &amp;quot;PRuid&amp;quot;, 
    &amp;quot;PRname&amp;quot;, &amp;quot;PRename&amp;quot;, &amp;quot;PRfname&amp;quot;, &amp;quot;PReabbr&amp;quot;, &amp;quot;PRfabbr&amp;quot;)
censusData$DBpop2011 &amp;lt;- as.numeric(censusData$DBpop2011)
censusData$DBpop2011[is.na(censusData$DBpop2011)] &amp;lt;- 0

censusData$DBtdwell2011 &amp;lt;- as.numeric(censusData$DBtdwell2011)
censusData$DBtdwell2011[is.na(censusData$DBtdwell2011)] &amp;lt;- 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From this data we get block level:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;populations (DBpop2011)&lt;/li&gt;
&lt;li&gt;total private dwellings (DBtdwell2011)&lt;/li&gt;
&lt;li&gt;privale dwellings occupied by usual residents (DBurdwell2011)&lt;/li&gt;
&lt;li&gt;block land area (DBarea)&lt;/li&gt;
&lt;li&gt;dissemination area id (DAuid)&lt;/li&gt;
&lt;li&gt;representative point x coordinate in Lambert projection (DAlamx)&lt;/li&gt;
&lt;li&gt;rep. point y coordinate in Lambert projection (DAlamy)&lt;/li&gt;
&lt;li&gt;rep. point latitude (DAlat)&lt;/li&gt;
&lt;li&gt;rep. point longitude (DAlong)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This should be everything we need to do the investigation we want.&lt;/p&gt;

&lt;h2&gt;Dissemination Area Long. and Lat.&lt;/h2&gt;

&lt;p&gt;We need to find the unique dissemination areas, and get out their latitudes and longitudes for querying in other databases. Note that the longitude and latitude provided here actually are weighted representative locations based on population. However, given the size of them, I don&amp;#39;t think using them will be a problem for &lt;code&gt;Foursquare&lt;/code&gt;. Because areas are what we have location data for, we will summarize everything at the area level, summing the population counts for all the blocks within an area.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;uniqAreas &amp;lt;- unique(censusData$DAuid)

summarizeArea &amp;lt;- function(areaID) {
    areaData &amp;lt;- censusData[(censusData$DAuid == areaID), ]
    outData &amp;lt;- data.frame(uid = areaID, lamx = areaData[1, &amp;quot;DAlamx&amp;quot;], lamy = areaData[1, 
        &amp;quot;DAlamy&amp;quot;], lat = areaData[1, &amp;quot;DAlat&amp;quot;], long = areaData[1, &amp;quot;DAlong&amp;quot;], 
        pop = sum(areaData[, &amp;quot;DBpop2011&amp;quot;]), dwell = sum(areaData[, &amp;quot;DBtdwell2011&amp;quot;]), 
        prov = areaData[1, &amp;quot;PRename&amp;quot;])
    return(outData)
}
areaData &amp;lt;- adply(uniqAreas, 1, summarizeArea)
.sessionInfo &amp;lt;- sessionInfo()
.timedate &amp;lt;- Sys.time()
write.table(areaData, file = &amp;quot;../timmysData/areaData.txt&amp;quot;, sep = &amp;quot;\t&amp;quot;, row.names = F, 
    col.names = T)
save(areaData, .sessionInfo, .timedate, file = &amp;quot;../timmysData/areaDataFile.RData&amp;quot;, 
    compress = &amp;quot;xz&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Run queries on Foursquare&lt;/h2&gt;

&lt;h3&gt;Load up the data and verify what we have.&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;load(&amp;quot;../timmysData/areaDataFile.RData&amp;quot;)
head(areaData)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Generate queries and run&lt;/h3&gt;

&lt;p&gt;For each dissemination area (DA), we are going to use as the location for the query the latitude and longitude of each DA, as well as the search string &amp;ldquo;tim horton&amp;rdquo;. &lt;/p&gt;

&lt;p&gt;Because Foursquare limits the number of userless requests to &lt;a href=&quot;https://developer.foursquare.com/overview/ratelimits&quot;&gt;5000 / hr&lt;/a&gt;. To make sure we stay under this limit, the &lt;code&gt;runQueries&lt;/code&gt; function will only 5000 queries an hour.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;runQueries(areaData, idFile = &amp;quot;../timmysData/clientid.txt&amp;quot;, secretFile = &amp;quot;../timmysData/clientsecret.txt&amp;quot;, 
    outFile = &amp;quot;../timmysData/timmysLocs2.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Clean up the results&lt;/h3&gt;

&lt;p&gt;Due to the small size of the DAs, we have a lot of duplicate entries. Now lets remove all the duplicate entries.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;cleanUpResults(&amp;quot;../timmysData/timmysLocs2.txt&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Visualize Locations&lt;/h2&gt;

&lt;p&gt;First lets read in the data and make sure that we have Tims locations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;# read in and clean up the data
timsLocs &amp;lt;- scan(file = &amp;quot;../timmysData/timmysLocs2.txt&amp;quot;, what = character(), 
    sep = &amp;quot;\n&amp;quot;)
timsLocs &amp;lt;- strsplit(timsLocs, &amp;quot;:&amp;quot;)

timsName &amp;lt;- sapply(timsLocs, function(x) {
    x[1]
})
timsLat &amp;lt;- sapply(timsLocs, function(x) {
    x[2]
})
timsLong &amp;lt;- sapply(timsLocs, function(x) {
    x[3]
})

locData &amp;lt;- data.frame(description = timsName, lat = as.numeric(timsLat), long = as.numeric(timsLong))
hasNA &amp;lt;- is.na(locData[, &amp;quot;lat&amp;quot;]) | is.na(locData[, &amp;quot;long&amp;quot;])
locData &amp;lt;- locData[!(hasNA), ]

timsStr &amp;lt;- c(&amp;quot;tim hortons&amp;quot;, &amp;quot;tim horton&amp;#39;s&amp;quot;)

hasTims &amp;lt;- (grepl(timsStr[1], locData$description, ignore.case = T)) | (grepl(timsStr[2], 
    locData$description, ignore.case = T))

locData &amp;lt;- locData[hasTims, ]
timsLocs &amp;lt;- locData
rm(timsName, timsLat, timsLong, hasNA, locData, hasTims, timsStr)
.timedate &amp;lt;- Sys.time()
.sessionInfo &amp;lt;- sessionInfo()
save(timsLocs, .timedate, .sessionInfo, file = &amp;quot;../timmysData/timsLocs.RData&amp;quot;, 
    compress = &amp;quot;xz&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Put them on a map&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;data(timsLocs)
data(areaDataFile)
canada &amp;lt;- map_data(&amp;quot;world&amp;quot;, &amp;quot;canada&amp;quot;)

p &amp;lt;- ggplot(legend = FALSE) + geom_polygon(data = canada, aes(x = long, y = lat, 
    group = group)) + theme(panel.background = element_blank()) + theme(panel.grid.major = element_blank()) + 
    theme(panel.grid.minor = element_blank()) + theme(axis.text.x = element_blank(), 
    axis.text.y = element_blank()) + theme(axis.ticks = element_blank()) + xlab(&amp;quot;&amp;quot;) + 
    ylab(&amp;quot;&amp;quot;)

sp &amp;lt;- timsLocs[1, c(&amp;quot;lat&amp;quot;, &amp;quot;long&amp;quot;)]

p2 &amp;lt;- p + geom_point(data = timsLocs[, c(&amp;quot;lat&amp;quot;, &amp;quot;long&amp;quot;)], aes(x = long, y = lat), 
    colour = &amp;quot;green&amp;quot;, size = 1, alpha = 0.5)

print(p2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog/researchBlog/img/mapIt.png&quot; alt=&quot;plot of chunk mapIt&quot;/&gt; &lt;/p&gt;

&lt;h3&gt;How far??&lt;/h3&gt;

&lt;p&gt;And now lets also calculate the minimum distance of a given DA from Timmys locations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;queryLocs &amp;lt;- matrix(c(timsLocs$long, timsLocs$lat), nrow = nrow(timsLocs), ncol = 2, 
    byrow = F)  # these are the tims locations
distLocs &amp;lt;- matrix(c(areaData$long, areaData$lat), nrow = nrow(areaData), ncol = 2, 
    byrow = F)  # the census centers
allDists &amp;lt;- apply(queryLocs, 1, function(x) {
    min(distHaversine(x, distLocs))  # only need the minimum value to determine 
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From the &lt;code&gt;allDists&lt;/code&gt; variable above, we can determine that the maximum distance any census dissemination area (DA) is from a Tim Hortons is 51.5 km (31.9815 miles). This is based on distances calculated &amp;ldquo;as the crow flies&amp;rdquo;, but still, that is pretty close. Assuming roads, the furthest a Canadian should have to travel is less than an hour to get their Timmys fix. &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;totPopulation &amp;lt;- sum(areaData$pop, na.rm = T)
lessDist &amp;lt;- seq(50, 51.6 * 1000, 50)  # distances are in meters, so multiply by 1000 to get reasonable km

percPop &amp;lt;- sapply(lessDist, function(inDist) {
    isLess &amp;lt;- allDists &amp;lt; inDist
    sum(areaData$pop[isLess], na.rm = T)/totPopulation * 100
})

plotDistPerc &amp;lt;- data.frame(distance = lessDist, population = percPop, logDist = log10(lessDist))
ggplot(plotDistPerc, aes(x = logDist, y = population)) + geom_point() + xlab(&amp;quot;Log10 Distance&amp;quot;) + 
    ylab(&amp;quot;% Population&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog/researchBlog/img/percPopulation.png&quot; alt=&quot;plot of chunk percPopulation&quot;/&gt; &lt;/p&gt;

&lt;p&gt;What gets really interesting, is how much of the population lives within a given distance of a Timmys. By summing up the percentage of the population within given distances. The plot above shows that 50% of the population is within 316.2278 &lt;strong&gt;meters&lt;/strong&gt; of a Tim Hortons location. &lt;/p&gt;

&lt;p&gt;I guess Canadians really do like their Tim Hortons Coffee (and donuts!).&lt;/p&gt;

&lt;h2&gt;Replication&lt;/h2&gt;

&lt;p&gt;All of the necessary processed data and code is available in the &lt;code&gt;R&lt;/code&gt; package &lt;a href=&quot;https://github.com/rmflight/timmysDensity&quot;&gt;&lt;code&gt;timmysDensity&lt;/code&gt;&lt;/a&gt;. You can install it using &lt;code&gt;devtools&lt;/code&gt;. The original data files are linked in the relevant sections above.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;library(devtools)
install_github(&amp;quot;timmysDensity&amp;quot;, &amp;quot;rmflight&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Caveats&lt;/h3&gt;

&lt;p&gt;I originally did this work based on a different set of data, that I have not been able to locate the original source for. I have not compared these results to that data to verify their accuracy. When I do so, I will update the package, vignette and blog post.&lt;/p&gt;

&lt;h2&gt;Posted&lt;/h2&gt;

&lt;p&gt;This work exists as the vignette of &lt;a href=&quot;https://github.com/rmflight/timmysDensity&quot;&gt;&lt;code&gt;timmysDensity&lt;/code&gt;&lt;/a&gt;, on my &lt;a href=&quot;http://rmflight.github.io/posts/2013/06/timmysDensity.html&quot;&gt;web-blog&lt;/a&gt;, and independently as the front page for the &lt;a href=&quot;http://rmflight.github.io/timmysDensity&quot;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Disclaimer&lt;/h2&gt;

&lt;p&gt;Tim Hortons was not involved in the creation or preparation of this work. I am not regularly updating the location information obtained from Foursquare, it is only valid for May 31, 2013. All code used in preparing these results was written by me, except in the case where code from other &lt;code&gt;R&lt;/code&gt; packages was used. All opinions and conclusions are my own, and do not reflect the views of anyone else or any institution I may be associated with.&lt;/p&gt;

&lt;h2&gt;Other information when this vignette was generated&lt;/h2&gt;

&lt;h3&gt;Session Info&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## R version 3.0.0 (2013-04-03)
## Platform: x86_64-unknown-linux-gnu (64-bit)
## 
## locale:
##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
##  [7] LC_PAPER=C                 LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] tools     stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] geosphere_1.2-28    sp_1.0-11           ggplot2_0.9.3.1    
##  [4] maps_2.3-2          plyr_1.8            timmysDensity_0.0.6
##  [7] samatha_0.3         XML_3.98-1.1        RJSONIO_1.0-3      
## [10] markdown_0.6.1      knitr_1.3           stringr_0.6.2      
## 
## loaded via a namespace (and not attached):
##  [1] colorspace_1.2-2   dichromat_2.0-0    digest_0.6.3      
##  [4] evaluate_0.4.4     formatR_0.8        grid_3.0.0        
##  [7] gtable_0.1.2       labeling_0.2       lattice_0.20-15   
## [10] lubridate_1.3.0    MASS_7.3-27        munsell_0.4.2     
## [13] proto_0.3-10       RColorBrewer_1.0-5 RCurl_1.95-4.1    
## [16] reshape2_1.2.2     scales_0.2.3
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Time and Date&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;Sys.time()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2013-09-16 14:02:57 EDT&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description></item>
    <item><title>Storing package data in custom environments</title><link>http://rmflight.github.io/posts/2013/05/customEnvironments.html</link><category>R</category><category>package</category><description>&lt;h1&gt;Storing package data in custom environments&lt;/h1&gt;

&lt;p&gt;If you do &lt;code&gt;R&lt;/code&gt; package development, sometimes you want to be able to store variables specific to your package, without cluttering up the users workspace. One way to do this is by modifying the global &lt;code&gt;options&lt;/code&gt;. This is done by packages &lt;code&gt;grDevices&lt;/code&gt; and &lt;code&gt;parallel&lt;/code&gt;. Sometimes this doesn&amp;#39;t seem to work quite right (see this &lt;a href=&quot;https://github.com/cboettig/knitcitations/issues/14&quot;&gt;issue&lt;/a&gt; for example.&lt;/p&gt;

&lt;p&gt;Another way to do this is to create an environment within your package, that only package functions will be able to see, and therefore read from and modify. You get a space to put package specific stuff, the user can&amp;#39;t see it or modify it directly, and you just need to write functions that do the appropriate things to that environment (adding variables, reading them, etc). This sounds great in practice, but I wasn&amp;#39;t clear on how to do this, even after reading the &lt;a href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/base/html/environment.html&quot;&gt;help page on environments&lt;/a&gt;, the &lt;a href=&quot;http://cran.r-project.org/doc/manuals/r-release/R-intro.html&quot;&gt;R documentation&lt;/a&gt;, or even &lt;a href=&quot;https://github.com/hadley/devtools/wiki/Environments&quot;&gt;Hadley&amp;#39;s excellent writeup&lt;/a&gt;. From all these sources, I could glean that one can create environments, name them, modify them, etc, but wasn&amp;#39;t sure how to work with this within a package.&lt;/p&gt;

&lt;p&gt;I checked out the &lt;code&gt;[knitcitations](https://github.com/cboettig/knitcitations)&lt;/code&gt; package to see how it was done. When I looked, I realized that it was pretty obvious in retrospect. In &lt;code&gt;zzz.R&lt;/code&gt;, initialize the environment, assigning it to a variable. When you need to work with the variables inside, this variable will be accessible to your package, and you simply use the &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;assign&lt;/code&gt; functions like you would if you were doing anything on the command line.&lt;/p&gt;

&lt;p&gt;To make sure I had it figured out, I created a very &lt;a href=&quot;https://github.com/rmflight/testEnvironment&quot;&gt;tiny package&lt;/a&gt; to create a custom environment and functions for modifying it. Please feel free to examine, download, install (using &lt;code&gt;[devtools](https://github.com/hadley/devtools)]&lt;/code&gt;) and see for yourself.&lt;/p&gt;

&lt;p&gt;I have at least two projects where I know I will use this, and I&amp;#39;m sure others might find it useful as well.&lt;/p&gt;
</description></item>
    <item><title>Writing up scientific results and literate programming</title><link>http://rmflight.github.io/posts/2013/05/writing_up_results.html</link><category>literateprogramming</category><category>writing</category><description>&lt;h1&gt;Writing up scientific results and literate programming&lt;/h1&gt;

&lt;p&gt;As an academic researcher, my primary purpose is to find some new insight, and subsequently communicate this insight to the general public. The process of doing this is traditionally thought to be:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;from observations of the world, generate a hypothesis&lt;/li&gt;
&lt;li&gt;design experiments to test hypothesis&lt;/li&gt;
&lt;li&gt;analyse results of the experiments to determine if hypothesis correct&lt;/li&gt;
&lt;li&gt;write report to communicate results to others (academics and / or general public)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And then repeat.&lt;/p&gt;

&lt;p&gt;This is the way people envision it happening. And I would imagine that in some rare cases, this is what actually happens. However, I think many researchers would agree that this is not what normally happens. In the process of doing steps &lt;strong&gt;3&lt;/strong&gt; and &lt;strong&gt;4&lt;/strong&gt;, your hypothesis in &lt;strong&gt;1&lt;/strong&gt; will be modified, which modifies the experiments in &lt;strong&gt;2&lt;/strong&gt;, and so on and so forth. This makes the process of scientific discovery a very iterative process, often times right up to the report writing. &lt;/p&gt;

&lt;p&gt;For some of this, it takes a long time to figure this out. I&amp;#39;ll never forget a professor during my PhD who suggested that you write the paper, and then figure out what experiments you should do to generate the results that would support or disprove the hypothesis you made in the paper. At the time I thought he was nuts, but when you start writing stuff, and looking at how all the steps of experiment and reporting can become intertwined, it doesn&amp;#39;t seem like a bad idea. &lt;a href=&quot;#note1&quot;&gt;note1&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;Literate programming&lt;/h2&gt;

&lt;p&gt;What does this have to do with &lt;a href=&quot;http://en.wikipedia.org/wiki/Literate_programming&quot;&gt;&lt;code&gt;literate programming&lt;/code&gt;&lt;/a&gt;? For those who don&amp;#39;t know, &lt;code&gt;literate programming&lt;/code&gt; is a way to mix code and prose together in one document (in &lt;code&gt;R&lt;/code&gt; we use &lt;code&gt;knitr&lt;/code&gt; &amp;amp; &lt;code&gt;sweave&lt;/code&gt;, &lt;code&gt;python&lt;/code&gt; now has the &lt;code&gt;iPython&lt;/code&gt; notebook as an option). This &lt;code&gt;literate programming&lt;/code&gt; paradigm (combined with &lt;code&gt;markdown&lt;/code&gt; as the markup language instead of &lt;code&gt;latex&lt;/code&gt; thanks to &lt;code&gt;knitr&lt;/code&gt;) is changing how I actually write my papers and do research in general. &lt;/p&gt;

&lt;h2&gt;How that changes writing&lt;/h2&gt;

&lt;p&gt;As I&amp;#39;ve previously described &lt;a href=&quot;http://robertmflight.blogspot.com/2012/10/writing-papers-using-r-markdown.html&quot;&gt;1&lt;/a&gt;&lt;a href=&quot;http://robertmflight.blogspot.com/2012/08/loving-markdown.html&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;http://rstudio.org&quot;&gt;&lt;code&gt;RStudio&lt;/code&gt;&lt;/a&gt; makes the use of &lt;code&gt;knitr&lt;/code&gt; and generation of literate documents using computations in &lt;code&gt;R&lt;/code&gt; incredibly easy. Because my writing environment and programming environment are tightly coupled together, I can easily start writing what looks like a shareable, readable publication as soon as I start writing code. Couple this together with a CVS like &lt;code&gt;git&lt;/code&gt;, and you have a way to follow the complete providence of a publication from start to finish.&lt;/p&gt;

&lt;p&gt;Instead of commenting my code to explain &lt;strong&gt;why&lt;/strong&gt; I am doing something, I explain what I am doing in the prose, and then write the code to carry out that analysis. This changes my writing and coding style, and makes the interplay among the steps of writing scientific reports above much more explicit. I think it is a good thing, and will hopefully make my writing and research more productive.&lt;/p&gt;

&lt;h3&gt;Notes&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;note1&quot;&gt;1.&lt;/a&gt; I am not suggesting that one only do experiments that will support the experiment, but writing out the paper at least gives you a framework for doing the experiments, and doing initial analysis. One should always be willing to modify the publication / hypothesis based on what the experiments tell you.&lt;/p&gt;

&lt;h3&gt;Sources&lt;/h3&gt;

&lt;p&gt;Published 10.05.13 &lt;a href=&quot;http://robertmflight.blogspot.com/scientific-writing.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The source markdown for this document is available &lt;a href=&quot;https://github.com/rmflight/blogPosts/blob/master/writing_up_results.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
</description></item>
    <item><title>Hive Plots using R and Cytoscape</title><link>http://rmflight.github.io/posts/2012/11/hiveplots_example.html</link><category>R</category><category>hiveplot</category><category>cytoscape</category><description>&lt;h1&gt;Hive Plots using R and Cytoscape&lt;/h1&gt;

&lt;h2&gt;Hive Plots??&lt;/h2&gt;

&lt;p&gt;I found out about &lt;a href=&quot;http://www.hiveplot.net/&quot;&gt;&lt;code&gt;HivePlots&lt;/code&gt;&lt;/a&gt; this past summer, and although I thought they looked incredibly 
useful and awesome, I didn&amp;#39;t have a personal use for them at the time, and therefore put off doing anything with them. 
That recently changed when I encountered some particularly nasty hairballs of force-directed graphs. Unfortunately, the 
&lt;a href=&quot;http://academic.depauw.edu/%7Ehanson/HiveR/HiveR.html&quot;&gt;&lt;code&gt;HiveR&lt;/code&gt;&lt;/a&gt; package does not create interactive hiveplots (at least for 2D), and that is particularly important for me. I don&amp;#39;t necessarily want to be able to compare networks (one of the selling points made by Martin Krzywinski), but I do want to be able to explore the networks that I create. For 
that reason I have been a big fan of the &lt;a href=&quot;http://db.systemsbiology.net:8080/cytoscape/RCytoscape/versions/current/index.html&quot;&gt;&lt;code&gt;RCytoscape&lt;/code&gt;&lt;/a&gt; &lt;code&gt;Bioconductor&lt;/code&gt; package since I encountered it, as it allows me to easily create graphs in &lt;code&gt;R&lt;/code&gt;, and then interactively and programmatically explore them in &lt;a href=&quot;http://cytoscape.org&quot;&gt;&lt;code&gt;Cytoscape&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So I decided last week to see how hard it would be to generate a hive plot that could be visualized and interacted with in 
&lt;code&gt;Cytoscape&lt;/code&gt;. For this example I&amp;#39;m going to use the data in the &lt;code&gt;HiveR&lt;/code&gt; package, and actually use the structures already 
encoded, because they are useful.&lt;/p&gt;

&lt;h2&gt;Load Data&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;require(RCytoscape)
require(HiveR)
require(graph)
options(stringsAsFactors = F)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;dataDir &amp;lt;- file.path(system.file(&amp;quot;extdata&amp;quot;, package = &amp;quot;HiveR&amp;quot;), &amp;quot;E_coli&amp;quot;)
EC1 &amp;lt;- dot2HPD(file = file.path(dataDir, &amp;quot;E_coli_TF.dot&amp;quot;), node.inst = NULL, 
    edge.inst = file.path(dataDir, &amp;quot;EdgeInst_TF.csv&amp;quot;), desc = &amp;quot;E coli gene regulatory network (RegulonDB)&amp;quot;, 
    axis.cols = rep(&amp;quot;grey&amp;quot;, 3))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## No node instructions provided, proceeding without them
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;str(EC1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## List of 5
##  $ nodes    :&amp;#39;data.frame&amp;#39;:   1597 obs. of  6 variables:
##   ..$ id    : int [1:1597] 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ lab   : chr [1:1597] &amp;quot;pstB&amp;quot; &amp;quot;hybE&amp;quot; &amp;quot;fadE&amp;quot; &amp;quot;phnF&amp;quot; ...
##   ..$ axis  : int [1:1597] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ radius: num [1:1597] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ size  : num [1:1597] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ color : chr [1:1597] &amp;quot;transparent&amp;quot; &amp;quot;transparent&amp;quot; &amp;quot;transparent&amp;quot; &amp;quot;transparent&amp;quot; ...
##  $ edges    :&amp;#39;data.frame&amp;#39;:   3893 obs. of  4 variables:
##   ..$ id1   : int [1:3893] 932 612 932 1510 1510 413 528 652 1396 400 ...
##   ..$ id2   : int [1:3893] 832 620 51 525 797 151 5 1058 1396 1559 ...
##   ..$ weight: num [1:3893] 1 1 1 1 1 1 1 1 1 1 ...
##   ..$ color : chr [1:3893] &amp;quot;red&amp;quot; &amp;quot;red&amp;quot; &amp;quot;green&amp;quot; &amp;quot;green&amp;quot; ...
##  $ desc     : chr &amp;quot;E coli gene regulatory network (RegulonDB)&amp;quot;
##  $ axis.cols: chr [1:3] &amp;quot;grey&amp;quot; &amp;quot;grey&amp;quot; &amp;quot;grey&amp;quot;
##  $ type     : chr &amp;quot;2D&amp;quot;
##  - attr(*, &amp;quot;class&amp;quot;)= chr &amp;quot;HivePlotData&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Process Data&lt;/h2&gt;

&lt;p&gt;So here we have the data. The &lt;code&gt;nodes&lt;/code&gt; is a data frame with the &lt;code&gt;id&lt;/code&gt;, a &lt;code&gt;label&lt;/code&gt; describing the node, which &lt;code&gt;axis&lt;/code&gt; the node 
belongs on, and its &lt;code&gt;radius&lt;/code&gt;, or how far out on the axis the node should be, as well as a &lt;code&gt;size&lt;/code&gt;. These are all modifiable
attributes that can be changed depending on how one wants to map different pieces of data. This of course is the beauty of
hive plots, because they result in networks that are dependent on attributes that the user decides on.&lt;/p&gt;

&lt;p&gt;In this case, we have a transcription factor regulation network. I am going to point you to the previous links as to why
a normal force-directed network diagram is not really that informative for these types of networks. I&amp;#39;m not out to 
convince you that &lt;code&gt;HivePlots&lt;/code&gt; are useful, if you don&amp;#39;t get it from the publication and examples, then you should stop
here. This is more about how to do some calculations to lay them out and work with them in &lt;code&gt;Cytoscape&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Bryan has implemented some nice functions to work with this type of network and perform simple calculations to assign
axes and locations based on properties of the nodes. For example, it is easy to locate nodes on an axis based on the total number of edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;EC2 &amp;lt;- mineHPD(EC1, option = &amp;quot;rad &amp;lt;- tot.edge.count&amp;quot;)
sumHPD(EC2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  E coli gene regulatory network (RegulonDB)
##  This hive plot data set contains 1597 nodes on 1 axes and 3893 edges.
##  It is a  2D data set.
## 
##      Axis 1 has 1597 nodes spanning radii from 1 to 434 
## 
##      Axes 1 and 1 share 3893 edges
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then to assign the axis to be plotted on based on the whether edges are incoming (sink), outgoing (source), or both (manager). These are the types of decisions that influence whether you get anything insightful or useful out of a 
&lt;code&gt;HivePlot&lt;/code&gt;, and changing these options can of course change the conclusions you will make on a particular network.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;EC3 &amp;lt;- mineHPD(EC2, option = &amp;quot;axis &amp;lt;- source.man.sink&amp;quot;)
sumHPD(EC3)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  E coli gene regulatory network (RegulonDB)
##  This hive plot data set contains 1597 nodes on 3 axes and 3893 edges.
##  It is a  2D data set.
## 
##      Axis 1 has 45 nodes spanning radii from 1 to 83 
##      Axis 2 has 1416 nodes spanning radii from 1 to 11 
##      Axis 3 has 136 nodes spanning radii from 2 to 434 
## 
##      Axes 1 and 2 share 400 edges
##      Axes 1 and 3 share 21 edges
##      Axes 3 and 2 share 3158 edges
##      Axes 3 and 3 share 314 edges
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We also remove any nodes that have zero edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;EC4 &amp;lt;- mineHPD(EC3, option = &amp;quot;remove zero edge&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## 
##   125 edges that start and end on the same point were removed
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally re-order the edges (not sure how this would affect plotting using Cytoscape).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;edges &amp;lt;- EC4$edges
edgesR &amp;lt;- subset(edges, color == &amp;quot;red&amp;quot;)
edgesG &amp;lt;- subset(edges, color == &amp;quot;green&amp;quot;)
edgesO &amp;lt;- subset(edges, color == &amp;quot;orange&amp;quot;)
edges &amp;lt;- rbind(edgesO, edgesG, edgesR)
EC4$edges &amp;lt;- edges
EC4$edges$weight = 0.5
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Calculate Node Locations&lt;/h2&gt;

&lt;p&gt;In this case we have three axes, so we are going to calculate the axes locations as 0, 120, and 240 degrees. However, we
need to use radians, because the conversion from spherical to cartesian coordinates involves using cosine and sine, which 
in &lt;code&gt;R&lt;/code&gt; is based on radians.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;r2xy &amp;lt;- function(inRad, inPhi) {
    x &amp;lt;- inRad * sin(inPhi)
    y &amp;lt;- inRad * cos(inPhi)
    cbind(x, y)
}

tmpDat &amp;lt;- EC4$nodes[, c(&amp;quot;id&amp;quot;, &amp;quot;axis&amp;quot;, &amp;quot;radius&amp;quot;)]
tmpDat$radius &amp;lt;- tmpDat$radius * 3  # bump it up as cytoscape coordinates are small
tmpDat$phi &amp;lt;- ((tmpDat$axis - 1) * 120) * (pi/180)

nodeXY &amp;lt;- r2xy(tmpDat$radius, tmpDat$phi)  # contains cartesian coordinates
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Create GraphNEL&lt;/h2&gt;

&lt;p&gt;Initialize the graph with the nodes and the edges.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;hiveGraph &amp;lt;- new(&amp;quot;graphNEL&amp;quot;, nodes = as.character(EC4$nodes$id), edgemode = &amp;quot;directed&amp;quot;)
hiveGraph &amp;lt;- addEdge(as.character(EC4$edges$id1), as.character(EC4$edges$id2), 
    hiveGraph)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We also want to put information we know about the nodes and edges in the graph, so that we can modify colors and stuff
based on those attributes. For example, in this case we might want to modify the node color based on the axis it is on.
Using attributes means we are not stuck using the colors that we previously assigned.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;nodeDataDefaults(hiveGraph, &amp;quot;nodeType&amp;quot;) &amp;lt;- &amp;quot;&amp;quot;
attr(nodeDataDefaults(hiveGraph, &amp;quot;nodeType&amp;quot;), &amp;quot;class&amp;quot;) &amp;lt;- &amp;quot;STRING&amp;quot;

nodeTypes &amp;lt;- c(`1` = &amp;quot;source&amp;quot;, `2` = &amp;quot;man&amp;quot;, `3` = &amp;quot;sink&amp;quot;)
nodeData(hiveGraph, as.character(EC4$nodes$id), &amp;quot;nodeType&amp;quot;) &amp;lt;- nodeTypes[as.character(EC4$nodes$axis)]

edgeDataDefaults(hiveGraph, &amp;quot;interactionType&amp;quot;) &amp;lt;- &amp;quot;&amp;quot;
attr(edgeDataDefaults(hiveGraph, &amp;quot;interactionType&amp;quot;), &amp;quot;class&amp;quot;) &amp;lt;- &amp;quot;STRING&amp;quot;

interactionType &amp;lt;- c(red = &amp;quot;repressor&amp;quot;, green = &amp;quot;activator&amp;quot;, orange = &amp;quot;dual&amp;quot;)
edgeData(hiveGraph, as.character(EC4$edges$id1), as.character(EC4$edges$id2), 
    &amp;quot;interactionType&amp;quot;) &amp;lt;- interactionType[EC4$edges$color]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Transfer to Cytoscape&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;ccHive &amp;lt;- CytoscapeWindow(&amp;quot;hiveTest&amp;quot;, hiveGraph)
displayGraph(ccHive)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;nodeType&amp;quot;
## [1] &amp;quot;label&amp;quot;
## [1] &amp;quot;interactionType&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now lets move those nodes to their positions based on the Hive Graph calculations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;setNodePosition(ccHive, as.character(EC4$nodes$id), nodeXY[, 1], nodeXY[, 2])
fitContent(ccHive)
setDefaultNodeSize(ccHive, 5)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And set the colors based on attributes:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;nodeColors &amp;lt;- hcl(h = c(0, 120, 240), c = 55, l = 45)  # darker for the nodes
edgeColors &amp;lt;- hcl(h = c(0, 120, 60), c = 45, l = 75)  # lighter for the edges

setNodeColorRule(ccHive, &amp;quot;nodeType&amp;quot;, c(&amp;quot;source&amp;quot;, &amp;quot;man&amp;quot;, &amp;quot;sink&amp;quot;), nodeColors, 
    &amp;quot;lookup&amp;quot;)
setNodeBorderColorRule(ccHive, &amp;quot;nodeType&amp;quot;, c(&amp;quot;source&amp;quot;, &amp;quot;man&amp;quot;, &amp;quot;sink&amp;quot;), nodeColors, 
    &amp;quot;lookup&amp;quot;)
setEdgeColorRule(ccHive, &amp;quot;interactionType&amp;quot;, c(&amp;quot;repressor&amp;quot;, &amp;quot;activator&amp;quot;, &amp;quot;dual&amp;quot;), 
    edgeColors, &amp;quot;lookup&amp;quot;)
setNodeFontSizeDirect(ccHive, as.character(EC4$nodes$id), 0)
redraw(ccHive)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;fitContent(ccHive)
saveImage(ccHive, file.path(imgPath, &amp;quot;hive_nonScaled.png&amp;quot;), &amp;quot;PNG&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog/researchBlog/img/hive_nonScaled.png&quot; alt=&quot;nonScaled&quot;/&gt;&lt;/p&gt;

&lt;p&gt;This view doesn&amp;#39;t help us a whole lot, unfortunately. What if we normalize the radii for each axis to use a maximum value
of 100?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;useMax &amp;lt;- 100
invisible(sapply(c(1, 2, 3), function(inAxis) {
    isCurr &amp;lt;- EC4$nodes$axis == inAxis
    currMax &amp;lt;- max(EC4$nodes$radius[isCurr])
    scaleFact &amp;lt;- useMax/currMax
    EC4$nodes$radius[isCurr] &amp;lt;&amp;lt;- EC4$nodes$radius[isCurr] * scaleFact
}))

tmpDat &amp;lt;- EC4$nodes[, c(&amp;quot;id&amp;quot;, &amp;quot;axis&amp;quot;, &amp;quot;radius&amp;quot;)]
tmpDat$radius &amp;lt;- tmpDat$radius * 3  # bump it up as cytoscape coordinates are small
tmpDat$phi &amp;lt;- ((tmpDat$axis - 1) * 120) * (pi/180)

nodeXY &amp;lt;- r2xy(tmpDat$radius, tmpDat$phi)

setNodePosition(ccHive, as.character(EC4$nodes$id), nodeXY[, 1], nodeXY[, 2])
fitContent(ccHive)
redraw(ccHive)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;fitContent(ccHive)
saveImage(ccHive, file.path(imgPath, &amp;quot;hive_scaledAxes.png&amp;quot;), &amp;quot;PNG&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog/researchBlog/img/hive_scaledAxes.png&quot; alt=&quot;scaledAxes&quot;/&gt;&lt;/p&gt;

&lt;p&gt;This looks pretty awesome! And I can zoom in on it, and examine it, and look at various properties! And I get the full 
scripting power of &lt;code&gt;R&lt;/code&gt; if I want to do anything else with, such as select sets of edges or nodes and then query who is 
attached to whom. &lt;/p&gt;

&lt;h2&gt;Disadvantages&lt;/h2&gt;

&lt;p&gt;We don&amp;#39;t get the &lt;strong&gt;arced&lt;/strong&gt; edges. This kind of sucks, but from what little I have done with these, that actually is not that
big a deal. Would be cool if there was a way to do that, however. I do see that the web version of Cytoscape does allow
you to set a value for how much &amp;ldquo;arcness&amp;rdquo; you want on an edge. &lt;/p&gt;

&lt;p&gt;This does mean that any plot with only two axes would need special consideration. Instead of doing two axes end to end 
(using 180 deg), it might be better to make them parallel to each other.&lt;/p&gt;

&lt;p&gt;With more than three axes, line crossings may become a problem. In that case, it may be worth looking to see if there are
ways to tell Cytoscape in what order to draw edges. I don&amp;#39;t know if that is possible using the XMLRPC pipe that is used
by RCy.&lt;/p&gt;

&lt;h2&gt;RCy Tip&lt;/h2&gt;

&lt;p&gt;If you want to know how the image will look when saving a network to an image, use &lt;code&gt;showGraphicsDetails(obj, TRUE)&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Other Visualizations&lt;/h2&gt;

&lt;p&gt;Of course, I had just wrapped my head around using HivePlots in my own work, when I encountered ISBs &lt;a href=&quot;http://www.biofabric.org/&quot;&gt;&lt;code&gt;BioFabric&lt;/code&gt;&lt;/a&gt;. Given how they are representing this, could we find a way to draw this in Cytoscape??&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;deleteWindow(ccHive)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] TRUE
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;Session Info&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;Sys.time()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2013-09-16 14:53:05 EDT&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;sessionInfo()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## R version 3.0.0 (2013-04-03)
## Platform: x86_64-unknown-linux-gnu (64-bit)
## 
## locale:
##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
##  [7] LC_PAPER=C                 LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] tools     stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] HiveR_0.2-16      RCytoscape_1.10.0 XMLRPC_0.3-0     
##  [4] graph_1.38.2      samatha_0.3       XML_3.98-1.1     
##  [7] RJSONIO_1.0-3     markdown_0.6.1    knitr_1.3        
## [10] stringr_0.6.2    
## 
## loaded via a namespace (and not attached):
##  [1] BiocGenerics_0.6.0 digest_0.6.3       evaluate_0.4.4    
##  [4] formatR_0.8        grid_3.0.0         parallel_3.0.0    
##  [7] plyr_1.8           RColorBrewer_1.0-5 RCurl_1.95-4.1    
## [10] stats4_3.0.0       tcltk_3.0.0        tkrgl_0.7
&lt;/code&gt;&lt;/pre&gt;
</description></item>
    <item><title>Writing papers using R Markdown</title><link>http://rmflight.github.io/posts/2012/10/papersinRmd.html</link><category>R</category><category>openscience</category><category>reproducibility</category><category>tutorial</category><category>science</category><description>&lt;h1&gt;Writing papers using R Markdown&lt;/h1&gt;

&lt;p&gt;I have been watching the activity in &lt;a href=&quot;http://rstudio.org&quot;&gt;&lt;code&gt;RStudio&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;http://yihui.name/knitr/&quot;&gt;&lt;code&gt;knitr&lt;/code&gt;&lt;/a&gt; for a while, and
have even been using &lt;code&gt;Rmd&lt;/code&gt; (R markdown) files in my own work as a way to easily provide commentary on an actual dataset
analysis. Yihui has proposed &lt;a href=&quot;http://yihui.name/en/2012/03/a-really-fast-statistics-journal/&quot;&gt;writing papers&lt;/a&gt; in markdown and posting them to a blog as a way to host a statistics journal, and lots of people are now using &lt;code&gt;knitr&lt;/code&gt; as a way
to create reproducible blog posts that include code (including yours truly).&lt;/p&gt;

&lt;p&gt;The idea of writing a paper that actually includes the necessary code to perform the analysis, and is actually readable
in its raw form, and that someone else could actually run was pretty appealing. Unfortunately, I had not had the time
or opportunity to actually try it, until recently our group submitted a conference paper that included a lot of analysis in &lt;code&gt;R&lt;/code&gt; that seemed like the perfect opportunity to try this. 
(I will link to the paper here when I hear more, or get clearance from my PI). Originally we wrote the paper in Microsoft&amp;reg; Word, but after submission I decided to see what it would have taken to write it as an &lt;code&gt;Rmd&lt;/code&gt; document that could
then generate &lt;code&gt;markdown&lt;/code&gt; or &lt;code&gt;html&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It turned out that it was not that hard, but it did force me to do some things differently. This is what I want to 
discuss here.&lt;/p&gt;

&lt;h2&gt;Advantages&lt;/h2&gt;

&lt;p&gt;I actually found it much easier to have the text with the analysis (in contrast to having to be separate in a Word 
document), and upon doing the conversion, discovered some possible numerical errors that crept in because of having
to copy numerical results separately (that is the nice thing about being able to insert variable directly into the text). 
In addition, the Word template for the submission didn&amp;#39;t play nice with automatic table and figure numbering, so our
table and figure numbering got messed up in the submission. So overall, I&amp;#39;d say it worked out better with the &lt;code&gt;Rmd&lt;/code&gt; file
overall, even with the having to create functions to handle table and figure numbering properly myself (see below).&lt;/p&gt;

&lt;h2&gt;Tables and Figures&lt;/h2&gt;

&lt;p&gt;As I&amp;#39;m sure most of you know, Word (and other WYSIWYG editors) have ability to keep track of your object numbers, this
is especially nice for keeping your figure and table numbers straight. Of course, there is no such ability built into
a static text file, but I found it was easy to write a couple of functions for this. The way I came up with is to have
a variable that contains a label for the figure or table, a function that increments the counter when new figures or 
tables are added, and a function that prints the associated number for a particular label. This does require a bit of 
forethought on the part of the writer, because you may have to add a table or figure label to the variable long before
you actually create it, but as long as you use sane (i.e. descriptive) labels, it shouldn&amp;#39;t be a big deal. Let me show
you what I mean.&lt;/p&gt;

&lt;h3&gt;Counting&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;incCount &amp;lt;- function(inObj, useName) {
    nObj &amp;lt;- length(inObj)
    useNum &amp;lt;- max(inObj) + 1
    inObj &amp;lt;- c(inObj, useNum)
    names(inObj)[nObj + 1] &amp;lt;- useName
    inObj
}
figCount &amp;lt;- c(`_` = 0)
tableCount &amp;lt;- c(`_` = 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;incCount&lt;/code&gt; function is very simple, it takes an object, checks the maximum count, and then adds an incremental value
with the supplied name. In this example, I initialized the &lt;code&gt;figCount&lt;/code&gt; and &lt;code&gt;tableCount&lt;/code&gt; objects with a non-sensical named
value of zero. &lt;/p&gt;

&lt;p&gt;Now in the process of writing, I decide I&amp;#39;m going to need a table on the amount of time spent by post-docs writing blog
posts in different years of their post-doc training. Lets call this &lt;code&gt;t.blogPostDocs&lt;/code&gt;. Notice that this is a fairly 
descriptive name. We can assign it a number like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;tableCount &amp;lt;- incCount(tableCount, &amp;quot;t.blogPostDocs&amp;quot;)
tableCount
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##              _ t.blogPostDocs 
##              0              1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Inserting&lt;/h3&gt;

&lt;p&gt;So now we have a variable with a named number we can refer to. But how do we insert it into the text? We are going to
use another function that will let us insert either the text with a link, or just the text itself.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;pasteLabel &amp;lt;- function(preText, inObj, objName, insLink = TRUE) {
    objNum &amp;lt;- inObj[objName]

    useText &amp;lt;- paste(preText, objNum, sep = &amp;quot; &amp;quot;)
    if (insLink) {
        useText &amp;lt;- paste(&amp;quot;[&amp;quot;, useText, &amp;quot;](#&amp;quot;, objName, &amp;quot;)&amp;quot;, sep = &amp;quot;&amp;quot;)
    }
    useText
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function allows us to insert the table number like so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;r I(pasteLabel(&amp;quot;Table&amp;quot;, tableCount, &amp;quot;t.blogPostDocs&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This would be inserted into a normal &lt;code&gt;inline&lt;/code&gt; code block. The &lt;code&gt;I&lt;/code&gt; makes sure that the text will appear as normal text,
and not get formatted as a code block. The default behavior is to insert as a relative link, thereby enabling the use
of relative links to link where a table / figure is mentioned to its actual location. For example, we can insert the 
anchor link like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;a id=&amp;quot;t.blogPostDocs&amp;quot;&amp;gt;&amp;lt;/a&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;Markdown Tables&lt;/h3&gt;

&lt;p&gt;Followed by the actual table text. This brings up the subject of &lt;code&gt;markdown&lt;/code&gt; tables. I also wrote a function (thanks to
Yihui again) that transforms a normal &lt;code&gt;R&lt;/code&gt; &lt;code&gt;data.frame&lt;/code&gt; to a &lt;code&gt;markdown&lt;/code&gt; table.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;tableCat &amp;lt;- function(inFrame) {
    outText &amp;lt;- paste(names(inFrame), collapse = &amp;quot; | &amp;quot;)
    outText &amp;lt;- c(outText, paste(rep(&amp;quot;---&amp;quot;, ncol(inFrame)), collapse = &amp;quot; | &amp;quot;))
    invisible(apply(inFrame, 1, function(inRow) {
        outText &amp;lt;&amp;lt;- c(outText, paste(inRow, collapse = &amp;quot; | &amp;quot;))
    }))
    return(outText)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lets see it in action.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;postDocBlogs &amp;lt;- data.frame(PD = c(&amp;quot;p1&amp;quot;, &amp;quot;p2&amp;quot;, &amp;quot;p3&amp;quot;), NBlog = c(4, 10, 2), Year = c(1, 
    4, 2))
postDocBlogs
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##   PD NBlog Year
## 1 p1     4    1
## 2 p2    10    4
## 3 p3     2    2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;
postDocInsert &amp;lt;- tableCat(postDocBlogs)
postDocInsert
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;PD | NBlog | Year&amp;quot; &amp;quot;--- | --- | ---&amp;quot;   &amp;quot;p1 |  4 | 1&amp;quot;      
## [4] &amp;quot;p2 | 10 | 4&amp;quot;       &amp;quot;p3 |  2 | 2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To actually insert it into the text, use a code chunk with &lt;code&gt;results=&amp;#39;asis&amp;#39;&lt;/code&gt; and &lt;code&gt;echo=FALSE&lt;/code&gt;. &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;cat(postDocInsert, sep = &amp;quot;\n&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;PD&lt;/th&gt;
&lt;th&gt;NBlog&lt;/th&gt;
&lt;th&gt;Year&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;p1&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;p2&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;p3&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

&lt;p&gt;Before inserting the table though, you might want an inline code with the table number and caption, like this:&lt;/p&gt;

&lt;p&gt;&lt;code&gt;I(pasteLabel(&amp;quot;Table&amp;quot;, tableCount, &amp;quot;t.blogPostDocs&amp;quot;, FALSE))&lt;/code&gt; This is the number of blog posts and year of training for post-docs.&lt;/p&gt;

&lt;p&gt;Table 1 This is the number of blog posts and year of training for post-docs.&lt;/p&gt;

&lt;p&gt;Remember for captions to set the &lt;code&gt;insLink&lt;/code&gt; variable to &lt;code&gt;FALSE&lt;/code&gt; so that you don&amp;#39;t generate a link from the caption.&lt;/p&gt;

&lt;h3&gt;Figures&lt;/h3&gt;

&lt;p&gt;Oftentimes, you will have code that generates the figure, and then you want to insert the figure at a different point.
This is accomplished by the judicious use of &lt;code&gt;echo&lt;/code&gt; and &lt;code&gt;include&lt;/code&gt; chunk options.&lt;/p&gt;

&lt;p&gt;For example, we can create a &lt;code&gt;ggplot2&lt;/code&gt; figure and store it in a variable in one chunk, and then &lt;code&gt;print&lt;/code&gt; it in a later
chunk to actually insert it into the text body.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;plotData &amp;lt;- data.frame(x = rnorm(1000, 1, 5), y = rnorm(1000, 0, 2))
plotKeep &amp;lt;- ggplot(plotData, aes(x = x, y = y)) + geom_point()
figCounts &amp;lt;- incCount(figCount, &amp;quot;f.randomFigure&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now we decide to actually insert it using &lt;code&gt;print(plotKeep)&lt;/code&gt; with the option of &lt;code&gt;echo=FALSE&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://rmflight.github.io/mlab/data/rmflight/Documents/researchBlog//researchBlog/img/figureInsert.png&quot; alt=&quot;plot of chunk figureInsert&quot;/&gt; &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#f.randomFigure&quot;&gt;Figure 1&lt;/a&gt;. A random figure.&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Numerical result formatting&lt;/h2&gt;

&lt;p&gt;When &lt;code&gt;R&lt;/code&gt; prints a number, it normally likes to do so with lots of digits. This is not probably what you want either in
a table or when reporting a number in a sentence. You can control that by using the &lt;code&gt;format&lt;/code&gt; function. When generating
a new variable, the number of digits to display when printing will be saved, and when used on a variable directly,
only the defined number of digits will display.&lt;/p&gt;

&lt;h2&gt;Echo and Include&lt;/h2&gt;

&lt;p&gt;This brings up the issue of how to keep the code from appearing in the text body. I found depending on the particulars,
either using &lt;code&gt;echo=FALSE&lt;/code&gt; or &lt;code&gt;include=FALSE&lt;/code&gt; would do the job. This is meant to be a paper, a reproducible one, but a 
paper nonetheless, and therefore the code should not end up in the text body. &lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;p&gt;One thing I haven&amp;#39;t done yet is convert all the references. I am planning to try using the &lt;a href=&quot;https://github.com/cboettig/knitcitations/&quot;&gt;knitcitations&lt;/a&gt; package. I will probably post on that experience.&lt;/p&gt;

&lt;h2&gt;HTML generation&lt;/h2&gt;

&lt;p&gt;Because I use &lt;code&gt;RStudio&lt;/code&gt;, I set up a modified function For generating a full &lt;code&gt;html&lt;/code&gt; version of the paper, changing the 
default &lt;code&gt;RStudio&lt;/code&gt; &lt;code&gt;markdown&lt;/code&gt; render options like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;htmlOptions &amp;lt;- markdownHTMLOptions(defaults=TRUE)
htmlOptions &amp;lt;- htmlOptions[htmlOptions != &amp;quot;hard_wrap&amp;quot;]
markdownToHTML(inputFile, outputFile, options = htmlOptions)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should be added to a &lt;code&gt;.Rprofile&lt;/code&gt; file either in your &lt;code&gt;home&lt;/code&gt; directory or in the directory you start &lt;code&gt;R&lt;/code&gt; in (this
is especially useful for modification on a per project basis).&lt;/p&gt;

&lt;p&gt;I do this because when I write my documents, I want the source to be readable online. If this is a &lt;code&gt;github&lt;/code&gt; hosted repo,
that means being displayed in the &lt;code&gt;github&lt;/code&gt; file browser, which does not do line wrapping. So I set up a 120 character
line in my editor, and try very hard to stick to that. &lt;/p&gt;

&lt;h2&gt;Function source&lt;/h2&gt;

&lt;p&gt;You can find the previously mentioned functions in a github gist &lt;a href=&quot;https://gist.github.com/3858973&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Post source&lt;/h2&gt;

&lt;p&gt;The source files for this blog post can be found at: &lt;a href=&quot;https://github.com/rmflight/blogPosts/blob/master/papersinRmd.Rmd&quot;&gt;&lt;code&gt;Rmd&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://github.com/rmflight/blogPosts/blob/master/papersinRmd.md&quot;&gt;&lt;code&gt;md&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;https://github.com/rmflight/blogPosts/blob/master/papersinRmd.html&quot;&gt;&lt;code&gt;html&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Posted on October 9, 2012, at &lt;a href=&quot;http://robertmflight.blogspot.com/2012/10/writing-papers-using-r-markdown.html&quot;&gt;http://robertmflight.blogspot.com/2012/10/writing-papers-using-r-markdown.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Edit: added section on formatting numerical results&lt;/p&gt;

&lt;p&gt;Edit: added session info&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;R version 3.0.1 (2013-05-16)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=C                 LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] tools     stats     graphics  grDevices utils     datasets  methods  
[8] base     

other attached packages:
[1] ggplot2_0.9.3.1 samatha_0.3     XML_3.98-1.1    RJSONIO_1.0-3  
[5] markdown_0.6.3  knitr_1.4.12    stringr_0.6.2  

loaded via a namespace (and not attached):
 [1] colorspace_1.2-3   dichromat_2.0-0    digest_0.6.3      
 [4] evaluate_0.4.7     formatR_0.9        grid_3.0.1        
 [7] gtable_0.1.2       labeling_0.2       MASS_7.3-29       
[10] munsell_0.4.2      plyr_1.8           proto_0.3-10      
[13] RColorBrewer_1.0-5 reshape2_1.2.2     scales_0.2.3      
&lt;/code&gt;&lt;/pre&gt;
</description></item>
    <item><title>Journal Club: 15.08.12</title><link>http://rmflight.github.io/posts/2012/08/jc_150812.html</link><description>&lt;h1&gt;Journal Club: 15.08.12&lt;/h1&gt;

&lt;p&gt;I just came back from our Bioinformatic group (a rather loose association of various
researchers at UofL interested in and doing bioinformatics) journal club, where we 
discussed this recent paper:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002511&quot;&gt;Google Goes Cancer: Improving Outcome Prediction for Cancer Patients by Network-Based Ranking of Marker Genes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Besides the catchy title that makes one believe that perhaps Google is getting into
cancer research (maybe they are and we don&amp;#39;t know it yet), there were some interesting
aspects to this paper. &lt;/p&gt;

&lt;h2&gt;Premise&lt;/h2&gt;

&lt;p&gt;The premise is that they can combine gene expression data and network data to find 
better associations between gene expression data and a particular disease endpoint.
The way this is carried out is through the use of the TRANSFAC transcription factor -
gene target database for the network, the correlation of the gene expression with
the disease status as the importance of a gene with the disease, and the Google
&lt;a href=&quot;http://en.wikipedia.org/wiki/PageRank&quot;&gt;PageRank&lt;/a&gt; as the means to transfer the network knowledge to the gene expression
data. They call their method &lt;strong&gt;NetRank&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;Note that the general idea had already been tried in this paper on &lt;a href=&quot;http://dx.doi.org/10.1186/1471-2105-6-233&quot;&gt;GeneRank&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Implementation&lt;/h2&gt;

&lt;p&gt;Rank the genes with disease status (poor or good prognosis) using a method (SAM,
t-test, fold-change, correlation, NetRank). Pick &lt;em&gt;n&lt;/em&gt; top genes, and develop a
predictive model using a support vector machine. Wash, rinse, repeat several times
to find the best set, varying the number of top genes, and the number of samples used
in the training set.&lt;/p&gt;

&lt;p&gt;For &lt;strong&gt;NetRank&lt;/strong&gt;, the top genes were decided by using a sub-optimization based on
varying &lt;em&gt;d&lt;/em&gt;, the dampening factor in the PageRank algorithm that determines how
much information can be transferred to other genes. The best value of &lt;em&gt;d&lt;/em&gt; determined
in this study was 0.3.&lt;/p&gt;

&lt;p&gt;All other methods used just the 8000 genes that passed filtering, but NetRank used
all the genes on the array, with those that were filtered out had their initial
correlations set to 0, so that they were still in the network representation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002511.g001&amp;amp;representation=PNG_I&quot; alt=&quot;Monte Carlo cross-validation&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;Did it work?&lt;/h2&gt;

&lt;p&gt;From the paper, it appears to have worked. Using a monte-carlo cross-validation,
they were able to achieve over 70% prediction rates. And this was better than any
of the other methods they used to associate genes with the disease, including SAM,
t-test, fold-change, and raw correlations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.ploscompbiol.org/article/fetchObject.action?uri=info:doi/10.1371/journal.pcbi.1002511.g002&amp;amp;representation=PNG_I&quot; alt=&quot;NetRank feature selection performance&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;Issues&lt;/h2&gt;

&lt;p&gt;As we discussed the article, some questions did come up.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What was the variation in &lt;em&gt;d&lt;/em&gt; depending on the size of the training set?&lt;/li&gt;
&lt;li&gt;How consistent were the genes that came out as biomarkers?

&lt;ul&gt;
&lt;li&gt;It would be nice to try this methodology on a series of independent, but
related cancer datasets (ie breast or lung cancer) and see how consistent the lists
are. This was done &lt;a href=&quot;http://www.biomedcentral.com/1471-2105/13/182/abstract&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;What happens if the genes that don&amp;#39;t pass filtering are removed from the network
entirely?&lt;/li&gt;
&lt;li&gt;Were the problems reported with not-filtering genes due to having only two
disease points (poor and good prognosis) to calculate a correlation of expression 
with?&lt;/li&gt;
&lt;li&gt;How many iterations does it take to achieve convergence?&lt;/li&gt;
&lt;li&gt;The list of genes they come up with are fairly well known cancer genes. We
were kindof surprised that they didn&amp;#39;t seem to come up novel genes associated
directly with pancreatic cancer.&lt;/li&gt;
&lt;li&gt;Why is &lt;em&gt;d&lt;/em&gt; so variable depending on the cancer examined?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Things to try&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Could we improve on this by instead of taking just the top-ranked genes, look for 
the top ranked cliques, i.e. take the top gene, remove anything in its immediate
neighborhood, and then go to the next one?&lt;/li&gt;
&lt;li&gt;What would happen if we used a directed network based on connected Reactome
or KEGG pathways?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Find this post online at: &lt;a href=&quot;http://robertmflight.blogspot.com/2012/08/journal-club-150812.html&quot;&gt;http://robertmflight.blogspot.com/2012/08/journal-club-150812.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Authored using Markdown, and the R Markdown package. Published on 15.08.12&lt;/p&gt;
</description></item>
  </channel>
</rss>
