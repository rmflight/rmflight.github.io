[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deciphering Life: One Bit at a Time",
    "section": "",
    "text": "Customizing the Displayed Date in Quarto Pubs\n\n\nMaking the date field more useful in quarto docs and publications.\n\n\n\n\nquarto\n\n\nR\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Changes in Height Over Time\n\n\nI have a citizen science project I want to try, that involves individuals measureing their own height daily over a long period of two months. I think I figured out how to do it.\n\n\n\n\nR\n\n\ndevelopment\n\n\ncitizen-science\n\n\nheight\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Academicons in Your Quarto Blog\n\n\nIf you have trouble getting the academicons in your quarto blog, this might help you too!\n\n\n\n\nrandom-code-snippets\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an Analysis With a targets Workflow\n\n\nHow I work through an -omics analysis using the targets package.\n\n\n\n\nanalysis\n\n\ndevelopment\n\n\nmass-spectrometry\n\n\ntargets\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompiling Against Python.h\n\n\nHow to make sure that cython can find python.h when using a self compiled python.\n\n\n\n\npython\n\n\ncython\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Consultation Template\n\n\nWhat our labs project consult template looks like.\n\n\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeatmap Colormaps!\n\n\nExamples of good colormaps to use for heatmaps, both divergent and incremental.\n\n\n\n\ncolormaps\n\n\nvisualization\n\n\nheatmap\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating Correlation Values from Another Manuscript\n\n\nDocumenting my journey trying to recreate some correlations calculated in another manuscript.\n\n\n\n\nreproducibility\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCairo and XQuartz in Mac GitHub Actions\n\n\nIncluding cairo and xquartz in MacOS github actions\n\n\n\n\nrandom-code-snippets\n\n\ncairo\n\n\nxquartz\n\n\ndevelopment\n\n\ngithub\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDplyr in Packages and Global Variables\n\n\nHow to include dplyr in a package, and avoid warnings around global variables.\n\n\n\n\nrandom-code-snippets\n\n\npackages\n\n\ndplyr\n\n\nrlang\n\n\nR\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPie Charts in RCy3\n\n\nHow to represent nodes as pie charts using Cytoscape and RCy3\n\n\n\n\nrandom-code-snippets\n\n\nrcy3\n\n\ncytoscape\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReusing ggplot2 Colors\n\n\nWhen you want to reuse ggplot2 default colors across plots.\n\n\n\n\nrandom-code-snippets\n\n\nggplot2\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating Self-Hosted GitLab Projects to GitHub\n\n\nWe wanted to migrate from self-hosted GitLab projects to GitHub repos. Here is some background on how we accomplished that.\n\n\n\n\nversion-control\n\n\ngithub\n\n\ngitlab\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZooming GGraph Plots\n\n\nSome code demonstrating how to zoom into portions of a ggraph.\n\n\n\n\ngraphing\n\n\nggraph\n\n\nvisualization\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR CMD Check and Non-PDF Vignettes\n\n\nR CMD Check complaining about missing files? Here was my solution.\n\n\n\n\npackages\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Vacation in Barometric Pressure\n\n\nWhat can we see from my phone’s barometric pressure readings?\n\n\n\n\ngraphing\n\n\nmaps\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Classification Using Parsnip\n\n\nHow to make sure you get a classification fit and not a probability fit from a random forest model using the tidymodels framework.\n\n\n\n\nparsnip\n\n\ntidymodels\n\n\nmachine-learning\n\n\nrandom-forest\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColoring Dendrogram Edges with ggraph\n\n\nHere is how I got edges colored in a dendrogram with ggraph. Use “node.” in front of the node data column you want.\n\n\n\n\nrandom-code-snippets\n\n\nvisualization\n\n\ngraphing\n\n\ndendrogram\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeeping Figures in an Rmd – Word Document\n\n\nHow to make your rmarkdown to word conversion also generate a directory of figures.\n\n\n\n\nrandom-code-snippets\n\n\nreproducibility\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Geographic Introduction\n\n\nAdapting Piping Hot Data’s Geographic Introduction animation for myself.\n\n\n\n\nmaps\n\n\ngraphing\n\n\nvisualization\n\n\nanimation\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProportional Error in Mass Spectrometry\n\n\nDemonstrating the existence of proportional error in mass spectrometry measurements.\n\n\n\n\nmass-spectrometry\n\n\nproportional-error\n\n\nomics\n\n\nmetabolomics\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighlighting a Row of A ComplexHeatmap\n\n\nA simple way to highlight or bring attention to a row or column in a ComplexHeatmap.\n\n\n\n\nrandom-code-snippets\n\n\nheatmap\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Map of Routes Weighted by Travel\n\n\nI made a map of my spouse’s travel since we got Google phones for her birthday last fall. Here’s how I did it.\n\n\n\n\ngraphing\n\n\nmaps\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Code Snippets\n\n\nIntroducing random-code-snippets.\n\n\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackages Don’t Work Well for Analyses in Practice\n\n\nI was wrong about using packages to structure statistical analyses. Also why I finally switched to {drake}.\n\n\n\n\nR\n\n\ndevelopment\n\n\npackages\n\n\nvignettes\n\n\nprogramming\n\n\nanalysis\n\n\nworkflow\n\n\ntargets\n\n\ndrake\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings I Learned About distill\n\n\nThe various things I learned about the distill blog setup while converting posts over from my old blogdown site.\n\n\n\n\ndistill\n\n\nblogdown\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing group_by Instead of Splits\n\n\nHow to use group_by instead of split’s to summarize things.\n\n\n\n\nR\n\n\ndplyr\n\n\nsplit\n\n\ngroup-by\n\n\nprogramming\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2020\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNarrower PDF Kable Tables\n\n\nThis is how you should make narrower kable tables in rmarkdown PDF documents.\n\n\n\n\nR\n\n\nknitr\n\n\nkable\n\n\nlearning\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2019\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing Scientific Programming\n\n\nHow and when should we get people in academia programming? What if we had a unified front across the science labs?\n\n\n\n\nR\n\n\nreproducibility\n\n\nprogramming\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2019\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComments enabled via utterances\n\n\nHow I got utterances working on blogdown.\n\n\n\n\nblogdown\n\n\ncommenting\n\n\nutterances\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2019\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparisons using for loops vs split\n\n\nfor loops often hide much of the actual logic of your code because of all the necessary boilerplate of running a loop. split-ting your data can oftentimes be clearer, and faster.\n\n\n\n\nR\n\n\nfor-loop\n\n\nsplit\n\n\npurrr\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2019\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNicer PNG Graphics\n\n\nHere are some tips for getting nicer graphics in your rmarkdown outputs.\n\n\n\n\n\n\n\n\n\nDec 6, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t do PCA After Statistical Testing!\n\n\nYou might be tempted to do PCA after a statistical test. Read more to discover why this is a bad idea.\n\n\n\n\npca\n\n\nbioinformatics\n\n\nR\n\n\nt-test\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Modes Using Kernel Density Estimates\n\n\nExamples of finding the mode of a univeriate distribution in R and Python.\n\n\n\n\nR\n\n\npython\n\n\nkernel-density\n\n\npdf\n\n\nprobability-density\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSplit - Unsplit Anti-Pattern\n\n\nGetting some speed using dplyr::join than my more intuitive split –> unsplit pattern.\n\n\n\n\nR\n\n\ndevelopment\n\n\nprogramming\n\n\npurrr\n\n\ndplyr\n\n\njoin\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing IRanges for Non-Integer Overlaps\n\n\nI wanted to make use of IRanges awesome interval logic, but for non-integer data.\n\n\n\n\nR\n\n\niranges\n\n\ndevelopment\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTurn Robert’s Beard Purple!\n\n\nI’m trying to raise money for the Walk for Alzheimer’s, will you sponsor me?\n\n\n\n\nalzheimers\n\n\nfundraising\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nknitrProgressBar Package\n\n\nEver wanted a progress bar output visible in a knitr document? Now you can!\n\n\n\n\npackages\n\n\nR\n\n\ndevelopement\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLicensing R Packages that Include Others Code\n\n\nI wanted to include others code in my package, and couldn’t find any good resources.\n\n\n\n\nR\n\n\npackages\n\n\nopen-science\n\n\nlicensing\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndocopt & Numeric Options\n\n\nEvery input is a string in docopt. Every Input!!\n\n\n\n\nR\n\n\ndevelopment\n\n\nprogramming\n\n\ndocopt\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2018\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinking to Manually Inserted Images in Blogdown / Hugo\n\n\nThis is my method to include something manually in a blogdown post.\n\n\n\n\nhugo\n\n\nR\n\n\nblogdown\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2017\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferences in Posted Date vs sessionInfo()\n\n\nIf you see differences in the sessionInfo output and the date the post was published, this is why.\n\n\n\n\ndevelopment\n\n\nblogdown\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2017\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Deployment Script\n\n\nI don’t want to use Netlify for hosting, so I came up with this simple script to deploy my blog.\n\n\n\n\nR\n\n\nblogdown\n\n\ndevelopment\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2017\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI was Part of the Problem\n\n\nWhy do so many men think it’s OK to lavish unwanted attention on women who don’t want it?\n\n\n\n\nmetoo\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2017\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriticizing a Publication, and Lying About It\n\n\nCritics of our last publication claimed we didn’t make our data available, which is an outright lie.\n\n\n\n\npublications\n\n\npeer-review\n\n\nzinc\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2017\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuthentication of Key Resources for Data Analysis\n\n\nNIH is asking for authentication of key resources. How does this apply to data analyses?\n\n\n\n\nreproducibility\n\n\nopen-science\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 23, 2016\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest vs PLS on Random Data\n\n\nComparing random-forest and partial-least-squares discriminant-analysis on random data to show the problems inherent in PLS-DA.\n\n\n\n\nrandom-forest\n\n\nmachine-learning\n\n\npartial-least-squares\n\n\nstatistics\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2015\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNovel Zinc Coordination Geometries\n\n\nA bit of an explainer on our labs recent publication on finding and classifying zinc coordination geometries in protein structures.\n\n\n\n\nzinc\n\n\nstructural-biochemistry\n\n\nopen-science\n\n\nreproducibility\n\n\nvisualization\n\n\npublications\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2015\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMouse / Human Transcriptomics and Batch Effects\n\n\nA recent paper dug into some data from another paper, casting doubts on the first, all thanks to the data being available.\n\n\n\n\nopen-science\n\n\ntranscriptomics\n\n\nbatch-effects\n\n\npublications\n\n\npeer-review\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2015\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Open Post-Publication Peer Review, with Credit!\n\n\nA story about my first open peer-review.\n\n\n\n\nopen-science\n\n\npeer-review\n\n\npublications\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2015\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeing a PhD Student and Post-Doc with Migraines\n\n\nWhat it’s like having migraines as a PhD student and PostDoc.\n\n\n\n\nphdisabled\n\n\nmigraines\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTravis-CI to GitHub Pages\n\n\nHow I automatically have some stuff get pushed to GitHub pages from a Travis CI job.\n\n\n\n\nR\n\n\nreproducibility\n\n\ntravis-ci\n\n\ngithub\n\n\npublishing\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Career Goals\n\n\nI don’t want to be a lab PI, but I want to stay in academia.\n\n\n\n\npost-doc\n\n\ncareer\n\n\nacademia\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyses as Packages\n\n\nWhy I think packages make good ways to structure an analysis.\n\n\n\n\nR\n\n\ndevelopment\n\n\npackages\n\n\nvignettes\n\n\nprogramming\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackages vs ProjectTemplate\n\n\nWhy I think packages are better than the projectTemplate package.\n\n\n\n\nR\n\n\npackages\n\n\nanalysis\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating an Analysis as a Package and Vignette\n\n\nA walkthrough creating an analysis project as a package.\n\n\n\n\nR\n\n\ndevelopment\n\n\npackages\n\n\nvignettes\n\n\nanalysis\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Job Notifications Using Twitter\n\n\n\n\n\n\n\nR\n\n\ntwitter\n\n\ndevelopment\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearcher Discoverability\n\n\nWhy do we need corporate products to enhance “researcher discoverability”?\n\n\n\n\ngithub\n\n\nopen-science\n\n\nacademia\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBioinformatics Presentations that Lack Results (or Biological Relevance)\n\n\nWhy do people have bionformatics presentations lacking relevance or results?\n\n\n\n\nbioinformatics\n\n\nacademia\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategoryCompare Paper Finally Out!\n\n\nMy first first author publication since starting my PostDoc is finally out, about my meta-annotation-enrichment software package categoyrCompare.\n\n\n\n\nR\n\n\nbioconductor\n\n\nmeta-analysis\n\n\npublications\n\n\ngit\n\n\ngithub\n\n\nopen-science\n\n\nvisualization\n\n\nannotation-enrichment\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-Written Function Help\n\n\nDo you want to be able to read function documentation for your own functions? Make your own package.\n\n\n\n\nR\n\n\npackages\n\n\ndocumentation\n\n\ndevelopment\n\n\ndevtools\n\n\nroxygen2\n\n\ndocstrings\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling MatLab vs Installing R\n\n\nPersonal frustrations around installing MatLab led to this particular rant.\n\n\n\n\nMatLab\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPackage Version Increment Pre- and Post-Commit Hooks\n\n\nTwo git commit hooks for incrementing the package version as part of commits.\n\n\n\n\nR\n\n\ngit\n\n\npackages\n\n\ndevelopment\n\n\nprogramming\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotivated Learning\n\n\nTwo personal stories on times I was very motivated to learn, as part of my Software-Carpentry instructor training.\n\n\n\n\nlearning\n\n\nsoftware-carpentry\n\n\ngit\n\n\ncalculus\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPubmedCommons API\n\n\nPubmed commons is a new commenting system for pubmed articles.\n\n\n\n\nopen-science\n\n\npubmed\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen vs Closed Analysis Languages\n\n\nTalking about R & Python vs MatLab as examples of open and closed data analysis languages.\n\n\n\n\nopen-science\n\n\nR\n\n\npython\n\n\nMatLab\n\n\nprogramming\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPre-Calculating Large Tables of Values\n\n\nDemonstrating a way to generate a large amount of numbers that otherwise might take a long time to calculate.\n\n\n\n\nR\n\n\npre-calculations\n\n\nprogramming\n\n\ndevelopment\n\n\nc++\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPortable, Personal Packages\n\n\nMy take on creating simple little packages for your own commonly used functions.\n\n\n\n\nR\n\n\npackages\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nK-12 Wants Scientists!!\n\n\nI recently attended a talk about PhD scientists in grade school teaching.\n\n\n\n\npost-doc\n\n\nacademia\n\n\nscience\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR, RStudio, and Release and Dev Bioconductor\n\n\nWorking with the development version of Bioconductor on linux can be a pain. This is one way to do it.\n\n\n\n\nR\n\n\nbioconductor\n\n\nrstudio\n\n\nprogramming\n\n\npackages\n\n\ndevelopment\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible Methods\n\n\nA short missive on reproducibility, especially within computational work.\n\n\n\n\nopen-science\n\n\nreproducibility\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Interface for Teaching\n\n\nWhat is the best interface for teaching a language like R?\n\n\n\n\nR\n\n\nteaching\n\n\nnotebooks\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTim Hortons Density\n\n\nHow far away are most Canadians from a Tim Hortons?\n\n\n\n\nR\n\n\nmapping\n\n\ntim-hortons\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStoring Package Data in Custom Environments\n\n\nHow do you keep track of stuff for your own package without cluttering the users global space or setting a bunch of options?\n\n\n\n\nR\n\n\npackages\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Up Scientific Results and Literate Programming\n\n\nMy thoughts on using literate programming to investigate and report scientific results\n\n\n\n\nliterate-programming\n\n\nacademia\n\n\nnotebooks\n\n\nreproducibility\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2013\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Papers Using R Markdown\n\n\nHow I used RMarkdown to write a manuscript\n\n\n\n\nR\n\n\nopen-science\n\n\nreproducibility\n\n\nliterate-programming\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2012\n\n\nRobert M Flight\n\n\n\n\n\n\n  \n\n\n\n\nJournal Club 2012-08-15\n\n\nA summary of the paper Google Goes Cancer as was discussed in our journal club.\n\n\n\n\njournal-club\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2012\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Custom CDFs for Affymetrix Chips in Bioconductor\n\n\nExamples of messing with Affymetrix CDF data in Bioconductor.\n\n\n\n\nR\n\n\nbioconductor\n\n\nbioinformatics\n\n\ncdf\n\n\naffymetrix\n\n\nmicroarray\n\n\nrandom-code-snippets\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2012\n\n\nRobert M Flight\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a place for me to share analysis tips and tricks, musings on open and reproducible science, as well as recent publications, and whatever is on my mind. You are likely to find personal analysis projects, as well as my musings on R code and different types of algorithms.\nAll opinions are my own and do not reflect those of my employer or the university where I work."
  },
  {
    "objectID": "about.html#me",
    "href": "about.html#me",
    "title": "About",
    "section": "Me",
    "text": "Me\nI’m a senior research associate in bioinformatics at the Moseley Bioinformatics and Systems Biology Lab, which is part of the Markey Cancer Center at the University of Kentucky. I work with our collaborators to analyse a variety of -omics data, including transcriptomics and metabolomics using both previously established and newly developed methods.\nI have been analyzing -omics type data since 2004, and programming in R since 2010. I authored the categoryCompare Bioconductor package for comparing -omics data using annotations.\nYou can find my publications (and other research outputs) on my ORCID profile.\n\nInterests\nOpen Science | Bioinformatics | Metabolomics | Structural Biology | Systems Biology\n\n\nEducation\nPhD | Chemistry | 2009 | Dalhousie University\nMSc | Chemistry | 2004 | University of New Brunswick\nBSc | Biology-Chemistry | 2002 | University of New Brunswick"
  },
  {
    "objectID": "random_code_snippets.html",
    "href": "random_code_snippets.html",
    "title": "Random Code Snippets",
    "section": "",
    "text": "Customizing the Displayed Date in Quarto Pubs\n\n\nMaking the date field more useful in quarto docs and publications.\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nHeatmap Colormaps!\n\n\nExamples of good colormaps to use for heatmaps, both divergent and incremental.\n\n\n\n\n\n\n\n\n\nJan 31, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nCairo and XQuartz in Mac GitHub Actions\n\n\nIncluding cairo and xquartz in MacOS github actions\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nPie Charts in RCy3\n\n\nHow to represent nodes as pie charts using Cytoscape and RCy3\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nDplyr in Packages and Global Variables\n\n\nHow to include dplyr in a package, and avoid warnings around global variables.\n\n\n\n\n\n\n\n\n\nJan 6, 2022\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nReusing ggplot2 Colors\n\n\nWhen you want to reuse ggplot2 default colors across plots.\n\n\n\n\n\n\n\n\n\nDec 21, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nZooming GGraph Plots\n\n\nSome code demonstrating how to zoom into portions of a ggraph.\n\n\n\n\n\n\n\n\n\nNov 11, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nR CMD Check and Non-PDF Vignettes\n\n\nR CMD Check complaining about missing files? Here was my solution.\n\n\n\n\n\n\n\n\n\nOct 27, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nRandom Forest Classification Using Parsnip\n\n\nHow to make sure you get a classification fit and not a probability fit from a random forest model using the tidymodels framework.\n\n\n\n\n\n\n\n\n\nAug 30, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nColoring Dendrogram Edges with ggraph\n\n\nHere is how I got edges colored in a dendrogram with ggraph. Use “node.” in front of the node data column you want.\n\n\n\n\n\n\n\n\n\nAug 3, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nKeeping Figures in an Rmd – Word Document\n\n\nHow to make your rmarkdown to word conversion also generate a directory of figures.\n\n\n\n\n\n\n\n\n\nJul 22, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nHighlighting a Row of A ComplexHeatmap\n\n\nA simple way to highlight or bring attention to a row or column in a ComplexHeatmap.\n\n\n\n\n\n\n\n\n\nMar 26, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nRandom Code Snippets\n\n\nIntroducing random-code-snippets.\n\n\n\n\n\n\n\n\n\nMar 5, 2021\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nCustom Deployment Script\n\n\nI don’t want to use Netlify for hosting, so I came up with this simple script to deploy my blog.\n\n\n\n\n\n\n\n\n\nDec 27, 2017\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nPackage Version Increment Pre- and Post-Commit Hooks\n\n\nTwo git commit hooks for incrementing the package version as part of commits.\n\n\n\n\n\n\n\n\n\nFeb 7, 2014\n\n\nRobert M Flight\n\n\n\n\n\n\n\n\nCreating Custom CDFs for Affymetrix Chips in Bioconductor\n\n\nExamples of messing with Affymetrix CDF data in Bioconductor.\n\n\n\n\n\n\n\n\n\nJul 13, 2012\n\n\nRobert M Flight\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-12-06-nicer-png-graphics/index.html",
    "href": "posts/2018-12-06-nicer-png-graphics/index.html",
    "title": "Nicer PNG Graphics",
    "section": "",
    "text": "If you are getting crappy looking png images from rmarkdown html or word documents, try using type='cairo' or dev='CairoPNG' in your chunk options."
  },
  {
    "objectID": "posts/2018-12-06-nicer-png-graphics/index.html#png-graphics",
    "href": "posts/2018-12-06-nicer-png-graphics/index.html#png-graphics",
    "title": "Nicer PNG Graphics",
    "section": "PNG Graphics??",
    "text": "PNG Graphics??\nSo, I write a lot of reports using rmarkdown and knitr, and have been using knitr for quite a while. My job involves doing analyses for collaborators and communicating results. Most of the time, I will generate a pdf report, and I get beautiful graphics, thanks to the eps graphics device. However, there are times when I want to generate either word or html reports, and in those cases, I tend to get very crappy looking graphics. See this example image below:\n\nlibrary(ggplot2)\np = ggplot(mtcars, aes(mpg, wt)) +\n  geom_point(size = 3) +\n  labs(x=\"Fuel efficiency (mpg)\", y=\"Weight (tons)\",\n       title=\"Seminal ggplot2 scatterplot example\",\n       subtitle=\"A plot that is only useful for demonstration purposes\",\n       caption=\"Brought to you by the letter 'g'\")\np\n\n\n\n\nNote: This was generated on self-compiled R under Ubuntu 16.04. As we can see, knitr is using the png device, because we are generating html output.\n\nknitr::opts_chunk$get(\"dev\")\n\n[1] \"png\""
  },
  {
    "objectID": "posts/2018-12-06-nicer-png-graphics/index.html#increased-resolution",
    "href": "posts/2018-12-06-nicer-png-graphics/index.html#increased-resolution",
    "title": "Nicer PNG Graphics",
    "section": "Increased Resolution",
    "text": "Increased Resolution\nOf course, we just need to increase the resolution! So let’s do so. Just to go whole hog on this, let’s increase it to 300!\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, 300 dpi\")\n\n\n\n\nIf you compare this one to the previous, you can see that the quality is marginally better, but doesn’t seem to be anything like what you should be able to get."
  },
  {
    "objectID": "posts/2018-12-06-nicer-png-graphics/index.html#use-svg",
    "href": "posts/2018-12-06-nicer-png-graphics/index.html#use-svg",
    "title": "Nicer PNG Graphics",
    "section": "Use SVG??",
    "text": "Use SVG??\nAlternatively, we could tell knitr to use the svg device instead! Vector graphics always look nice!\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, dev = 'svg'\")\n\n\n\n\nIt’s so crisp! But, for word documents especially, this could be a problem, as the images might not show up. The nice thing about png is it should be usable in just about any format!\nAnd, if you have a plot with a lot of points (> 200), the svg will start to take up some serious disk space, as every single point is encoded in the svg file. This is also a good reason to use png."
  },
  {
    "objectID": "posts/2018-12-06-nicer-png-graphics/index.html#png-via-cairo",
    "href": "posts/2018-12-06-nicer-png-graphics/index.html#png-via-cairo",
    "title": "Nicer PNG Graphics",
    "section": "PNG via Cairo",
    "text": "PNG via Cairo\nAfter pulling out my hair yesterday as I tried to generate nice png images embedded in a word report (and settling on converting every figure from svg to png and saving to a folder to pass on, see this), I finally decided to try a different device.\nNow, your R installation does need to have either cairo capabilities, or be able to use the Cairo package. Mine has both.\n\ncapabilities()\n\n       jpeg         png        tiff       tcltk         X11        aqua \n       TRUE        TRUE        TRUE        TRUE        TRUE       FALSE \n   http/ftp     sockets      libxml        fifo      cledit       iconv \n       TRUE        TRUE       FALSE        TRUE       FALSE        TRUE \n        NLS       Rprof     profmem       cairo         ICU long.double \n       TRUE        TRUE       FALSE        TRUE        TRUE        TRUE \n    libcurl \n       TRUE \n\npackageVersion(\"Cairo\")\n\n[1] '1.6.0'\n\n\nLet’s change the device (two different ways) and plot it again. First, we will still use the png device, but add the type = \"cairo\" argument (see ?png). Just for information, that looks like the below in the chunk options:\nr plot_cairo, dev.args = list(type = \"cairo\")\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, type = 'cairo'\")\n\n\n\n\nWow! This looks great! So much nicer than the other device. Secondly, let’s use the CairoPNG device (dev = \"CairoPNG\")\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, dev = 'CairoPNG'\")\n\n\n\n\nFinally, we can also increase the resolution as well.\n\np + ggtitle(\"Seminal ggplot2 scatterplot example, dev = 'CairoPNG', dpi = 300\")\n\n\n\n\nSo there you have it. Very crisp png images, with higher resolutions if needed, and no jaggedness, without resorting to conversion via inkscape (my previous go to)."
  },
  {
    "objectID": "posts/2018-12-06-nicer-png-graphics/index.html#incorporating-into-reports",
    "href": "posts/2018-12-06-nicer-png-graphics/index.html#incorporating-into-reports",
    "title": "Nicer PNG Graphics",
    "section": "Incorporating Into Reports",
    "text": "Incorporating Into Reports\nAs I previously mentioned, I often default to pdf reports, but will then generate a word or html report if necessary. How do you avoid changing the options even in a setup chunk if you want this to happen every time you specify word_document as the output type? This is what I settled on, the setup chunk checks the output type (based on being called from rmarkdown::render), and sets it appropriately.\nif (knitr::opts_knit$get(\"rmarkdown.pandoc.to\") != \"latex\") {\n  knitr::opts_chunk$set(dpi = 300, dev.args = list(type = \"cairo\"))\n})"
  },
  {
    "objectID": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html",
    "href": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html",
    "title": "My Vacation in Barometric Pressure",
    "section": "",
    "text": "I recently took a proper vacation, that involved driving from Lexington, KY, USA to Digby, NS, Canada and all points in between. I also have an app, Barometer Reborn, on my phone that measures the air pressure every 15 minutes or so. This data is useful for someone who suffers from pressure induced migraines. I decided it would be interesting to examine the air pressure readings over the course of my vacation, given that we crossed an extremely wide variety of terrain (and weather) in our travels.\nSo lets see what kinds of things we can see. A big caveat with this data is that we are essentially recording local pressure, a combination of pressure changes from both elevation and weather changes, as I don’t have the correction for elevation turned on. I think it will still be interesting to examine whats in here."
  },
  {
    "objectID": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#load-data",
    "href": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#load-data",
    "title": "My Vacation in Barometric Pressure",
    "section": "Load Data",
    "text": "Load Data\nI’ve previously uploaded data to Google Drive from my phone and downloaded it. Although it says “csv”, it turns out it’s actually tab-separated. I also do the conversion to a date-time format explicitly here, because it was easier than parsing the date format that the app uses.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(ggforce)\ntheme_set(cowplot::theme_cowplot())\nbarometer_readings = read.table(here::here(\"data_files/barometer_2021-09-17.csv\"), sep = \"\\t\", header = TRUE) %>%\n  dplyr::mutate(pressure = Pressure..mBar.,\n                datetime = lubridate::as_datetime(Timestamp..Unix.Time. / 1000, tz = \"America/New_York\")) %>%\n  dplyr::filter(datetime >= \"2021-08-21\", datetime <= \"2021-09-15\")"
  },
  {
    "objectID": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#initial-plot",
    "href": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#initial-plot",
    "title": "My Vacation in Barometric Pressure",
    "section": "Initial Plot",
    "text": "Initial Plot\nWe can plot the whole thing, with the pressure at sea level as well.\n\nggplot(barometer_readings, aes(x = datetime,\n                               y = pressure)) +\n  geom_line(size = 1.5) +\n  geom_hline(yintercept = 1013.25, color = \"red\", size = 1.5, alpha = 0.5) +\n  labs(x = \"Date\", y = \"Pressure (mBar)\")\n\n\n\n\nThis is kinda cool. At both ends it’s easy to see the passage there and back through the mountains (not big ones, but still)! It’s also easy to see that outside of those, I spent a lot of time near sea level, which is expected from Bangor, ME on."
  },
  {
    "objectID": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#annotations",
    "href": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#annotations",
    "title": "My Vacation in Barometric Pressure",
    "section": "Annotations",
    "text": "Annotations\nLet’s annotate it! We can come up with some simple annotations, like the driving sections, and then when we were in a location, and add those to the plot. The (1) and (2) labels are to keep from having weird plot artifacts appearing in the line plot.\n\nbarometer_readings = barometer_readings %>%\n  dplyr::mutate(location =\n   dplyr::case_when(\n     \n     datetime <= \"2021-08-22 08:00\" ~ \"Lexington - Bangor\",\n     datetime <= \"2021-08-23 08:00\" ~ \"Waiting on Covid Test\",\n     datetime <= \"2021-08-23 19:00\" ~ \"Bangor - Digby\",\n     datetime <= \"2021-08-28 08:00\" ~ \"Digby 1\",\n     datetime <= \"2021-08-28 14:00\" ~ \"Daytrip to Liverpool\",\n     datetime <= \"2021-09-03 10:00\" ~ \"Digby 2\",\n     datetime <= \"2021-09-03 16:30\" ~ \"Digby - Burton\",\n     datetime <= \"2021-09-08 09:00\" ~ \"Burton 1\",\n     datetime <= \"2021-09-08 20:00\" ~ \"Daytrip to Moncton\",\n     datetime <= \"2021-09-13 08:00\" ~ \"Burton 2\",\n     datetime <= \"2021-09-15\" ~ \"Burton - Lexington\"\n   ),\n   day = lubridate::as_date(datetime))\n\n\nbig_plot = ggplot(barometer_readings, aes(x = datetime, y = pressure, color = location)) + \n  geom_line(size = 1.5) +\n  theme(legend.position = c(0.2, 0.3)) +\n  labs(x = \"Date\", y = \"Pressure (mBar)\")\nbig_plot\n\n\n\nbig_plot2 = big_plot + theme(legend.position = \"none\")\n\nWe can plot some of these as subsets and zoom in on them.\n\nbig_plot2 + \n  facet_zoom(x = location %in% c(\"Lexington - Bangor\", \"Waiting on Covid Test\", \"Bangor - Digby\")) +\n  labs(caption = \"Going from Lexington to Bangor, awaiting test results, and then driving to Digby.\")\n\n\n\n\n\nbig_plot2 +\n  facet_zoom(x = location %in% c(\"Digby 1\", \"Daytrip to Liverpool\", \"Digby 2\")) +\n  labs(caption = \"Digby, with a trip across land to Liverpool.\")\n\n\n\n\n\nbig_plot2 +\n  facet_zoom(x = location %in% c(\"Digby - Burton\", \"Burton 1\", \"Daytrip to Moncton\", \"Burton 2\")) +\n  labs(caption = \"Traveling from Digby to Burton, a daytrip to Moncton, and remainder in Burton.\")\n\n\n\n\nYou can actually see towards the tail end of our stay in Burton, the big increase in local pressure from a high-front that gave me a massive migraine and kept me home from activities with the family. Thankfully that was the only one.\n\nbig_plot2 +\n  facet_zoom(x = as.character(day) %in% c(\"2021-09-10\", \"2021-09-11\")) +\n  labs(caption = \"Migraine day, stayed home.\")\n\n\n\n\n\nbig_plot2 +\n  facet_zoom(x = location %in% c(\"Burton - Lexington\")) +\n  labs(caption = \"Traveling home to Lexington from Burton over 2 days.\")"
  },
  {
    "objectID": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#conclusions",
    "href": "posts/2021-09-20-my-vacation-in-barometric-pressure/index.html#conclusions",
    "title": "My Vacation in Barometric Pressure",
    "section": "Conclusions",
    "text": "Conclusions\nI dunno, honestly. This would probably be more interesting if it was one or the other of changes due to altitude, or due to changes in the weather with an adjustment for the altitude. Many of the dips are due to changes occurring because of a change in altitude, with the biggest ones noted as we drove through the Appalachian mountains."
  },
  {
    "objectID": "posts/2019-02-13-for-loops-vs-split/index.html",
    "href": "posts/2019-02-13-for-loops-vs-split/index.html",
    "title": "Comparisons using for loops vs split",
    "section": "",
    "text": "Sometimes for loops are useful, and sometimes they shouldn’t really be used, because they don’t really help you understand your data, and even if you try, they might still be slow(er) than other ways of doing things."
  },
  {
    "objectID": "posts/2019-02-13-for-loops-vs-split/index.html#comparing-groups",
    "href": "posts/2019-02-13-for-loops-vs-split/index.html#comparing-groups",
    "title": "Comparisons using for loops vs split",
    "section": "Comparing Groups",
    "text": "Comparing Groups\nI have some code where I am trying to determine duplicates of a group of things. This data looks something like this:\n\ncreate_random_sets = function(n_sets = 1000){\n  set.seed(1234)\n  \n  sets = purrr::map(seq(5, n_sets), ~ sample(seq(1, .x), 5))\n  \n  item_sets = sample(seq(1, length(sets)), 10000, replace = TRUE)\n  item_mapping = purrr::map2_df(item_sets, seq(1, length(item_sets)), function(.x, .y){\n    data.frame(v1 = as.character(.y), v2 = sets[[.x]], stringsAsFactors = FALSE)\n  })\n  item_mapping\n}\nlibrary(dplyr)\nmapped_items = create_random_sets()\n\nhead(mapped_items, 20)\n\n   v1  v2\n1   1 375\n2   1 255\n3   1 268\n4   1  52\n5   1 241\n6   2 143\n7   2 401\n8   2 127\n9   2 372\n10  2 100\n11  3  62\n12  3 109\n13  3  72\n14  3 390\n15  3  94\n16  4  57\n17  4  55\n18  4 147\n19  4 236\n20  4 120"
  },
  {
    "objectID": "posts/2019-02-13-for-loops-vs-split/index.html#looping",
    "href": "posts/2019-02-13-for-loops-vs-split/index.html#looping",
    "title": "Comparisons using for loops vs split",
    "section": "Looping",
    "text": "Looping\nIn this case, every item in v1 has 5 things in v2. I really want to group multiple things of v1 that have the same combination of things in v2. My initial function to do this splits everything in v2 by v1, and then compares all the splits to each other, removing things that have been compared and found to be the same, and saving them as we go. This required two loops, basically while there was data to check, check all the other things left in the list against it (the for). Pre-initialize the list of things that are identical to each other so we don’t take a hit on allocation, and delete the things that have been checked or noted as identical. Although the variable names are changed, the code for that function is below.\n\nloop_function = function(item_mapping){\n  split_items = split(item_mapping$v2, item_mapping$v1)\n  \n  matched_list = vector(\"list\", length(split_items))\n  \n  save_item = 1\n  save_index = 1\n  \n  while (length(split_items) > 0) {\n    curr_item = names(split_items)[save_item]\n    curr_set = split_items[[save_item]]\n    \n    for (i_item in seq_along(split_items)) {\n      if (sum(split_items[[i_item]] %in% curr_set) == length(curr_set)) {\n        matching_items = unique(c(curr_item, names(split_items)[i_item]))\n        save_item = unique(c(save_item, i_item))\n      }\n    }\n    matched_list[[save_index]] = curr_set\n    split_items = split_items[-save_item]\n    save_index = save_index + 1\n    save_item = 1\n  }\n  \n  n_in_set = purrr::map_int(matched_list, length)\n  matched_list = matched_list[n_in_set > 0]\n  n_in_set = n_in_set[n_in_set > 0]\n  matched_list\n}\n\nThe code works, but it doesn’t really make me think about what it’s doing, the two loops hide the fact that what is really going on is comparing things to one another. Miles McBain recently posted on this fact, that loops can be necessary, but one should really think about whether they are really necessary, or do they hide something about the data, and can we think about different ways to do the same thing.\nThis made me realize that what I really wanted to do was split the items in v1 by the unique combinations of things in v2, because split will group things together nicely for you, without any extra work. But I don’t have those combinations in a way that split can use them. So my solution is to iterate over the splits using purrr, create a representation of the group as a character value, and then call split again at the very end based on the character representation.\n\nsplit_function = function(item_mapping){\n  mapped_data = split(item_mapping$v2, item_mapping$v1) %>%\n    purrr::map2_dfr(., names(.), function(.x, .y){\n      set = unique(.x)\n      tmp_frame = data.frame(item = .y, set_chr = paste(set, collapse = \",\"), stringsAsFactors = FALSE)\n      tmp_frame$set = list(set)\n      tmp_frame\n    })\n  matched_list = split(mapped_data, mapped_data$set_chr)\n}\n\nNot only is the code cleaner, the grouping is explicit (as long as you know how split works), and its also 4x faster!\n\nmicrobenchmark::microbenchmark(\n  loop_function(mapped_items),\n  split_function(mapped_items),\n  times = 5\n)\n\nUnit: seconds\n                         expr       min        lq      mean    median        uq\n  loop_function(mapped_items) 10.672729 10.694685 10.914005 10.872513 10.984026\n split_function(mapped_items)  3.011281  3.013769  3.082842  3.089458  3.131173\n      max neval\n 11.34607     5\n  3.16853     5"
  },
  {
    "objectID": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html",
    "href": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html",
    "title": "Proportional Error in Mass Spectrometry",
    "section": "",
    "text": "The other day, Kareem Carr asked for a statistics / data science opinion that results in the daggers being drawn on you (Carr, n.d.), and I replied (Flight, n.d.):\n\nEvery physical analytical measurement in -omics suffers from some proportional error. Either model it (neg binomial in RNA-seq for example), or transform your data appropriately (log-transform).\nThis includes DNA microarrays, RNA-seq, Mass Spec, and NMR (I need data to confirm)\n\nI wanted to give some proof of what I’m talking about, because I don’t think enough people understand or care. For example, if you get mass-spec data from Metabolon, their Surveyor tool defaults to using non-log-transformed data. Trust me, you should log-transform the values."
  },
  {
    "objectID": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html#example-data",
    "href": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html#example-data",
    "title": "Proportional Error in Mass Spectrometry",
    "section": "Example Data",
    "text": "Example Data\nI have some direct-injection mass spec data on a polar fraction examining ECF derivatized amino-acids, with multiple “scans” from a Thermo Fisher Fusion Tribrid Mass Spectrometer. Each scan is the product of a small number of micro-scans. However, based on the spectrometer, and the lack of chromatography, it would not be unreasonable to expect that each scan is essentially a “replicate” of the other scans, so comparing one to any other is reasonable.\nI’m going to load up the data, and plot two scans in “raw” space. The data are already log10 transformed, so we will “untransform” it back to “raw” first before plotting it.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nscan_data = readRDS(here::here(\"data_files/mass_spec_example_data.rds\"))\nraw_data = purrr::map_dfc(scan_data, ~ 10^.x)\n\nraw_data %>%\n  dplyr::filter(scan174 <= 1e7) %>%\nggplot(aes(x = scan174, y = scan175)) + \n  geom_point() +\n  coord_equal() +\n  labs(subtitle = \"Scan-Scan Intensities, Raw Values\")\n\n\n\n\nAs you can see here, the points really start to diverge as they cross the line at 2.5 x 10^6.\nNow lets plot the log10 transformed values.\n\nscan_data %>%\n  dplyr::filter(scan174 <= 1e7) %>%\nggplot(aes(x = scan174, y = scan175)) + \n  geom_point() +\n  coord_equal() +\n  labs(subtitle = \"Scan-Scan Intensities, Log10 Values\")\n\n\n\n\nAnd now everything is coming to a point as the intensity increases.\nIf you’ve worked with microarray or RNASeq data, this is also commonly seen in those data.\nTo show the presence of the proportional error more generally, we can calculate the mean and variance of each point across the scans, and plot those. To make sure we are getting “good” representations of the data, we will only use points that were present in at least 20 scans.\n\npoint_mean = rowMeans(raw_data, na.rm = TRUE)\npoint_sd = apply(raw_data, 1, sd, na.rm = TRUE)\nn_present = apply(raw_data, 1, function(.x){sum(!is.na(.x))})\n\nmean_sd = data.frame(mean = point_mean,\n                     sd = point_sd,\n                     n = n_present)\nmean_sd %>%\n  dplyr::filter(n >= 20) %>%\n  ggplot(aes(x = mean, y = sd)) + geom_point() +\n  labs(subtitle = \"Standard Deviation vs Mean\")\n\n\n\n\nThere you have it, the standard deviation is increasing with the mean, and doing so in two different ways, one increasing slowly, and one increasing quickly. Either of these are not good. Whether increasing slowly or quickly, the point is that the error / variance / standard deviation is somehow dependent on the mean, which most standard statistical methods do not handle well. This actually drove the whole development of using the negative-binomial distribution in RNASeq!"
  },
  {
    "objectID": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html#why",
    "href": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html#why",
    "title": "Proportional Error in Mass Spectrometry",
    "section": "Why??",
    "text": "Why??\nWhy does this happen? Try this thought experiment:\n\nI take 5 people in a room, and ask you to count how many people are in the room in 60 seconds and give me an answer.\nI ask you to do this 20 times.\nYour answer should be 5 each time, right?\nNow I put 10 people in the room, and ask again how many are there. And ask multiple times.\nNow 20 people in the same size room.\nAnd then 30 people …\nAnd then 40 people …\nAnd so on.\n\nIf you think about it, given an constantly sized room, it will get harder and harder to count the people in it as their number increases, and with repeated counting, your estimates are likely to have more and more variance or a higher standard deviation as the number of people goes up. Thus the “error” is proportional or depends on the actual number of things you are estimating, much like the mass spec data above.\nIn DNA microarrays and RNA-seq, the technology was measuring photons. The higher the number of photons, the more difficult it is to be sure how many there are. And that gets translated to any downstream quantity based on the number of photons.\nIn Fourier-transform mass spectrometry, the instrument is still trying to quantify “how many” of each ion there is, and the more ions, the more difficult it is to quantify them. And we end up with proportional error.\nI think in any technology that is trying to “count” how many of something there is within a finite space, these properties will manifest themselves. You just have to know how to look."
  },
  {
    "objectID": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html#solution",
    "href": "posts/2021-04-09-proportional-error-in-mass-spectrometry/index.html#solution",
    "title": "Proportional Error in Mass Spectrometry",
    "section": "Solution",
    "text": "Solution\nIn the short term, transform your data using log’s. In the long term, I think we need more biostatisticians working with more mass-spectrometry and NMR data to determine what the error structure is for more instruments and analytical measurement types."
  },
  {
    "objectID": "posts/2018-02-19-knitrprogressbar/index.html",
    "href": "posts/2018-02-19-knitrprogressbar/index.html",
    "title": "knitrProgressBar Package",
    "section": "",
    "text": "If you like dplyr progress bars, and wished you could use them everywhere, including from within Rmd documents, non-interactive shells, etc, then you should check out knitrProgressBar (cran github)."
  },
  {
    "objectID": "posts/2018-02-19-knitrprogressbar/index.html#why-yet-another-progress-bar",
    "href": "posts/2018-02-19-knitrprogressbar/index.html#why-yet-another-progress-bar",
    "title": "knitrProgressBar Package",
    "section": "Why Yet Another Progress Bar??",
    "text": "Why Yet Another Progress Bar??\nI didn’t set out to create another progress bar package. But I really liked dplyrs style of progress bar, and how they worked under the hood (thanks to the examples from Bob Rudis).\nAs I used them, I noticed that no progress was displayed if you did rmarkdown::render() or knitr::knit(). That just didn’t seem right to me, as that means you get no progress indicator if you want to use caching facilities of knitr. So this package was born."
  },
  {
    "objectID": "posts/2018-02-19-knitrprogressbar/index.html#how",
    "href": "posts/2018-02-19-knitrprogressbar/index.html#how",
    "title": "knitrProgressBar Package",
    "section": "How??",
    "text": "How??\nThese are pretty easy to setup and use.\n\nlibrary(knitrProgressBar)\n\n# borrowed from example by @hrbrmstr\narduously_long_nchar <- function(input_var, .pb=NULL) {\n  \n  update_progress(.pb) # function from knitrProgressBar\n  \n  Sys.sleep(0.01)\n  \n  nchar(input_var)\n  \n}\n\n# using stdout() here so progress is part of document\npb <- progress_estimated(26, progress_location = stdout())\n\npurrr::map(letters, arduously_long_nchar, .pb = pb)\n\n\n|======                                                | 12% ~0 s remaining     \n|================                                      | 31% ~0 s remaining     \n|===========================                           | 50% ~0 s remaining     \n|=====================================                 | 69% ~0 s remaining     \n|===============================================       | 88% ~0 s remaining     \nCompleted after 0 s                                                             \n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1\n\n[[3]]\n[1] 1\n\n[[4]]\n[1] 1\n\n[[5]]\n[1] 1\n\n[[6]]\n[1] 1\n\n[[7]]\n[1] 1\n\n[[8]]\n[1] 1\n\n[[9]]\n[1] 1\n\n[[10]]\n[1] 1\n\n[[11]]\n[1] 1\n\n[[12]]\n[1] 1\n\n[[13]]\n[1] 1\n\n[[14]]\n[1] 1\n\n[[15]]\n[1] 1\n\n[[16]]\n[1] 1\n\n[[17]]\n[1] 1\n\n[[18]]\n[1] 1\n\n[[19]]\n[1] 1\n\n[[20]]\n[1] 1\n\n[[21]]\n[1] 1\n\n[[22]]\n[1] 1\n\n[[23]]\n[1] 1\n\n[[24]]\n[1] 1\n\n[[25]]\n[1] 1\n\n[[26]]\n[1] 1\n\n\nThe main difference to dplyrs progress bars is that here you have the option to set where the progress gets written to, either automatically using the built-in make_kpb_output_decisions(), or directly. Also, I have provided the update_progress function to actually do the updating or finalizing properly.\nThere are also package specific options to control how the decisions are made.\nSee the main documentation, as well as the included vignette."
  },
  {
    "objectID": "posts/2018-02-19-knitrprogressbar/index.html#multi-processing",
    "href": "posts/2018-02-19-knitrprogressbar/index.html#multi-processing",
    "title": "knitrProgressBar Package",
    "section": "Multi-Processing",
    "text": "Multi-Processing\nAs of V1.1.0 (should be on CRAN soon), the package also supports indicating progress on multi-processed jobs. See the included vignette for more information.\nBy the way, I know this method is not ideal, but I could not get the combination of later and processx to work in my case. If anyone is willing to help out, that would be great."
  },
  {
    "objectID": "posts/2014-02-20-self-written-function-help/index.html",
    "href": "posts/2014-02-20-self-written-function-help/index.html",
    "title": "Self-Written Function Help",
    "section": "",
    "text": "I have noted at least one instance (and there are probably others) about how Python’s docStrings are so great, and wouldn’t it be nice to have a similar system in R. Especially when you can have your new function tab completion available depending on your development environment.\nThis is a false statement, however. If you set up your R development environment properly, you can have these features available in R. It will take a little bit of work, however.\nThis approach heavily depends on devtools and roxygen2."
  },
  {
    "objectID": "posts/2014-02-20-self-written-function-help/index.html#rstudio",
    "href": "posts/2014-02-20-self-written-function-help/index.html#rstudio",
    "title": "Self-Written Function Help",
    "section": "RStudio",
    "text": "RStudio\nI do recommend using the RStudio IDE, as much of what I am discussing is very much integrated into the IDE itself. However, much of what I discuss is applicable regardless of what development environment you use."
  },
  {
    "objectID": "posts/2014-02-20-self-written-function-help/index.html#packages",
    "href": "posts/2014-02-20-self-written-function-help/index.html#packages",
    "title": "Self-Written Function Help",
    "section": "Packages",
    "text": "Packages\nFirst off, you probably want to put your functions into packages. Really, it’s not that hard. A DESCRIPTION file, NAMESPACE, and an R directory with your function definitions. If you are using RStudio, then you can create a new project as a package. Alternatively, you can set up a new package directory using package.skeleton.\nIf you are using RStudio, I recommend setting your package options:\n\nGenerate package documentation with roxygen (roxygen2)\n\nSelect all the options (especially regenerate Rd files on Build & Reload)\n\n\nWhenever you make changes to your package functions, you simply commit (you are using version control, right??), and then Build & Reload (if using RStudio) or install() if using devtools."
  },
  {
    "objectID": "posts/2014-02-20-self-written-function-help/index.html#roxygen2-documentation",
    "href": "posts/2014-02-20-self-written-function-help/index.html#roxygen2-documentation",
    "title": "Self-Written Function Help",
    "section": "Roxygen2 Documentation",
    "text": "Roxygen2 Documentation\nFor documentation that lives with your functions, I heavily recommend using roxygen2. Although the normal way to document stuff in R is through the use of Rd files, roxygen2 allows you to put the following in your R\\functions.r file:\n#' this is a silly function\n#' @param input1 this is an input to our function\n#' @param input2 this is another input\n#' @return some value\n#' @export\nsillyFunction <- function(input1, input2){\n  FunctionBody\n}\nWhen you do Build & Reload (or install), the required Rd file will be generated automatically, and upon Reloading the package, you will have full access to your documentation, and tab completion of your new function, along with descriptions of the parameters if you are using RStudio. Note, if you are not using RStudio, then you should do document to re-generate Rd files prior to doing install.\nThis particular workflow is how I now work in R, for almost every project that includes any self written functions, including analysis projects. Why I use this (and not another format such as ProjectTemplate) is another post, hopefully soon.\nUpdated 2014.02.21: Note that the original post mentioned roxygen instead of roxygen2. Thanks Carl for pointing that out. You should use roxygen2 for documentation."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html",
    "title": "Novel Zinc Coordination Geometries",
    "section": "",
    "text": "Currently available methods to discover metal geometries make too many assumptions. We were able to discover novel zinc coordination geometries using a less-biased method that makes fewer assumptions. These novel geometries seem to also have specific functionality. This work was recently published under an #openaccess license in Proteins Journal (Yao et al. 2015)."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#zinc-coordination",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#zinc-coordination",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Zinc Coordination",
    "text": "Zinc Coordination\nAs I’m sure many people know, zinc is a very important metal for biology. It is one of the most numerous metal ions, and plays a role in many different types of proteins. Zinc ions in protein structures can have anywhere from 4 to 6 ligands, in many different coordination geometries.\n\n\n\n\n\nFig 1 from (Yao et al. 2015)\n\n\n\n\nHow the zinc ion is coordinated is going to depend on the protein sequence of amino acids that surround it, and knowledge of the protein sequence should enable knowledge of how the zinc ions are coordinated. Therefore, being able to characterize zinc ion coordination geometries (CGs) from structural data (such as protein structures in the world-wide protein data bank) and associate them with protein sequences is very important."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#previous-attempts",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#previous-attempts",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Previous Attempts",
    "text": "Previous Attempts\nOther groups had done this previously, and we now have hidden-markov models for determining zinc binding thanks to this work. However, in all the cases that we could find, the determination of CG was made by comparison to previously known geometries that have been described from zinc-compound crystal structures.\nWhen the biologically based CGs are compared to previously known, there will be a bunch of CGs that are outliers or will remain unclassified."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#our-initial-attempt",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#our-initial-attempt",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Our Initial Attempt",
    "text": "Our Initial Attempt\nWhen Sen (the lead author, currently a third year PhD student in the University of Louisville Bioinformatics program) first tried to use bootstrapping and a expectation-maximization algorithm to automatically classify the zinc CGs, she started getting rather funny results, as in, why is the algorithm not converging and we are getting these huge variances in the various measures that characterize the CGs.\nA single visualization was key in unlocking what was going on. Figure 2 in the paper is a histogram of the minimum angle (where angle is ligand-zinc-ligand) in degrees.\n\n\n\n\n\nFig 2 from (Yao et al. 2015)\n\n\n\n\nBased on Figure 1, we would expect the minimum angle to be 90, but as you can see in Figure 2, there are an awful lot of angles less than 90. We are very sure that they are real given the definition of zinc-ligand bond-length that was used to define potential ligands, and the statistical decision used to define how many ligands a given zinc ion has.\nSo we have a bunch of zinc-ions that make ligand-zinc-ligand bond angles at 60, and even 30 degrees! It turns out that these small angles are largely (but not all) due to bidentate ligands, where for example the zinc ion forms bonds with two oxygen’s from an aspartate or glutamate amino acid."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#separate-out-compressed-and-cluster",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#separate-out-compressed-and-cluster",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Separate Out Compressed and Cluster!",
    "text": "Separate Out Compressed and Cluster!\nTo make the problem tractable, we first have to separate out the compressed angle zinc sites, and then we can do clustering on the set of ligand-zinc-ligand angles to determine in a mostly un-biased fashion what CGs are present and which zinc-site belongs to which CG."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#clustering",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#clustering",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Clustering",
    "text": "Clustering\nThe use of k-means clustering on the angles also required developing a way to order the ligand-zinc-ligand angles in such a way that they are comparable across all of the different CGs. The final method used in the paper was to order them using largest-sortedmiddle-opposite, which is the largest angle, the middle angles sorted in order, and lastly the opposite of the largest angle. This keeps them from being scrambled from site to site, and allows them to be comparable across zinc sites.\nAlso, because the number of true clusters was unknown as we are trying to discover in an unbiased way all CGs, the number of clusters was varied, and for each k-clusters, replicate clusterings done, and the stability of the clusters was assessed by cluster membership and locations across replicates."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#novel-cgs",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#novel-cgs",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Novel CGs",
    "text": "Novel CGs\nBased on all this work, we were able to compare generated clusters from both the normal and compressed groups with canonical CGs to determine if a CG from clustering corresponds to a known CG or a novel CG. The normal clusters mostly corresponded to known CGs or unsurprising variants thereof. What was really interesting is that there were multiple clusters corresponding to a tetrahedral CG. To determine if the compressed CGs were merely a compressed variant of the canonical, the compressed angle was removed from the comparison to generate a probability of assignment. At least one of the compressed groups appear to be novel, in that it has not been described before, either structurally or functionally."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#functional-characterization",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#functional-characterization",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Functional Characterization",
    "text": "Functional Characterization\nWe also wanted to determine if there was a functional component to the CGs, i.e. different CGs have different functionality (this is my primary but not only contribution to this manuscript). To do this we annotated all of the PDB sequences using InterProScan, and kept annotations that intersected the range of amino-acids that potentially interact with the zinc ion. This bit is tricky, because many different CGs can have many different functionality, merely doing hypergeometric-enrichment of annotations in each cluster doesn’t lead to a convincing picture; as we quickly found out (it was the first thing I tried). However, if we generate a measure of functional similarity between clusters (based on a faux covariance, really, read the paper, it is rather neat) and compare this functional cluster similarity measure with a cluster distance based on the angles, there is an extremely high Spearman correlation of 0.88 for the normal and 0.66 for the compressed CGs, implying that CG and function are intimately related.\n\n\n\n\n\nFig 9 from Yao et al., 2015\n\n\n\n\nFinally, we considered all normal and compressed clusters as separate groups and performed hypergeometric-enrichment on both the InterProScan and EC number annotations, finding enriched annotations specific to each group of clusters.\n\n\n\n\n\nFig 10 from Yao et al, 2015"
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#implications",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#implications",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Implications",
    "text": "Implications\n\nMaking too many assumptions about biological structure can be a bad thing. Given that, however, if you are using canonical structures for assignment and you get a ton of outliers, maybe you need to re-examine your data and methods.\nMachine learning methods such as random-forest, k-means clustering, and statistical classification can be readily used to discover and assign metal-ion CG. If you are careful, and separate out the compressed from normal first.\nThere is a rather tight relationship between a zinc-ion’s CG and the functionality of the protein it is embedded within."
  },
  {
    "objectID": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#reproducibility",
    "href": "posts/2015-06-17-novel-zinc-coordination-geometries/index.html#reproducibility",
    "title": "Novel Zinc Coordination Geometries",
    "section": "Reproducibility",
    "text": "Reproducibility\nAlthough we have admittedly dropped the ball on this, there will be a tarball of all of the scripts used to generate the results including a README explaining how to run them available soon.\nEdit: We have a tarball of the code and data used on our website and from figshare (Yao et al. 2016)"
  },
  {
    "objectID": "posts/2014-01-28-motivated-learning/index.html",
    "href": "posts/2014-01-28-motivated-learning/index.html",
    "title": "Motivated Learning",
    "section": "",
    "text": "As part of the instructor training of Software-Carpentry, we were asked to write a blog post about two things:\nHere are mine. I thought others who read my ramblings might find them useful. Note that these are cross-posted on the Software-Carpentry teaching blog."
  },
  {
    "objectID": "posts/2014-01-28-motivated-learning/index.html#being-motivated-to-learn-calculus",
    "href": "posts/2014-01-28-motivated-learning/index.html#being-motivated-to-learn-calculus",
    "title": "Motivated Learning",
    "section": "Being Motivated to Learn Calculus",
    "text": "Being Motivated to Learn Calculus\nAs part of my undergraduate degree, I was required to take higher level mathematics, either calculus or linear algebra. I had previously taken introductory calculus in high school, that consisted mainly in learning how to do derivatives of functions. Unfortunately, I tended to coast in high school, and my first year of university I did the same. I got a C in my first semester of calculus (differential calculus), and then a D in my second (integral calculus). I tried a different course, and recieved an F. This was my first (and thankfully my only) F in all of my school years. By this time I was in my third year, and was slowly starting to figure out how to study effectively. However, this class was required if I was to be awarded my degree in my chosen majors (biology and chemistry double major). So, I buckled down, and started truly trying to understand the material. I did the problems, and when I couldn’t figure them out, I went and got help. I asked questions, I worked with others in the class, and I kept trying, and trying, and really trying to understand the material. I finally finished that class with I think a B grade, and a much better understanding of why integrals, and calculus, is important."
  },
  {
    "objectID": "posts/2014-01-28-motivated-learning/index.html#personal-experience-with-git",
    "href": "posts/2014-01-28-motivated-learning/index.html#personal-experience-with-git",
    "title": "Motivated Learning",
    "section": "Personal Experience with Git",
    "text": "Personal Experience with Git\nI do bioinformatics type research for a living, which necessarily means writing code, because I need code to do my analyses. As part of writing code, in the last year I have started heavily using the git revision control system. This past year, I was co-author on a publication about software that I had written. That software project is in a git repo, and I am very glad it is. The reviewers of my manuscript requested many changes to the underlying software package. Because I had it in git, I was able to create a new branch and start making requested changes, all without worrying about breaking the current codebase. Due to keeping the code on GitHub, I was also able to point to actual commits that address the reviewers concerns.\nIn addition, some of the reviewer points required writing entirely new analyses. As I worked on this, I needed to write over 500 lines of new functionality, as well as the new analyses. All of this went into the git repo, allowing me to keep track of progress, and easily revert or branch off as necessary when things went wrong. I don’t think I could have done the work that I did in a timely manner without version control."
  },
  {
    "objectID": "posts/2014-06-05-bioinformatics-presentations-that-lack-results-or-biological-relevance/index.html",
    "href": "posts/2014-06-05-bioinformatics-presentations-that-lack-results-or-biological-relevance/index.html",
    "title": "Bioinformatics Presentations that Lack Results (or Biological Relevance)",
    "section": "",
    "text": "In bioinformatics research we need to show validated results (if doing classification or discovery of new things), or show biological relevance. If you do neither of those things in a paper or presentation, then I’m not going to believe your method is worth anything."
  },
  {
    "objectID": "posts/2014-06-05-bioinformatics-presentations-that-lack-results-or-biological-relevance/index.html#seminar-without-results",
    "href": "posts/2014-06-05-bioinformatics-presentations-that-lack-results-or-biological-relevance/index.html#seminar-without-results",
    "title": "Bioinformatics Presentations that Lack Results (or Biological Relevance)",
    "section": "Seminar Without Results",
    "text": "Seminar Without Results\nI attended a seminar yesterday (I’m not going to comment on who gave the seminar or what it was about, so please don’t ask) where the presenter had a distinct lack of any useful results. Their presentation consisted almost entirely of methods description, with various discussions of why they did what they did, with the only result being:\nWe found X number of known things, and predicted Y number of novel things\nJust to clarify, they had a methodology that generated classifiers based on features from known items to find novel items, and then associate both known and novel with biological conditions (e.g. disease vs non-disease).\nIn all of the slides shown, there was no discussion of whether the features in the classifier were relevant, how well the classifier worked when splitting the known stuff into training / test data sets, and whether anything discovered (both of the known and unknown) had any biological relevance or had been validated by wet-lab collaborators. This is the worst kind of bioinformatics communication, because the audience had no idea in the end if the method was actually useful or generated biologically relevant results. This is really the point of bioinformatics and more broadly systems biology research, is making something that helps us decipher biological systems, even in some small way. If you don’t have any data on either the accuracy of classification (in this case finding novel things that were unknown) or the biological relevance of things found to be different, not even one data point, then you should not be giving the talk or writing the paper. Period."
  },
  {
    "objectID": "posts/2021-07-22-keep-figures-in-a-word-document/index.html",
    "href": "posts/2021-07-22-keep-figures-in-a-word-document/index.html",
    "title": "Keeping Figures in an Rmd – Word Document",
    "section": "",
    "text": "I’ve been working with a lot of collaborators who expect Word documents lately. I don’t really like it, but it makes it a lot easier for them to edit and work with things back and forth. So I’ve really been working on generating nice output in the Word document.\nRecently, someone brought up collecting the figures for a manuscript for submission. For those in the know, when you submit a manuscript, you often submit the text with or without figures embedded, and then you also submit “high” resolution figure files separately (yes it’s annoying, but that’s the way it is for many submission systems). Now, you could plot twice in each chunk, but thats freaking annoying. Ideally, Rmarkdown or knitr should do all the work for us. And thanks to the R Markdown Cookbook (Xie 2021), there is a simple way to do this.\nIn the yaml preamble, simply set the parameter: keep_md: true, like this:\noutput:\n  word_document:\n    keep_md: true\nAlternatively, if using Quarto:\noutput:\n  docx:\n    keep-md: true\nThis means the markdown intermediate is kept, as well as the directory of figures. The figures will be named with the name of the chunk, so you will want to name your chunks well to make it easy to find the figures.\nYou can also control where your figures get written to, as well as the default figure size, output type, and resolution.\nThis chunk sets the directory (path) the dpi to 300, to use cairo PNG output, the figure width to 8 in, height to 5 in. And then creates the directory (which I’m sure knitr does, but this is for my own sanity).\n# the trailing slash is important!\nfig_dir = here::here(\"doc\", \"figure_directory/\") \nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, dpi = 300,\n                      dev.args = list(png = list(type = \"cairo\")),\n                      fig.width = 8, fig.height = 5,\n                      fig.path = fig_dir)\nif (!dir.exists(fig_dir)) {\n  dir.create(fig_dir)\n}\nEdit: I rolled all of this functionality into a little package documentNumbering.\n\n\n\n\nReferences\n\nXie, C; Riederer, Y; Dervieux. 2021. “R Markdown Cookbook.” https://bookdown.org/yihui/rmarkdown-cookbook/keep-files.html.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2021,\n  author = {Robert M Flight},\n  title = {Keeping {Figures} in an {Rmd} -\\/- {Word} {Document}},\n  date = {2021-07-22},\n  url = {https://rmflight.github.io/posts/2021-07-22-keep-figures-in-a-word-document},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2021. “Keeping Figures in an Rmd -- Word\nDocument.” July 22, 2021. https://rmflight.github.io/posts/2021-07-22-keep-figures-in-a-word-document."
  },
  {
    "objectID": "posts/2014-11-05-travis-ci-to-github-pages/index.html",
    "href": "posts/2014-11-05-travis-ci-to-github-pages/index.html",
    "title": "Travis-CI to GitHub Pages",
    "section": "",
    "text": "I don’t remember how I got on this, but I believe I had a recent twitter exchange with some persons (or saw it fly by) about pushing R package vignettes to the web after building and checking on travis-ci. Hadley Wickham pointed to using such a scheme to push the web version of his book after each update and the S3 deploy hooks on travis-ci. Deploying your html content to S3 is great, but given the availability of the gh-pages branch on GitHub, I thought it would be neat to work out how to deploy the html output from an R package vignette to the gh-pages branch on GitHub. This is useful because more and more packages are being hosted only on GitHub and building and testing use the travis-ci service, and it takes work to remember to knit and push stuff separately to the gh-pages branch. In addition, although it is possible to deploy one’s html output to other services from within travis-ci, there is not an easy pushbutton solution to deploying to GitHub pages. After some searching and looking, this is what I have come up with for my own package.\nAll subsequent steps assume that:"
  },
  {
    "objectID": "posts/2014-11-05-travis-ci-to-github-pages/index.html#oauth",
    "href": "posts/2014-11-05-travis-ci-to-github-pages/index.html#oauth",
    "title": "Travis-CI to GitHub Pages",
    "section": "OAUTH",
    "text": "OAUTH\nTo start, you will need to generate an OAUTH token on GitHub that will be used to allow you to push back to your GitHub repo. This will be some really long alphanumeric string. You can generate one by going to settings -> applications -> personal access tokens -> generate new token. Make sure to copy this into a file that is not under version control.\nYou will also need to install the travis ruby framework.\ngem install travis\nAfter installing, navigate to your git repo for the project you want to enable automatic pushing of content for, and then login to travis-ci and secure the GitHub token.\ntravis login\n\ntravis encrypt GH_TOKEN=\"yourgithubtoken\"\nThis will generate output that should be copied to your .travis.yml file. Essentially we have created an environment variable GH_TOKEN with your actual GitHub token, that is encrypted on the travis-ci servers. So this way you don’t expose your actual GitHub token to anyone who looks at your .travis.yml file."
  },
  {
    "objectID": "posts/2014-11-05-travis-ci-to-github-pages/index.html#deploy-script",
    "href": "posts/2014-11-05-travis-ci-to-github-pages/index.html#deploy-script",
    "title": "Travis-CI to GitHub Pages",
    "section": "Deploy Script",
    "text": "Deploy Script\nWe also need a bash script that will actually push the content for us. As part of the R process on travis-ci, we get a tar.gz file of the package with the compiled vignette. So we just need to untar that file, copy the html file, and create the git repo and push. The code below is what I have done for my own package, categoryCompare. I saved this code in the file .push_gh_pages.sh.\n#!/bin/bash\n\nrm -rf out || exit 0;\nmkdir out;\n\nGH_REPO=\"@github.com/rmflight/categoryCompare.git\"\n\nFULL_REPO=\"https://$GH_TOKEN$GH_REPO\"\n\nfor files in '*.tar.gz'; do\n        tar xfz $files\ndone\n\ncd out\ngit init\ngit config user.name \"rmflight-travis\"\ngit config user.email \"travis\"\ncp ../categoryCompare/inst/doc/categoryCompare_vignette.html index.html\n\ngit add .\ngit commit -m \"deployed to github pages\"\ngit push --force --quiet $FULL_REPO master:gh-pages\nNote that we remove the directory where we want to create our git repo, create it, setup the remote repo with the token string, and then we untar and unzip the previously built package, and copy over the file that we want to be the index.html page on the gh-pages branch. Finally we add it, commit it, and do a force push.\nThis script is actually part of the package repo, but does not get included in the built tar.gz file (add it to .Rbuildignore). This makes it easy to keep it in sync with any changes to the overall package itself.\nNote that this is completely overriding the current contents of the gh-pages branch. If you wanted to do something nicer (i.e. preserving commits or working with an index page pointing to multiple vignettes), you could pull just the gh-pages branch first, and then make modifications."
  },
  {
    "objectID": "posts/2014-11-05-travis-ci-to-github-pages/index.html#modifying-.travis.yml",
    "href": "posts/2014-11-05-travis-ci-to-github-pages/index.html#modifying-.travis.yml",
    "title": "Travis-CI to GitHub Pages",
    "section": "Modifying .travis.yml",
    "text": "Modifying .travis.yml\nIn addition, we need to add three lines to the .travis.yml file.\n# under env: global:\n  - secure: \"yoursecurestring\"\n\n# under before_install:\n  - chmod 755 ./.push_gh_pages.sh\n\n# under after_success:\n  - ./.push_gh_pages.sh\n\nAdding the GH_TOKEN to the global environment variables\nMaking the deploy script executable\nAdding the running of the deploy script after_success, so only when build and check and tests run successfully\n\nAnd it seems to work quite nicely. As an example, my categoryCompare package now has it’s vignette on the gh-pages branch, and this will get updated every time I push a commit."
  },
  {
    "objectID": "posts/2014-11-05-travis-ci-to-github-pages/index.html#update",
    "href": "posts/2014-11-05-travis-ci-to-github-pages/index.html#update",
    "title": "Travis-CI to GitHub Pages",
    "section": "Update",
    "text": "Update\nAs Carson pointed out below, there was an error in the second code block. travis secure should be travis encrypt, as you are encrypting the credentials. secure is for decrypting something that was already encrypted. Thanks for catching it!"
  },
  {
    "objectID": "posts/2021-05-02-animating-a-geographic-introduction/index.html",
    "href": "posts/2021-05-02-animating-a-geographic-introduction/index.html",
    "title": "My Geographic Introduction",
    "section": "",
    "text": "I thought the recent animated map at Piping Hot Data {Pileggi (2021)} was a really neat way to demonstrate where someone has lived and what their various experiences may have been (while acknowledging that we are also more than the sum of where we have lived of course), so I thought I would take a go at creating my own, which includes stints in various parts of Canada, a stint in Germany, as well as two moves within Kentucky.\nI’ll start by getting all of the locations, as well as my time at each one. I had to add the year dates so I could get the number of years correct, as my initial try I was missing 6 years. Now I’m only missing 1.5, which isn’t bad if we are using years as our unit of time.\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(gganimate)\nlibrary(extrafont)\nlibrary(ggtext)\nresidence = tribble(\n~city,           ~state,  ~lat,   ~long, ~years, ~description, ~dates,\n\"Brandon\",  \"MB\", 49.8485, -99.9501, 2.5, \"Born\", \"1979-1982\",\n\"Lahr\",     \"Baden-Württemberg\", 48.3392, 7.8781, 6, \"Childhood\", \"1982-1988\",\n\"Oromocto\", \"NB\", 45.8487, -66.4813, 4, \"Childhood\", \"1988-1992\",\n\"Victoria\", \"BC\", 48.4284, -123.3656, 1, \"Childhood\", \"1992-1993\",\n\"Burton\", \"NB\", 45.8752, -66.3611, 9, \"Childhood<br>Undergrad at UNB\", \"1993-2002\",\n\"Fredericton\", \"NB\", 45.9636, -66.6431, 2, \"Masters at UNB\", \"2002-2004\",\n\"Halifax\", \"NS\", 44.6488, -63.5752, 5, \"PhD at Dalhousie\", \"2004-2009\",\n\"Louisville\", \"KY\", 38.2527, -85.7585, 2, \"PostDoc at UofL\", \"2010-2012\",\n\"Lexington\", \"KY\", 38.0406, -84.5037, 9, \"PostDoc at UK<br>Research Associate at UK\", \"2012-2021\"\n)\n\nI created a function from Shannon’s original code because I end up using it twice, and I was missing all of the variables I needed to change to make it work properly.\n\ncreate_connections = function(residence){\n  residence_connections_prelim = residence %>% \n  mutate(\n    # need this to create transition state ----\n    city_order = row_number() + 1,\n    # where I moved to next, for curved arrows ----\n    lat_next = lead(lat),\n    long_next = lead(long),\n    # label to show in plot, styled using ggtext ---\n    label = glue::glue(\"**{city}, {state}** ({years} yrs)<br>*{description}*\"),\n    # label of next location ----\n    label_next = lead(label)\n  ) \n  n_entry = nrow(residence_connections_prelim)\nresidence_connections = residence_connections_prelim %>%\n  # get first row of residence ----\n  slice(1) %>% \n  # manually modify for plotting ----\n  mutate(\n    city_order = 1,\n    label_next = label,\n    lat_next = lat,\n    long_next = long,\n    ) %>% \n  # combine with all other residences ----\n  bind_rows(residence_connections_prelim) %>% \n  # last (9th) row irrelevant ----\n  slice_head(n = n_entry) %>% \n  # keep what we neeed ----\n  dplyr::select(city_order, lat, long, lat_next, long_next, label_next)\n  residence_connections\n}\n\n\nworld_data = ggplot2::map_data(\"world\")\ntrim_world = world_data %>% \n  dplyr::filter(long >= -130 & long <= 20, lat >= 35, lat <= 70)\nggplot() + geom_polygon(data = trim_world, aes(x=long, y = lat, group = group)) +\ncoord_fixed(1.3)\n\n\n\n\nOK, at least that looks like the right region that I want to use. Basically from British Columbia to Germany, and the northern part of North America.\n\nbase_map = ggplot() +\n  # plot states ----\n  geom_polygon(\n    data = trim_world,\n    aes(\n      x     = long, \n      y     = lat, \n      group = group\n      ),\n    fill  = \"#F2F2F2\",\n    color = \"white\"\n  ) +\n  # lines for pins ----\n  geom_segment(\n    data = residence,\n    aes(\n      x    = long,\n      xend = long,\n      y    = lat,\n      yend = lat + 0.5\n      ),\n    color = \"#181818\",\n    size = 0.3\n    ) +\n    # pin heads, a bit above actual location, color with R ladies lighter purple ----\n  geom_point(\n    data = residence,\n    aes(\n      x = long, \n      y = lat + 0.5\n      ),\n    size = 0.5,\n    color = \"#88398A\"\n  ) +\n  theme_void() +\n  coord_fixed(1.3)\nbase_map\n\n\n\n\n\nres_connections = create_connections(residence)\nn_res = nrow(res_connections)\nanim = base_map +\n  # show arrows connecting residences ----\n  geom_curve(\n    # do not include 1st residence in arrows as no arrow is intended ----\n    # and inclusion messes up transition ---\n    data = res_connections %>% slice(-1),\n    # add slight adjustment to arrow positioning ----\n    aes(\n      y     = lat - 0.1,\n      x     = long,\n      yend  = lat_next - 0.2,\n      xend  = long_next,\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    color = \"#181818\",\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.02, \"npc\")),\n    size  = 0.2\n  ) +\n  # add in labels for pins, with inward positioning ----\n  # show labels either top left or top right of pin ----\n  geom_richtext(\n    data = res_connections,\n    aes(\n      x     = ifelse(long_next < -100, long_next + 1, long_next - 1),\n      y     = lat_next + 5,\n      label = label_next,\n      vjust = \"top\",\n      hjust = ifelse(long_next < -100, 0, 1),\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    size = 2,\n    label.colour = \"white\",\n    # R ladies purple ----\n    color = \"#562457\",\n    # R ladies font used in xaringan theme ----\n    family = \"Lato\"\n  ) +\n  # title determined by group value in transition ----\n  ggtitle(paste0(\"Home {closest_state} of \", n_res)) +\n  # create animation ----\n  transition_states(\n    city_order,\n    transition_length = 2,\n    state_length = 5\n    ) +\n  # style title ----\n  theme(\n    plot.title = element_text(\n      color = \"#562457\",\n      family = \"Permanent Marker\",\n      size = 12\n      )\n    )\n# render and save transition ----\n# the default nframes 100 frames, 150 makes the gif a bit longer for readability ----\n# changing dimensions for output w/ height & width ----\n# increasing resolution with res ----\nanimate(anim, nframes = 150, height = 2, width = 3, units = \"in\", res = 150)\n\n\n\nanim_save(\"homes_animation.gif\")\n\nThat’s not bad! The only issue with it is that because of the crossing of the Atlantic Ocean, the travels within North America, especially the very close travels from NB to NS, and then within KY are way too crushed together.\nSo lets see what happens if we trim to the region of North America, and remove the overseas trip to Germany.\n\ntrim_world2 = world_data %>% \n  dplyr::filter(long >= -130 & long <= -60, lat >= 35, lat <= 70)\nggplot() + geom_polygon(data = trim_world2, aes(x=long, y = lat, group = group)) +\ncoord_fixed(1.3)\n\n\n\n\n\nresidence2 = residence %>%\n  dplyr::filter(!grepl(\"Lahr\", city))\nres_connections2 = create_connections(residence2)\nn_res2 = nrow(res_connections2)\nbase_map2 = ggplot() +\n  # plot states ----\n  geom_polygon(\n    data = trim_world2,\n    aes(\n      x     = long, \n      y     = lat, \n      group = group\n      ),\n    fill  = \"#F2F2F2\",\n    color = \"white\"\n  ) +\n  # lines for pins ----\n  geom_segment(\n    data = residence2,\n    aes(\n      x    = long,\n      xend = long,\n      y    = lat,\n      yend = lat + 0.5\n      ),\n    color = \"#181818\",\n    size = 0.3\n    ) +\n    # pin heads, a bit above actual location, color with R ladies lighter purple ----\n  geom_point(\n    data = residence2,\n    aes(\n      x = long, \n      y = lat + 0.5\n      ),\n    size = 0.5,\n    color = \"#88398A\"\n  ) +\n  theme_void() +\n  coord_fixed(1.3)\nbase_map2\n\n\n\n\n\nanim2 = base_map2 +\n  # show arrows connecting residences ----\n  geom_curve(\n    # do not include 1st residence in arrows as no arrow is intended ----\n    # and inclusion messes up transition ---\n    data = res_connections2 %>% slice(-1),\n    # add slight adjustment to arrow positioning ----\n    aes(\n      y     = lat - 0.1,\n      x     = long,\n      yend  = lat_next - 0.2,\n      xend  = long_next,\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    color = \"#181818\",\n    curvature = -0.5,\n    arrow = arrow(length = unit(0.02, \"npc\")),\n    size  = 0.2\n  ) +\n  # add in labels for pins, with inward positioning ----\n  # show labels either top left or top right of pin ----\n  geom_richtext(\n    data = res_connections2,\n    aes(\n      x     = ifelse(long_next < -100, long_next + 1, long_next - 1),\n      y     = lat_next + 5,\n      label = label_next,\n      vjust = \"top\",\n      hjust = ifelse(long_next < -100, 0, 1),\n      # group is used to create the transition ----\n      group = seq_along(city_order)\n    ),\n    size = 2,\n    label.colour = \"white\",\n    # R ladies purple ----\n    color = \"#562457\",\n    # R ladies font used in xaringan theme ----\n    family = \"Lato\"\n  ) +\n  # title determined by group value in transition ----\n  ggtitle(paste0(\"Home {closest_state} of \", n_res2)) +\n  # create animation ----\n  transition_states(\n    city_order,\n    transition_length = 2,\n    state_length = 5\n    ) +\n  # style title ----\n  theme(\n    plot.title = element_text(\n      color = \"#562457\",\n      family = \"Permanent Marker\",\n      size = 12\n      )\n    )\n# render and save transition ----\n# the default nframes 100 frames, 150 makes the gif a bit longer for readability ----\n# changing dimensions for output w/ height & width ----\n# increasing resolution with res ----\nanimate(anim2, nframes = 150, height = 2, width = 3, units = \"in\", res = 150)\n\n\n\nanim_save(\"homes_animation2.gif\")\n\n\n\n\n\nReferences\n\nPileggi, Shannon. 2021. “PIPING HOT DATA: GGanimating a Geographic Introduction.” https://www.pipinghotdata.com/posts/2021-02-15-gganimating-a-geographic-introduction/.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2021,\n  author = {Robert M Flight},\n  title = {My {Geographic} {Introduction}},\n  date = {2021-05-02},\n  url = {https://rmflight.github.io/posts/2021-05-02-animating-a-geographic-introduction},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2021. “My Geographic Introduction.” May 2,\n2021. https://rmflight.github.io/posts/2021-05-02-animating-a-geographic-introduction."
  },
  {
    "objectID": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html",
    "href": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html",
    "title": "Mouse / Human Transcriptomics and Batch Effects",
    "section": "",
    "text": "This 2014 PNAS paper by S. Lin et al (Lin et al., PNAS, 2014) that compares transcription of tissues between species has a flawed experimental design, where species is almost perfectly confounded with machine / lane on which the sequencing was done. Y. Golad and O. Mizrahi-Man have published a manuscript describing the confounding and the results of removing it. This was possible because the original authors supplied the information about which publically available files were used in the original analysis. The data from this experiment is probably only suitable as an example of what not to do in high-throughput biology experimental design, and that there may be similarities in human and mouse transcriptional programs.\nWe discussed these papers in the Systems Biology and Omics Integration University of Kentucky Journal Club on June 1, 2015. I lead that discussion."
  },
  {
    "objectID": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#mouse-human-transcriptomic-differences",
    "href": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#mouse-human-transcriptomic-differences",
    "title": "Mouse / Human Transcriptomics and Batch Effects",
    "section": "Mouse / Human Transcriptomic Differences",
    "text": "Mouse / Human Transcriptomic Differences\nIn 2014, two papers were published by members of the ENCODE project purporting that tissue gene expression clustered more by species than by tissue. In the first (ENCODE Consortium, Nature, 2014, doi:10.1038/nature13992 2014), a large number of experiments were combined and compared. Figure 2a shows a PCA plot of expression in across 10 tissues in human and mouse.\n\n\n\n\n\nInterestingly enough, if you collapse PC1 (definitely species), then the tissues start to look quite similar. Which does not seem that unexpected, there are species specific differences, but the tissues are doing something similar in each species.\nThis was not the end of the story, however. A subsequent publication (S Lin et al., PNAS, 2014, doi: 10.1073/pnas.1413624111) went further in doing fresh sequencing of 13 tissues in both species, still showing bigger differences between species than between tissues."
  },
  {
    "objectID": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#is-everything-as-it-seems",
    "href": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#is-everything-as-it-seems",
    "title": "Mouse / Human Transcriptomics and Batch Effects",
    "section": "Is Everything As it Seems?",
    "text": "Is Everything As it Seems?\nOn April 28, Y. Gilad sent out this tweet:\n\nWe reanalyzed the data from http://t.co/Fv7z9WwLJ4 and found the following: pic.twitter.com/37eVs8Kln9\n\nThe left figure shows the original data, clustering by species, and on the right, reprocessed data, clustering by tissue. Now, I have to admit when he posted this I was kind of ticked off. Where was the blog-post or manuscript showing what exactly was done? If you look at the comments to Yoav on that tweet, it seems others were wondering the same thing. Thankfully, on May 19, the manuscript hit F1000Research (Gilad Y and Mizrahi-Man O. A reanalysis of mouse ENCODE comparative gene expression data [v1; ref status: approved with reservations 1, http://f1000r.es/5ez] F1000Research 2015, 4:121 (doi: 10.12688/f1000research.6536.1))."
  },
  {
    "objectID": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#batch-effects",
    "href": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#batch-effects",
    "title": "Mouse / Human Transcriptomics and Batch Effects",
    "section": "Batch Effects",
    "text": "Batch Effects\nYoav asked for the list of data files that were used in the PNAS paper, and then examined the read id line to extract the experimental design. This experimental design is captured in Figure 1:\n\n\n\n\n\nDo you notice a problem with this design?\n. . . . .\nHopefully you noticed that species (one of the main effects to investigate) is not randomly or even semi-randomly distributed across sequencers and / or lanes, but is almost perfectly confounded with sequencer / lane. It doesn’t matter if one technician handled all the samples, this is potentially a large batch effect / confounding variable.\nAnd Yoav shows that ignoring the batch effect produces data much like that reported in the Lin et al PNAS pub, while removing the batch effect using ComBat results in the tissues clustering together."
  },
  {
    "objectID": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#is-the-data-still-useful",
    "href": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#is-the-data-still-useful",
    "title": "Mouse / Human Transcriptomics and Batch Effects",
    "section": "Is the Data Still Useful?",
    "text": "Is the Data Still Useful?\nI presented these papers and the discussion around them at our weekly Systems Biology and Omics Integration (SBOI) Journal Club on June 1, 2015. There were a couple of concerns with the overall study:\n\nIs ENCODE data still useful?\n\nsome people seemed to be concerned that this brought the general ENCODE project into question. I think by the end we agreed that in general it is probably ok to be using ENCODE data, but this particular study was questionable.\n\nWhat does the data tell us?\n\nbased on the correction that Yoav did and reanalysis, is there anything actually telling in the data? Unfortunately, because species is so confounded with machine, I don’t think there is much of a conclusion to draw about species differences vs tissue similarity based on this data. There was some disagreement on this point.\n\nFull experimental designs should be published, and requested by reviewers\n\nReally, why a reviewer on the paper did not request more information about the experimental design is beyond me, especially given the claims of the manuscript. Why it is not the norm to provide this kind of experimental design information in a manuscript is also a good question.\n\n\nFinally, I think the best use of the 2014 PNAS pub and this dataset is an example of how not to design a biological experiment."
  },
  {
    "objectID": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#addendum",
    "href": "posts/2015-06-01-mouse-human-transcriptomics-and-batch-effects/index.html#addendum",
    "title": "Mouse / Human Transcriptomics and Batch Effects",
    "section": "Addendum",
    "text": "Addendum\nAs I looked at the comment section of the Gilad & Man article yesterday (June 1, 2015), I noticed that there were direct replies from S. Lin in a couple of places. In particular is a comment that Lin et al did a second set of sequencing with a new design, and reanalysed the data. Links are provided to two figures, a table of the new design and a new 3D PCA plot:\n\n\n\n\n\n\n\n\n\n\nThe new sequencing design seems much more reasonable, and the PCA plot has many characteristics of the original one from the original comparative analysis by Mouse ENCODE (see above), in that yes, there are species specific differences, but there also appears to be a way to collapse along PC2 and PC3 where the tissues will line up with each other, which I kind of would expect."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html",
    "title": "Things I Learned About distill",
    "section": "",
    "text": "So I converted this site to use distill from my previous blogdown site. This involved a bit of a learning curve to get things right. And my blogdown site was pretty old, as in the default in blogdown at the time was for each post to be it’s own file, not it’s own directory, and the theme I used was the Academic theme (which has since changed names and I believe gotten more complicated). This is not to knock on blogdown and Yihui (and other’s) work in this area. distill just felt like a better fit. Hopefully I’m right. And hopefully this will be the last time I change where posts are located (yes, I’m still not using Netlify, so I still don’t have redirects …), because I wanted to use the date-title format, and I didn’t keep the [slug] of the old YAML. I could have, but I wanted post locations to make sense across the site.\nSo I first used {blogdown} to convert my old blog directory to a bundled version using blogdown::bundle_site(). I then wrote a script to create the new directories based on the bundled ones, and then copy the old index.Rmd to the new direcotry with the proper name. I also decided I wanted to add a refs.bib to each directory so there was no friction in setting up references.\n\nnew_post_dir = \"~/Projects/personal/researchBlog_distill/_posts\"\nold_post_dir = \"~/Projects/personal/researchBlog_blogdown_convertFolders-bundle/content/post\"\n\nold_posts = dir(old_post_dir, full.names = TRUE, recursive = FALSE)\n\nold_properties = file.info(old_posts)\n\nkeep_old = old_posts[old_properties$isdir]\n\nnew_posts = dir(new_post_dir)\n\nkeep_old = keep_old[!(basename(keep_old) %in% new_posts)]\n\npurrr::walk(keep_old, function(in_dir){\n  new_loc = file.path(new_post_dir, basename(in_dir))\n  dir.create(new_loc)\n  index_loc = file.path(in_dir, \"index.Rmd\")\n  split_name = strsplit(basename(new_loc), \"-\", fixed = TRUE)[[1]]\n  split_name = split_name[seq(4, length(split_name))]\n  new_index = file.path(new_loc, paste0(paste(split_name, collapse = \"-\"), \".Rmd\"))\n  file.copy(index_loc, new_index)\n})\n\nnew_posts2 = dir(new_post_dir, full.names = TRUE)\npurrr::walk(new_posts2, function(in_dir){\n  file.create(file.path(in_dir, \"refs.bib\"))\n})\n\nAs I figured out what extra I needed (see some of the sections below) in the YAML, I opened up each post, modified the categories as necessary, and then copied in the extra bits of YAML containing a mock description and the rest regarding TOC and code highlighting."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#descriptions-do-or-do-not-but-be-consistent",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#descriptions-do-or-do-not-but-be-consistent",
    "title": "Things I Learned About distill",
    "section": "Descriptions: Do or Do Not, but Be Consistent",
    "text": "Descriptions: Do or Do Not, but Be Consistent\ndistill has these nice descriptions that are displayed on the index page. However, if you render the site and some posts have descriptions and some don’t, you will get a really, really weird error about ETL text or something. What to do, is if you have some posts with and without descriptions, is add to the ones without (having an empty description is fine), and then delete the XML files in your output director (default is _site) directory, and re-knit the posts and rebuild the site. Hopefully that makes the error go away."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#theming-can-be-a-bit-of-a-pain",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#theming-can-be-a-bit-of-a-pain",
    "title": "Things I Learned About distill",
    "section": "Theming can be a bit of a pain",
    "text": "Theming can be a bit of a pain\nOK, I brought this on myself, I admit. I really, really like dark themes. I know I could just leave it white, and keep using the excellent deluminate extension to keep things dark, but I believe if I like a dark website, then my actual website should be dark too. I had to modify a lot of divs to make my site look nice (you can see the theme here). Too many."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#table-of-contents-doesnt-seem-to-be-site-wide",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#table-of-contents-doesnt-seem-to-be-site-wide",
    "title": "Things I Learned About distill",
    "section": "Table of contents doesn’t seem to be site-wide",
    "text": "Table of contents doesn’t seem to be site-wide\nIncluding table of contents in the yaml of _site.yml doesn’t make them magically appear on every page. You have to set them on each post. Therefore, you decide before you render the post."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#utterances-was-easy",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#utterances-was-easy",
    "title": "Things I Learned About distill",
    "section": "Utterances was easy!",
    "text": "Utterances was easy!\nThanks to Miles McBain, getting the Utterances comment framework in was easy-peasy! And it’s something you can add after the fact, because it’s done as an HTML include and a simple addition to your _site.yml.\noutput:\n  distill::distill_article:\n    includes:\n      in_header: utterances.html\nTo check if it is working, you will need to serve the site using servr::httd(\"site_dir\") though, you can’t just open the html pages and look."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#search-is-awesome",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#search-is-awesome",
    "title": "Things I Learned About distill",
    "section": "Search is awesome!",
    "text": "Search is awesome!\nHaving search on the website is soooo nice! And it searches the descriptions, so as long as you write something useful in the description, it will actually work. Again, testing it requires servr::httd()."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#highlighting-must-be-set-on-each-article",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#highlighting-must-be-set-on-each-article",
    "title": "Things I Learned About distill",
    "section": "Highlighting must be set on each article",
    "text": "Highlighting must be set on each article\nIf I set the code highlight theme in _site.yml, it doesn’t seem to work."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#adding-images",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#adding-images",
    "title": "Things I Learned About distill",
    "section": "Adding Images",
    "text": "Adding Images\nIt took some reading to figure out I should be using knitr::include_graphics() to have images into my blog posts. From what I can tell, I should have been using this long ago."
  },
  {
    "objectID": "posts/2021-02-28-things-i-learned-about-distill/index.html#citations-footnotes-asides",
    "href": "posts/2021-02-28-things-i-learned-about-distill/index.html#citations-footnotes-asides",
    "title": "Things I Learned About distill",
    "section": "Citations, Footnotes, Asides",
    "text": "Citations, Footnotes, Asides\nThese are all really cool features that I want to use waaaayyy more in my posts. I started converting some of my posts to use them (see my code above that created refs.bib files in all of the post directories), and realized if I did that I’d never get the updated blog posted. So expect to see more of those things in new posts, and maybe as I get time (hah!) I’ll convert some older ones over."
  },
  {
    "objectID": "posts/2019-10-23-introducing-scientific-programming/index.html",
    "href": "posts/2019-10-23-introducing-scientific-programming/index.html",
    "title": "Introducing Scientific Programming",
    "section": "",
    "text": "We should get science undergraduate students programming by introducing R & Python in all first year science labs, and continuing throughout the undergraduate classes."
  },
  {
    "objectID": "posts/2019-10-23-introducing-scientific-programming/index.html#why",
    "href": "posts/2019-10-23-introducing-scientific-programming/index.html#why",
    "title": "Introducing Scientific Programming",
    "section": "Why?",
    "text": "Why?\nI’ve previously encountered ideas around getting graduate students to get programming, because to do the analyses that modern science requires you need to be able to at least do some basic scripting, either in a language like Python or R or on the command line. As successful as programs like Software Carpentry are for getting graduate students and others further along their scientific career into a programming and command line mindset, I think it needs to be a lot earlier, and introduced in a way that it becomes second nature. Ergo, undergraduate science labs."
  },
  {
    "objectID": "posts/2019-10-23-introducing-scientific-programming/index.html#undergraduate-science-labs",
    "href": "posts/2019-10-23-introducing-scientific-programming/index.html#undergraduate-science-labs",
    "title": "Introducing Scientific Programming",
    "section": "Undergraduate Science Labs",
    "text": "Undergraduate Science Labs\nBased on my recollection of undergraduate labs and later being a chemistry lab teaching assistant (both during my undergraduate and graduate degrees), there is a lot of calculations going on. Physics labs involved performing experiments to determine underlying constants or known quantities. Chemistry labs often involved quantitative determinations, even more so as labs advanced over the years. Even my biology labs frequently involved calculations and generation of reports to hand in. In my first year, calculators were relied upon, along with example worksheets showing where to fill things in and how to do the calculations. Over the years, Microsoft Excel was eventually introduced, and at one point during my senior Analytical Lab was basically required.\nWhat if students were consistently introduced to another way to do the necessary calculations for their labs?"
  },
  {
    "objectID": "posts/2019-10-23-introducing-scientific-programming/index.html#an-alternative-to-calculators-and-excel",
    "href": "posts/2019-10-23-introducing-scientific-programming/index.html#an-alternative-to-calculators-and-excel",
    "title": "Introducing Scientific Programming",
    "section": "An Alternative to Calculators and Excel",
    "text": "An Alternative to Calculators and Excel\nI’m imagining all of the undergraduate science labs, Biology, Chemistry, Physics and Geology, requiring the use of either Python or R to do the quantitative calculations and produce reports. It would likely require an immense effort on the part of teaching faculty for each lab, teaching assistants in the lab, as well as having one or two dedicated persons who are able to help students with issues running R / Python and getting packages installed. Ideally, students would be introduced to generating their full reports gradually, starting with highly scaffolded lab reports where they simply have to supply a few numbers, click knit (assuming we are using R within RStudio) and generate a report that can be submitted. Over time, the lab reports would involve more and more calculations being coded directly by the student, and more of the report written directly by them. This scaffolding would likely be repeated at each level of labs."
  },
  {
    "objectID": "posts/2019-10-23-introducing-scientific-programming/index.html#challenges-in-implementation",
    "href": "posts/2019-10-23-introducing-scientific-programming/index.html#challenges-in-implementation",
    "title": "Introducing Scientific Programming",
    "section": "Challenges in Implementation",
    "text": "Challenges in Implementation\nI see three main challenges in implementing something like the above.\n\nGetting all the Departments On Board\nSeriously, getting all of the lab teaching faculty on board so that this happens across all of the science departments and at all levels of labs. Although it would be very useful even if done consistently in a single department, I think the biggest bang for the buck is going to be all departments buying in.\n\n\nConverting all the Labs\nAfter convincing everyone that this is a worthy goal, then there is the herculean task of figuring out how to make the labs fit into this type of framework. The added wrinkle to this is that labs are frequently marked on how close one got to the right answer (known concentration of a standard, for example), which can depend both on how accurately one performs the experimental technique being taught, and how well one does the calculations (at least this was my experience as student and TA). Not only that, but many labs often had two parts, sometimes in the same lab period, where messing up the first part meant that you would have completely wrong answers in the second part. Getting even a first pass conversion would be a monumental effort on the part of someone with extensive scripting language knowledge and the teaching faculty.\nThere is also the question of how to convert the labs. I would imagine that first year would be done first, and then second, third, fourth. But this would also be interwoven with updates to the labs as issues are discovered within each year, and modifications made each year.\n\n\nSupporting the Students and Faculty\nLet’s face it, doing things this way requires more hardware than just having a calculator in lab with you. Python and R will install on almost any OS however, and the types of calculations necessary are not compute intensive. However, there will invariably be issues getting software and necessary packages installed, and keeping them up to date. There needs to be someone who is able to be present both in and outside lab time that can help diagnose package installation and update issues. This same person will also likely be tasked with helping teaching faculty and assistants install and update necessary software and packages.\nNot discussed above, but ideally the design of the lab reports should also make sure that they depend on a very low number of packages, and that those packages will install on any OS that the students come in with."
  },
  {
    "objectID": "posts/2019-10-23-introducing-scientific-programming/index.html#has-anyone-done-this",
    "href": "posts/2019-10-23-introducing-scientific-programming/index.html#has-anyone-done-this",
    "title": "Introducing Scientific Programming",
    "section": "Has Anyone Done This?",
    "text": "Has Anyone Done This?\nI’d be really curious if anyone has attempted anything like this. Please leave a comment if you know of any!"
  },
  {
    "objectID": "posts/2019-10-23-introducing-scientific-programming/index.html#interested",
    "href": "posts/2019-10-23-introducing-scientific-programming/index.html#interested",
    "title": "Introducing Scientific Programming",
    "section": "Interested?",
    "text": "Interested?\nI’ll admit, being the person behind an effort like this is probably the one thing right now that would convince me to leave my current position in trying to solve cancer metabolomics. Drop me a line if this sounds interesting to you!"
  },
  {
    "objectID": "posts/2013-08-16-reproducible-methods/index.html",
    "href": "posts/2013-08-16-reproducible-methods/index.html",
    "title": "Reproducible Methods",
    "section": "",
    "text": "Science is built on the whole idea of being able to reproduce results, i.e. if I publish something, it should be possible for someone else to reproduce it, using the description of the methods used in the publication. As biological sciences have become increasingly reliant on computational methods, this has become a bigger and bigger issue, especially as the results of experiments become dependent on independently developed computational code, or use rather sophisticated computer packages that have a variety of settings that can affect output, and multiple versions. For further discussion on this issue, you might want to read (Iddo 2012) and (Brown 2012).\nI recently read a couple of different publications that really made me realize how big a problem this is. I want to spend some time showing what the problem is in these publications, and why we should be concerned about the current state of computational analytical reproducibility in life-sciences.\nIn both the articles mentioned below, I do not believe that I, or anyone not associated with the project, would be able to generate even approximately similar results based solely on the raw data and the description of methods provided. Ultimately, this is a failure of both those doing the analysis, and the reviewers who reviewed the work, and is a rather deplorable situation for a field that prides itself verification of results. This is why I’m saying these are bad bioinformatics methods sections."
  },
  {
    "objectID": "posts/2013-08-16-reproducible-methods/index.html#puthanveettil-et-al.-synaptic-transcriptome",
    "href": "posts/2013-08-16-reproducible-methods/index.html#puthanveettil-et-al.-synaptic-transcriptome",
    "title": "Reproducible Methods",
    "section": "Puthanveettil et al., Synaptic Transcriptome",
    "text": "Puthanveettil et al., Synaptic Transcriptome\nPuthanveettil and coworkers had a paper out earlier titled A strategy to capture and characterize the synaptic transcriptome (Puthanveettil et al. 2013). Although the primary development reported is a new method of characterizing RNA complexes that are carried by kinesin, much of the following analysis is bioinformatic in nature.\nFor example, they used BLAST searches to identify the RNA molecules, a cutoff value is reported in the results. However, functional characterization using Gene Ontology (GO) was carried out by “Bioinformatics analyses” (see the top of pg3 in the PDF). No mention of where the GO terms came from, which annotation source was used, or any software mentioned. Not in the results, discussion, or methods, or the supplemental methods. The microarray data analysis isn’t too badly described, but the 454 sequencing data processing isn’t really described at all.\nMy point is, that even given their raw data, I’m not sure I would be able to even approximate their results based on the methods reported in the methods section."
  },
  {
    "objectID": "posts/2013-08-16-reproducible-methods/index.html#gulsuner-et-al.-schizophrenia-snps",
    "href": "posts/2013-08-16-reproducible-methods/index.html#gulsuner-et-al.-schizophrenia-snps",
    "title": "Reproducible Methods",
    "section": "Gulsuner et al., Schizophrenia SNPs",
    "text": "Gulsuner et al., Schizophrenia SNPs\nGulsuner and coworkers published a paper in Cell in August 2013 titled Spatial and Temporal Mapping of De Novo Mutations in Schizophrenia to a Fetal Prefrontal Cortical Network (Gulsuner et al. 2013) This one also looks really nice, they look for de novo mutations (i.e. new mutations in offspring not present in parents or siblings) that mess up genes that are in a heavily connected network, and also examine gene co-expression over brain development time-scales. Sounds really cool, and the results seem like they are legit, based on my reading of the manuscript. I was really impressed that they even used randomly generated networks to control the false discovery rate!\nHowever, almost all of the analysis again depends on a lot of different bioinformatic software. I do have to give the authors props, they actually give the full version of each tool used. But no mention of tool specific settings (which can generate vastly different results, see Exome Sequencing of the methods).\nThen there is this bombshell: “The predicted functional impact of each candidate de novo missense variant was assessed with in silico tools.” (near top of pg 525 of the PDF). Rrrreeeaaaalllly now. No actual quote of which tools were used, although the subsequent wording and references provided imply that they were PolyPhen2, SIFT, and the Grantham Method. But shouldn’t that have been stated up front? Along with any settings that were changed from default??\nThere is no raw data available, only their reported SNPs. Not even a list of all the SNPs that were potentially considered, so that I could at least go from those and re-run the later analysis. I have to take their word for it (although I am glad at least the SNPs they used in later analyses are reported).\nFinally, the random network generation. I’d like to be able to see that code, go over it, and see what exactly it was doing to verify it was done correctly. It likely was, based on the histograms provided, but still, these are where small errors creep in and result in invalid results.\nAs above, even if the raw data was available (didn’t see an SRA accession or any other download link), I’m not sure I could reproduce or verify the results."
  },
  {
    "objectID": "posts/2013-08-16-reproducible-methods/index.html#what-to-do",
    "href": "posts/2013-08-16-reproducible-methods/index.html#what-to-do",
    "title": "Reproducible Methods",
    "section": "What to do??",
    "text": "What to do??\nHow do we fix this problem? I think scripts and workflows used to run any type of bioinformatic analyses have to become first class research objects. And we have to teach scientists to write them and use them in a way that makes them first class research objects. So in the same way that a biologist might ask for verification of immunostaining, etc, bioinformaticians should ask that given known input, a script generates reasonable output.\nI know there has been discussion on this before, and disagreement, especially with the exploratory nature of research. However, once you’ve got something working right, you should be able to test it. Reviewers should be asking if it is testable, or the code should be available for others to test.\nI also think we as a community should do more to point out the problem. i.e. when we see it, point it out to others. I’ve done that here, but I don’t know how much should be formal. Maybe we need a new hashtag, #badbioinfomethodsection, and point links to papers that do this. Conversely, we should also point to examples when it is done right (#goodbioinfomethodsection??), and if you are bioinformatician or biologist who does a lot of coding, share your code, and at least supply it as supplemental materials. Oh, and maybe take a SoftwareCarpentry class, and look up git.\nPosted on August 16, 2013 at http://robertmflight.blogspot.com/2013/08/reproducible-methods-or-bad.html, raw markdown at https://github.com/rmflight/blogPosts/blob/master/reproducible_methods.md"
  },
  {
    "objectID": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html",
    "href": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html",
    "title": "Licensing R Packages that Include Others Code",
    "section": "",
    "text": "If you include others code in your own R package, list them as contributors with comments about what they contributed, and add a license statement in the file that includes their code."
  },
  {
    "objectID": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html#motivation",
    "href": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html#motivation",
    "title": "Licensing R Packages that Include Others Code",
    "section": "Motivation",
    "text": "Motivation\nI recently created the knitrProgressBar package. It is a really simple package, that takes the dplyr progress bars and makes it possible for them to write progress to a supplied file connection. The dplyr package itself is licensed under MIT, so I felt fine taking the code directly from dplyr itself. In addition, I didn’t want my package to depend on dplyr, so I wanted that code self-contained in my own package, and I wanted to be able to modify underlying mechanics that might have been more complicated if I had just made a new class of progress bar that inherited from dplyr’s.\nI also wanted to be able to release my code on CRAN, not just on GitHub. I knew to accomplish that I would have to have all the license stuff correct. However, I had not seen any guide on how to license a package and give proper attribution in the Authors@R field.\nNote that I did ask this question on StackOverflow and ROpenSci forums as well."
  },
  {
    "objectID": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html#information-from-cran",
    "href": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html#information-from-cran",
    "title": "Licensing R Packages that Include Others Code",
    "section": "Information from CRAN",
    "text": "Information from CRAN\nI should note here, that the CRAN author guidelines do provide a small hint in this regard in the Writing R Extensions guide:\n\nNote that all significant contributors must be included: if you wrote an R wrapper for the work of others included in the src directory, you are not the sole (and maybe not even the main) author.\n\nHowever, there is not any guidance provided on how these should ideally be listed."
  },
  {
    "objectID": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html#full-answer",
    "href": "posts/2018-02-14-licensing-r-packages-that-include-others-code/index.html#full-answer",
    "title": "Licensing R Packages that Include Others Code",
    "section": "Full Answer",
    "text": "Full Answer\nI am the main author and maintainer of the new package, that is easy. The original code is MIT licensed, authored by several persons, and has copyright held by RStudio.\nMy solution then was to:\n\nadd the MIT license from dplyr to the file that has the progress bar code\nadd all the authors of dplyr as contributors to my package, with a comment as to why they are listed\nadd RStudio as a copyright holder to my package, with a comment that this only applies to the one file\n\nSo the Authors@R line in my DESCRIPTION ended up being:\nAuthors@R: c(person(given = c(\"Robert\", \"M\"), family = \"Flight\", email = \"rflight79@gmail.com\", role = c(\"aut\", \"cre\")),\n            person(\"Hadley\", \"Wickham\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"Romain\", \"Francois\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"Lionel\", \"Henry\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"Kirill\", \"Müller\", role = \"ctb\", comment = \"Author of included dplyr fragments\"),\n            person(\"RStudio\", role = \"cph\", comment = \"Copyright holder of included dplyr fragments\"))"
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html",
    "title": "categoryCompare Paper Finally Out!",
    "section": "",
    "text": "I can finally say that the publication on my Bioconductor package categoryCompare is finally published in the Bioinformatics and Computational Biology section of Frontiers in Genetics (Flight et al. 2014). This has been a long time coming, and I wanted to give some background on the inspiration and development of the method and software."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#tldr",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#tldr",
    "title": "categoryCompare Paper Finally Out!",
    "section": "TL;DR",
    "text": "TL;DR\nThe software package has been in development in one form or another since 2010, released to Bioconductor in summer 2012, and the publication has bounced around and been revised since spring of 2013, and it is finally available to you. All of the supplementary data and methods are available as an R package on github. Version control using git was instrumental in getting this work out in a timely manner. There is still a bunch of work to do on the package.\nIf I did it again, I would: * Write the manuscript using R markdown, as a vignette in a package * Ask to be able to make reviewer points issues on Github * Submit a preprint with submission"
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#inspiration",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#inspiration",
    "title": "categoryCompare Paper Finally Out!",
    "section": "Inspiration",
    "text": "Inspiration\nIn spring of 2010 I started as a PostDoc with Eric Rouchka. One of his collaborators, Jeff Petruska is interested in the process of collateral sprouting of neurons, especially as it compares to regeneration. Early in my PostDoc, Jeff wanted to do a gene-level comparison of his microarray data of collateral sprouting in skin compared to previously published studies with muscle.\nCombing through the literature produced a number of genes differentially expressed in denervated muscle. However, when comparing with the genes resulting from the skin data, there was almost nothing in common and nothing that made sense from a functional standpoint. Skimming around the Bioconductor literature in GOStats, I was struck by Robert Gentleman’s example of coloring Gene Ontology nodes by which data set they originated from. This is a very simple meta-analysis and visualization. When I tried it with the skin - muscle comparison, I got some very interesting (i.e. the Petruska group thought the results were very interpretable) results.\nNote: At this point, because of the data sources (gene lists from publications with little to no original data), I was using the hypergeometric enrichment test in Category to determine significant GO terms from the two tissues."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#v-0.0000001",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#v-0.0000001",
    "title": "categoryCompare Paper Finally Out!",
    "section": "V 0.0000001",
    "text": "V 0.0000001\nI started developing this idea into a simple package (i.e. collection of R scripts), that was able to do at least GO term enrichment, and that could be hosted on our group webserver to enable others to make use of it. Visualization and interrogation used the imageMaps function in Robert Gentleman’s original demonstration, however any number of data sets could now be compared."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#categorycompare-method-summary",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#categorycompare-method-summary",
    "title": "categoryCompare Paper Finally Out!",
    "section": "categoryCompare Method – Summary",
    "text": "categoryCompare Method – Summary\nThe basic method is to take gene (or really any annotated feature) lists from multiple experiments, and perform annotation (Gene Ontology Terms, KEGG Pathways, etc) enrichment (either hypergeometric or GSEA type) on each gene list, determine significant annotations from each list, and then examine which annotations come from which list.\nBecause this results in a lot of data to parse through, exploration of the results is facilitated by considering the annotations as a network of annotations related by the number of shared genes between them, and interacting with the networks in Cytoscape.\nIf you want to know more, check out the paper (Flight et al. 2014), or the vignette in the Bioconductor package."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#others",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#others",
    "title": "categoryCompare Paper Finally Out!",
    "section": "Others",
    "text": "Others\nAs I developed this idea, I also started looking into other possible software implementations.\n\nShen & Tseng had recently published similar work in MAPE-I, but at my level of understanding it required the same identifiers. In retrospect, I should have read the paper better. However, they also did not have a released implementation with their publication.\nConceptGen is another interesting application, that allows very similar analyses to categoryCompare, except that it is not possible to explore how the concepts map between multiple user supplied data sets. The only way one can relate multiple data sets is if the data sets map as a concept to another, but one cannot visualize the interrelated concepts between user supplied data sets.\nEnrichment Map was another alternative, but did all of comparison mathematics in Cytoscape itself, based on enrichments calculated outside of Cytoscape. The publication does give an example of a similar type of analysis as performed by categoryCompare. However, I wanted everything, from enrichment calculation to visualization controlled by R. I did use their method of weighting the edges between annotations, however, ditching the GO directed acyclic graph (DAG) view I used initially."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#bioconductor-package",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#bioconductor-package",
    "title": "categoryCompare Paper Finally Out!",
    "section": "Bioconductor Package",
    "text": "Bioconductor Package\nAbout this time I realized that I wanted the package to fit into the Bioconductor ecosystem. This required a complete redesign and rewrite of the code, as I moved to actually used some sort of OOP model (S4 in this case), and creating an actual package.\nThe package was released to the wild in the fall of 2012, as part of Bioconductor v 2.10. Of course, this was the last Bioconductor release based on R 2.15, which brought some particular challenges in namespaces with the switch in R 3.0.0 the next year.\n\nGraphviz to RCytoscape\nThe original code used the graphviz package to do layouts for visualization, but it was difficult to install, and did not work the way I wanted. Thankfully Paul Shannon had just developed the RCytoscape package the year previous, and this enabled truly interactive visualization and passing data back and forth between R and Cytoscape.\nMy use of RCytoscape has actually lead to finding improvements in the package, and better use of it. I am also hoping to make much more use of RCytoscape and the igraph package and combining them in novel ways."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#first-publication-attempt",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#first-publication-attempt",
    "title": "categoryCompare Paper Finally Out!",
    "section": "First Publication Attempt",
    "text": "First Publication Attempt\nOur first attempt at publication focussed on the original data that inspired the method, and comparing it with gene-gene comparisons directly. We submitted to BMC Bioinformatics, and the reviews were not favorable, and took forever to get back. We actually wondered if the reviewers had read the paper that we submitted. We gave up on this venue. BTW, our last three publications have been submitted there, and the publication process has gotten worse and worse with every submission there. I don’t think our papers are getting worse over the years. I don’t know if we will bother submitting to BMC Bioinformatics again."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#second-publication-attempt",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#second-publication-attempt",
    "title": "categoryCompare Paper Finally Out!",
    "section": "Second Publication Attempt",
    "text": "Second Publication Attempt\nThe second attempt was submitting to the current home in the Bioinformatics and Computational Biology section of Frontiers in Genetics. This time, although the reviews were harsh (i.e. they were not immediately favorable), they were fair, and actually contained useful critique, and pointed to a way forward.\nUnfortunately for me, revising the manuscript to address the reviewers criticisms meant a lot of work to construct theoretical examples, as well as a lot of thought in order to pare the manuscript down to make sure our primary message was clearly communicated.\n\nFrontiers Interactive Review\nI would like to note that the Frontiers interactive review system, with the ability to discuss individual points with the reviewers (still anonymously) really helped make it possible to determine which points were make or break, and discuss different ways to approach things. This was the best review experience I have ever had, and a large part of that I think was due to being able to interact with the reviewers directly, and not just through letters mediated by the editor.\nI think it would be nice if Frontiers had an option for making the review history available if the authors and reviewers were agreeable to it."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#github-package-of-supplementary-materials",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#github-package-of-supplementary-materials",
    "title": "categoryCompare Paper Finally Out!",
    "section": "Github package of supplementary materials",
    "text": "Github package of supplementary materials\nIn the initial publication, I had included the set of scripts used for analysis, data files, results, etc. However, the amount of work required in the rewrite was so substantial, that I created an R package specifically for the analyses that went into the paper, with separate R markdown vignettes for each result type (hypothetical, two different experimental comparisons). This package has documents on how raw data was processed, as well as the semi-processed experimental data.\n\nPublication specific branch\nDue to specific changes to the categoryCompare software needed to address the reviewer comments, a publication specific branch of the development version was created (paper). This allowed me to quickly introduce code and features that the reviewers had asked for, without worrying about breaking the current development version that will be released in Bioconductor shortly."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#to-do",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#to-do",
    "title": "categoryCompare Paper Finally Out!",
    "section": "To Do",
    "text": "To Do\nAs with most software projects, there is still plenty to be done.\n\nIncorporate new functionality from paper branch into dev\n\nSpecifically ability to do GSEA built in (probably using limma’s romer and roast functions), and new visualization options\n\nChange from latex vignette to R markdown (this is technically done, but hasn’t made it into the dev branch)\nSwitch to roxygen2 documentation (will be interesting due to use of S4 objects and methods)\nImplement proper testing using testthat\nRefactor a lot of code to improve speed, use R conventions\n\nI was still a relatively R newbie when I wrote the package, and as I looked at the code while making changes for the reviewers, I noticed some places where I had done some silly things, mainly because I didn’t know better when I did it.\n\nConsider splitting visualization into its own package\n\nAlthough I developed the visualization specifically for categoryCompare, I think it is generic enough that others might benefit from having it available separately. Therefore I need to think about separating it out, and how best to go about it without making it hard for categoryCompare to keep working as it already does.\n\nMake it easy to investigate the actual genes with particularly interesting annotations, and how they are linked together by annotations or other data sources, as well as their original expression levels in the experiments."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#side-effects",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#side-effects",
    "title": "categoryCompare Paper Finally Out!",
    "section": "Side Effects",
    "text": "Side Effects\nA nice side effect of the package is that any annotation enrichment I do now, I almost always do it in the framework of categoryCompare, just because it is a lot easier to make sense of using the visualization capabilities and coupling with Cytoscape."
  },
  {
    "objectID": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#what-would-i-do-differently",
    "href": "posts/2014-04-08-categorycompare-paper-finally-out/index.html#what-would-i-do-differently",
    "title": "categoryCompare Paper Finally Out!",
    "section": "What Would I Do Differently?",
    "text": "What Would I Do Differently?\nKeeping in mind that I started this work almost 4 years ago, when I still didn’t know any R, and had yet to be exposed to the reproducibility and open science movements, or knitr, or pandoc, here are some things that if I started today I would do differently (you can probably guess some of these from above):\n\nPut the package under version control immediately! Thankfully I didn’t have any moments early in the process when things were not in a git repo, but I am very thankful later on I was able to do diff and branch on my code to figure out where things broke and introduce new features.\nStart thinking of the analysis as a standalone package from the very beginning, instead of a directory of data and scripts. This is what I do now (blogpost to come), and it makes it much easier if I come up with novel methods to spin them off into a fully fledged R / Bioconductor package\nDon’t underestimate the novelty of something as simple as visualization, and how much it may make or break your method. We ended up adding a good chunk of text to the manuscript on the visualization because we realized how important it was, but only after the reviewers pointed it out to us.\nWrite the paper as a vignette of the ccPaper package itself, and generate Word documents for collaborators who insist on Word docs using Pandoc\nStart a github repo for the paper, and ask collaborators to try and work on it there\nSubmit a preprint when submitted, so that we start getting feedback on the manuscript early\nAsk for permission to set up reviewer comments as issues on the github repo to easily track how well we are addressing them.\n\nWouldn’t it be cool in a totally open peer review journal to actually do all of the peer review on a service like Github, and have reviewers leave issues, tag them, and comment directly on the text of the publication using the commenting feature of commenting on commits?"
  },
  {
    "objectID": "posts/2014-07-28-analyses-as-packages/index.html",
    "href": "posts/2014-07-28-analyses-as-packages/index.html",
    "title": "Analyses as Packages",
    "section": "",
    "text": "Edit 2022-12-02: I don’t recommend this approach anymore."
  },
  {
    "objectID": "posts/2014-07-28-analyses-as-packages/index.html#tldr",
    "href": "posts/2014-07-28-analyses-as-packages/index.html#tldr",
    "title": "Analyses as Packages",
    "section": "TL;DR",
    "text": "TL;DR\nInstead of writing an analysis as a single or set of R scripts, use a package and include the analysis as a vignette of the package. Read below for the why, the how is in the next post."
  },
  {
    "objectID": "posts/2014-07-28-analyses-as-packages/index.html#analyses-and-reports",
    "href": "posts/2014-07-28-analyses-as-packages/index.html#analyses-and-reports",
    "title": "Analyses as Packages",
    "section": "Analyses and Reports",
    "text": "Analyses and Reports\nAs data science or statistical researchers, we tend to do a lot of analyses, whether for our own research or as part of a collaboration, or even for supervisors depending on where we work. As I have continued working in R, I have progressed from having a simple .R script (or collection of related scripts) to using a package to structure as much of my research as possible, including analyses that generate reports.\nNote that I have been meaning to write this post for a while, but the tipping point was seeing these tweets from Hilary Parker and David Nusinow\n\nI am all about the many short scripts rather than one long script when doing an analysis. I think I am alone here."
  },
  {
    "objectID": "posts/2014-07-28-analyses-as-packages/index.html#why-packages",
    "href": "posts/2014-07-28-analyses-as-packages/index.html#why-packages",
    "title": "Analyses as Packages",
    "section": "Why Packages",
    "text": "Why Packages\nPackages are R’s method for sharing code in a sensible way, making it possible for others to easily (more often than not) use functions that you have written (I’m looking at you python!). Why not use them? They also give you access to R’s facilities for documentation and sharing computable documents. Hadley Wickham has a nice section on packages in his Advanced R book.\nI use a lot of Hadley’s packages in the following sections, because they are useful, and promote practices that make it extremely practical to use packages as a way to make an analysis a self-contained unit.\nDuncan Murdoch has a nice slide deck on why to use packages and vignettes here\n\nStructure\nI want to breifly review the structure of package directories, you can read more about packages in Hadley’s book (link above), and in the official R documentation from CRAN.\nPackages impose a relatively simple structure on your project directory. /R contains the .R files with your actual functions, and /data can contain any .RData or .rda files that you might need. Other data types (.txt, .tab, .csv) can also go in /data, or they may go in /inst/extdata. Note that in /inst/extdata you can specify any directory structure that seems appropriate.\nOther required files are DESCRIPTION and NAMESPACE.\nYou may also have .Rmd or .Rnw / .Rtex files in /vignettes that generate html or pdf output that combines prose and R code into a single document. This is where things get really interesting in being able to package up an analysis, especially when combined with functions.\n\n\nFunctions\nAlmost any analysis I have done involves writing at least one function, generally more, because I almost never do anything once in an analysis. Packages are the primary method of sharing functions in R that make sure that your functions play nice with the R NAMESPACE, and allow one to define function dependencies from other packages. If you define a function in a package (and export it), it immediately becomes re-usable in multiple analyses, without worrying about suffering from copypasta.\n\n\nFunction Documentation\nThe easiest way to document functions is by using roxygen2 (see the intro vignette). This allows you to worry about the documentation right next to the function itself, and not worry about writing separate documentation files in /inst/doc (really, you don’t want to do it, I have, and it is painful). The keywords in roxygen2 make sense, and are not hard to remember.\n\n\nData\nAs mentioned above, you can include data with your package. The neat thing about including data, is you can document it and have that documentation available as part of the package.\nI find it really useful to put any raw data that you want to work with in /inst/extdata in whatever format it exists, and then process the data and save it as an .RData file in /data, with associated documentation. It is also really useful if part of the calculations are long running, then you can save the results as an associated data file, and simply load it when needed in the analysis.\nSmall note about documenting data sets. You put the roxygen2 comments in another file, and also need to provide @name explicitly, and follow the documentation block with NULL. Check the roxygen2 vignette “Generating Rd files” for a specific example."
  },
  {
    "objectID": "posts/2014-07-28-analyses-as-packages/index.html#why-vignettes-as-a-reporting-method",
    "href": "posts/2014-07-28-analyses-as-packages/index.html#why-vignettes-as-a-reporting-method",
    "title": "Analyses as Packages",
    "section": "Why Vignettes as a Reporting Method",
    "text": "Why Vignettes as a Reporting Method\nOne great feature of packages is that one can include multiple vignettes, long form text mixed with code (and/or figures) to explain or highlight functionality in a package. Normally these are used to write tutorials, demonstrate features, or group together documentation that wouldn’t normally be together in the general documentation. However, there are no limits as to what can actually be contained within the vignette as far as content, or how many vignettes a package can have.\nFor packages hosted by CRAN, vignettes are an optional component. However, the Bioconductor project requires that vignettes be included in each package.\nSo, R packages have a method to include long form prose that can be mixed with R code directly as part of the package, within which you have already put your functions and associated data.\nPrior to R 3.0, one generally had to write vignettes using sweave, a combination of latex and R code that generates a PDF file. However, since v3.0, it is possible to write vignettes using R markdown (and actually some other markup formats), which generates HTML output. The advantages to using R markdown over sweave are that the syntax for writing markdown is much simpler, and much more readable in it’s raw format.\nGiven that a package allows us to define sets of related functions, data, and documentation (with dependencies defined) all in one place that others can subsequently install and make use of and build on, why wouldn’t you want to use packages and vignettes to write long form analyses?\nFrom some of my descriptions above, it may appear that this incurs some overhead. However, thanks to the #hadleyverse and rstudio, it is rather trivial (note that rstudio is not essential, but I find it does make it easier). In my next post I am going to give a worked example from start to finish of generating an analysis that is a vignette as part of a package."
  },
  {
    "objectID": "posts/2021-10-27-r-cmd-check-and-non-pdf-vignettes/index.html",
    "href": "posts/2021-10-27-r-cmd-check-and-non-pdf-vignettes/index.html",
    "title": "R CMD Check and Non-PDF Vignettes",
    "section": "",
    "text": "If you’ve written an R package that you want hosted by CRAN (or even if not hosted), then you generally want to run the infamous R CMD check on your package.\nAlthough it can be a pain in the butt, it has a wide array of checks that make sense (and some that don’t, just look at posts on the R-devel list)."
  },
  {
    "objectID": "posts/2021-10-27-r-cmd-check-and-non-pdf-vignettes/index.html#missing-files",
    "href": "posts/2021-10-27-r-cmd-check-and-non-pdf-vignettes/index.html#missing-files",
    "title": "R CMD Check and Non-PDF Vignettes",
    "section": "Missing Files",
    "text": "Missing Files\nIf you are using rmarkdown to generate non-standard vignettes, or even using rmarkdown because of features that aren’t present directly in knitr, then the check process may fail after it re-builds the package. You may see this message:\nWarning: Files in the 'vignettes' directory but no files in 'inst/doc':\n  ‘vignette_file.Rmd’\nThe solution, as far as I can tell, is to:\n\nadd both knitr and rmarkdown to the Suggests field of DESCRIPTION.\nadd VignetteBuilder: knitr to the DESCRIPTION.\nadd %\\VignetteEngine{knitr::rmarkdown} to the vignette meta-data.\nuse output: … whatever actual output document type you wanted in the vignette itself.\n\nI think that the build command only recognizes the knitr engine properly, and that is the only one that builds the vignettes and shoves them in the right location.\nOtherwise, you should build them manually and double check the options to build and check so that vignettes don’t get rebuilt during either build or check.\nDepending on your needs, you might want to use something like Drew Schmidt’s approach (“Vignette-Less Articles with Pkgdown” 2020)."
  },
  {
    "objectID": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html",
    "href": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html",
    "title": "Measuring Changes in Height Over Time",
    "section": "",
    "text": "I have a possibly really weird theory, that I’ve not been able to find any literature on. I think that over the course of a full menstrual cycle, that some women’s height changes. The reason I think this happens, is that my spouse and I are almost exactly the same height, 99% of the time we look at each other directly in the eyes. However, there are regularly certain days of the month when we both notice that she is either slightly taller or slightly shorter than usual.\nSo, it would be really, really cool if we could record heights daily and see if it changes in any measurable way. Even neater would be if we could get a large sample of people at various life stages and genders to measure their height daily and compare amongst all of them to see if this change in height is specific to people who are regularly menstruating."
  },
  {
    "objectID": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html#ideal-method",
    "href": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html#ideal-method",
    "title": "Measuring Changes in Height Over Time",
    "section": "Ideal Method",
    "text": "Ideal Method\nIdeally we need to be able to do this anywhere in the world, and people should be able to do it themselves, without needing a partner or to travel anywhere. If we can do it with photos that do not involve taking a photo of the person themselves, nor use any other photo data except the date the photo was taken (if we want to be able to line up changes in height with stages of the menstrual cycle), that would also be ideal to help protect participants identity."
  },
  {
    "objectID": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html#what-i-came-up-with",
    "href": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html#what-i-came-up-with",
    "title": "Measuring Changes in Height Over Time",
    "section": "What I Came Up With",
    "text": "What I Came Up With\nJust FYI, last winter I tried a method that involved printed lines on paper, selfies, and eye detection using OpenCV. It was a total bust.\nA month ago, I was hashing this over in my mind again, and realized if only we could get someone else to measure people with a mark on the wall every day it might possibly work. And then I realized that if there were permanent marks on the wall (using permanent marker), and the person marked their own height with a pencil in between them, then the pencil mark location in between the two lines might possibly be used to detect changes in height.\nFigure 1 shows me measuring my own height, and Figure 2 an actual photo of one of my own pencil marks to test this on the right.\n\n\n\n\n\nFigure 1: Author taking a selfie while pretending to mark their own height.\n\n\n\n\n\n\n\n\n\nFigure 2: An actual height mark on the wall."
  },
  {
    "objectID": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html#testing-it",
    "href": "posts/2022-12-07-measuring-changes-in-height-over-time/index.html#testing-it",
    "title": "Measuring Changes in Height Over Time",
    "section": "Testing It",
    "text": "Testing It\nTo test the idea, I bought some relatively thin pegboard that has a thickness of 4mm (see Figure 3). I then marked my own height 4 times, with no pegboard (0), 1 (4 mm), and 2 (8 mm). Each marking was a full replicate of:\n\nstanding up against the wall,\nmaking a pencil mark,\nmoving away from the wall,\ntaking a photo,\nerasing the pencil mark,\nrepeat\n\n\n\n\n\n\nFigure 3: The pegboard I used to vary my own height.\n\n\n\n\nI then cropped the photos to just include the pencil mark and the lines, and then used {stringr} to generate random names for each of the photos so I wouldn’t know which height photo I was annotating.\nFor this test, I opened each photo in Glimpse, and measured the distance in pixels from the pencil mark to the top line, and then the pencil mark to the bottom line using the measure tool, and recorded them in an quarto-doc.\nFinally, I calculated the ratio of the distances in pixels between the top and bottom measured values.\nFigure 4 shows the distributions for each height. We can see that there is a fair bit of variance in the ratios of top / bottom distances, however, the variance between heights is larger than the variance within heights, even at a difference of 4mm.\n\n\n\n\n\nFigure 4: Plot of top to bottom distance ratios for the three heights, with 4 replicate measurements for each height.\n\n\n\n\nTherefore, I think this is doable. Even more so, if the lines are made a standard distance apart (say 5 cm or 2 in), then we should be able to calculate actuall changes in height. And I think we could easily make a Shiny app that loads a photo, and records three mouse clicks to annotate the top line location, height location, and bottom location, making it easier to extract measurements from a given photo.\nSo people anywhere in the world, with at least a digital camera, could make the permanent lines, self measure using a pencil and photograph every day (erasing the pencil mark). We could use a form to upload their photos, and then take information on reproductive status, age, and dates of menses (if they are open to providing them). To do this right we would want people of various genders / sexes, and at multiple stages of life: pre-pubescent, puberty, pregnant, with and without birth control, post-menopausal, etc.\nI’m open to other ideas, but I think Figure 4 shows that it could possibly work. I do plan to try taking my own height and my spouses (peri-menopausal) for a couple of months and see what we get for preliminary data."
  },
  {
    "objectID": "posts/2022-09-27-creating-an-analysis-using-targets/index.html",
    "href": "posts/2022-09-27-creating-an-analysis-using-targets/index.html",
    "title": "Creating an Analysis With a targets Workflow",
    "section": "",
    "text": "Setting up an analysis workflow using {targets} can be a little confusing without worked examples. This is my attempt to provide such an example."
  },
  {
    "objectID": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#example-repo",
    "href": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#example-repo",
    "title": "Creating an Analysis With a targets Workflow",
    "section": "Example Repo",
    "text": "Example Repo\nI’ve set up a GitHub repo that contains the data and scripts used for this analysis."
  },
  {
    "objectID": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#packages",
    "href": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#packages",
    "title": "Creating an Analysis With a targets Workflow",
    "section": "Packages",
    "text": "Packages\nFor this example, if you want to follow along yourself, you will need Miles McBain’s {tflow} package, which sets up an opinionated but loose directory / file structure. You will also need the {targets} package, and {tarchetypes}, and {rmarkdown}, as well as a few others.\ninstall.packages(\n c(\"conflicted\",\n   \"dotenv\",\n   \"targets\",\n   \"tarchetypes\",\n   \"BiocManager\",\n   \"viridis\")\n)\n\nBiocManager::install(\"limma\")\n\nremotes::install_github(\n  c(\"milesmcbain/tflow\",\n    \"moseleyBioinformaticsLab/ICIKendallTau\",\n    \"moseleyBioinformaticsLab/visualizationQualityControl\"))"
  },
  {
    "objectID": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#example-analysis",
    "href": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#example-analysis",
    "title": "Creating an Analysis With a targets Workflow",
    "section": "Example Analysis",
    "text": "Example Analysis\nThe example we will work through is a lipidomics data analysis. This data is based on real data, but sample IDs have been completely anonymized. I’m going to walk through all the steps I do to set up the analysis, mostly by commenting the example _targets.R file, and leaving the objects in the order they were created.\n\nCreate New Project\nWe start by creating a brand new RStudio project to work in, or any new directory.\n\n\nInitialize tflow\ntflow::use_tflow()\n✔ Setting active project to '/home/rmflight/Projects/personal/example_targets_workflow'\n✔ Creating 'R/'\n✔ Writing 'packages.R'\n✔ Writing '_targets.R'\n✔ Writing '.env'\nIn addition to this, I’ve created a data directory, and added the measurements CSV and metadata CSV to that directory.\ndir(\"data\")\n[1] \"sample_measurements.csv\" \"sample_metadata.csv\"\nSome important things about using {tflow} in this project:\n\n{tflow} uses the targets::tar_plan() function to contain the various targets in the workflow.\nThis means you don’t have to have a list of targets at the end of _targets.R.\nIt also means you can use {drake} style targets of the form:\n\n out_target = function_name(in_target),\n ...\nI personally think it makes the workflow much more readable than the default targets style of:\ntar_target(\n  out_target\n  function_name(in_target)\n)\n\n\nSetup Packages\nWe modify the packages.R file to add anything else we need.\n## library() calls go here\nlibrary(conflicted)\nlibrary(dotenv)\nlibrary(targets)\nlibrary(tarchetypes)\n# our extra packages for this analysis\nlibrary(visualizationQualityControl)\nlibrary(ggplot2)\nlibrary(ICIKendallTau)\nlibrary(dplyr)\nlibrary(limma)\n\n\nInitial _targets.R\nThis is what {tflow} sets up to run when you start.\n## Load your packages, e.g. library(targets).\nsource(\"./packages.R\")\n\n## Load your R files\nlapply(list.files(\"./R\", full.names = TRUE), source)\n\n## tar_plan supports drake-style targets and also tar_target()\ntar_plan(\n\n# target = function_to_make(arg), ## drake style\n\n# tar_target(target2, function_to_make2(arg)) ## targets style\n\n)\nNotice that the first thing that happens is that the packages.R file gets sourced, so that all your packages are loaded. Second, it runs through all of the files in R, and sources them to load all the functions. {tflow} and {targets} expound a functional workflow, where we put things in functions. Finally, it uses tar_plan() to run each of the targets.\nWe will specify each of the targets in turn to fill out the analysis we want to do.\n\n\nLoad Data\nLets first declare the measurement file and metadata file as actual targets, so if they change, then any dependent targets will be re-run as well.\ntar_plan(\n\n  tar_target(measurement_file,\n             \"data/sample_measurements.csv\",\n             format = \"file\"),\n  tar_target(metadata_file,\n             \"data/sample_metadata.csv\",\n             format = \"file\")\n\n)\nRunning it with tar_make() results in:\ntar_make()\n• start target measurement_file\n• built target measurement_file [0.001 seconds]\n• start target metadata_file\n• built target metadata_file [0.001 seconds]\n• end pipeline [0.078 seconds]\nThen we add in actually reading in the data.\ntar_plan(\n\n  tar_target(measurement_file,\n             \"data/sample_measurements.csv\",\n             format = \"file\"),\n  tar_target(metadata_file,\n             \"data/sample_metadata.csv\",\n             format = \"file\")\n\n  lipid_measurements = readr::read_csv(measurement_file),\n  lipid_metadata = readr::read_csv(metadata_file),\n\n)\ntar_make()\n✔ skip target measurement_file\n✔ skip target metadata_file\n• start target lipid_measurements\nRows: 1012 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): class, name, group_units, feature_id\ndbl (12): WT_1, WT_2, WT_3, WT_4, WT_5, WT_6, KO_1, KO_2, KO_3, KO_4, KO_5, ...\n|\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n• built target lipid_measurements [0.305 seconds]\n• start target lipid_metadata\nRows: 12 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): parent_sample_name, assay, cell_line, client_matrix, client_sample...\ndbl  (8): client_identifier, client_sample_number, group_number, sample_amou...\n/\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n• built target lipid_metadata [0.07 seconds]\n• end pipeline [0.47 seconds]\n\n\nExploratory Data Analysis\nFor the exploratory data analysis (EDA), I like to put that into a report. You set those up using tflow::use_rmd(\"document_name\") or tflow::use_qmd(\"document_name). An example EDA report is available at the actual repo.\ntflow::use_rmd(\"exploration\")\n✔ Setting active project to '/home/rmflight/Projects/personal/example_targets_workflow'\n✔ Writing 'doc/exploration.Rmd'\nAdd this target to your tar_plan():\n\ntar_render(exploration, \"doc/exploration.Rmd\")\ntar_plan(\n\n  tar_target(measurement_file,\n             \"data/sample_measurements.csv\",\n             format = \"file\"),\n  tar_target(metadata_file,\n             \"data/sample_metadata.csv\",\n             format = \"file\")\n\n  lipid_measurements = readr::read_csv(measurement_file),\n  lipid_metadata = readr::read_csv(metadata_file),\n  \n  tar_render(exploration, \"doc/exploration.Rmd\")\n)\n> targets::tar_make()\n✔ skip target measurement_file\n✔ skip target metadata_file\n✔ skip target lipid_measurements\n✔ skip target lipid_metadata\n• start target exploration\n• built target exploration [2.829 seconds]\n• end pipeline [2.922 seconds]\nNote we could have put these bits in their own functions instead of keeping them in the document proper:\n\nICI-Kt correlations;\nHeatmap figure;\nPCA decomposition;\nPCA visualization\n\nThese are left as an exercise for the reader.\n\n\nDifferential Analysis\nFollowing EDA, we need to do the differential analysis. But for that, we need to do normalization and imputation of missing values first. Each one of those should be their own functions. Here is the normalization function, here is the imputation, and here is the differential analysis function.\nAnd here is the output of running each step.\n# normalization\n> tar_make()\n✔ skip target measurement_file\n✔ skip target metadata_file\n✔ skip target lipid_measurements\n✔ skip target lipid_metadata\n• start target lipid_normalized\nNew names:\n• `` -> `...1`\n• `` -> `...2`\n• `` -> `...3`\n• `` -> `...4`\n• `` -> `...5`\n• `` -> `...6`\n• `` -> `...7`\n• `` -> `...8`\n• `` -> `...9`\n• `` -> `...10`\n• `` -> `...11`\n• `` -> `...12`\n• built target lipid_normalized [0.449 seconds]\n✔ skip target exploration\n• end pipeline [0.562 seconds]\n# imputing missing values\n> tar_make()\n✔ skip target measurement_file\n✔ skip target metadata_file\n✔ skip target lipid_measurements\n✔ skip target lipid_metadata\n✔ skip target lipid_normalized\n✔ skip target exploration\n• start target lipid_imputed\n• built target lipid_imputed [0.05 seconds]\n• end pipeline [0.169 seconds]\n# differential analysis\n> tar_make()\n✔ skip target measurement_file\n✔ skip target metadata_file\n✔ skip target lipid_measurements\n✔ skip target lipid_metadata\n✔ skip target lipid_normalized\n✔ skip target exploration\n✔ skip target lipid_imputed\n• start target lipids_differential\n• built target lipids_differential [0.198 seconds]\n• end pipeline [0.312 seconds]\nI try to build it up piecewise like this because it lets me easily load the previous target into the next function. Most of my functions will have tar_load(previous_target) at the top as a comment, or varname = tar_read(previous_target), if I’ve got a more generic variable name I want to use that doesn’t match the name of the target coming into the function. For example, here is the top of the normalization function.\nnormalize_samples = function(lipid_measurements){\n  # do normalization in here and return the df\n  # tar_load(lipid_measurements)\n...\nThis strategy lets me easily load the variables I need in that function space and develop the actual code of the function. I will very often couple this with restarting the R session, source(\"./packages.R\"), and if needed lapply(...) sourcing the R function files, and then load the necessary variables and start writing the code for the function.\n\n\nFinal Report\nFinally, we can put the differential results in our final report. So that they are together, we can make the EDA report a child document of the differential report, and include it as well. Let’s actually make a copy, and include it as a child document.\nWe add it to the plan as another target using the tar_target syntax, because as far as I know that is the only way to include something as a file dependency. If we want the differential_report target to get rerun when the exploration one is changed, this is the way to do it, have a target in the tar_plan, and then make sure to load the target in the differential_report itself.\n# on the command line / terminal / console\n>tflow::use_rmd(\"differential_report\")\n✔ Writing 'doc/differential_report.Rmd'\nAdd this target to your tar_plan():\n\ntar_render(differential_report2, \"doc/differential_report.Rmd\")\n# in the _targets.R file\n# add the child target\ntar_target(exploration_child,\n             \"doc/exploration_child.Rmd\",\n             format = \"file\"),\n             \n# and then generate final report\ntar_render(differential_report, \"doc/differential_report.Rmd\")\nIn the rmarkdown:\n```{r eda child='doc/exploration_child.Rmd'}\nplot(cars)\n```\n# run the differential report\n> tar_make()\n✔ skip target measurement_file\n✔ skip target metadata_file\n✔ skip target exploration_child\n✔ skip target lipid_measurements\n✔ skip target lipid_metadata\n✔ skip target lipid_normalized\n✔ skip target exploration\n✔ skip target lipid_imputed\n✔ skip target lipids_differential\n• start target differential_report\n• built target differential_report [3.018 seconds]\n• end pipeline [3.14 seconds]"
  },
  {
    "objectID": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#notes-about-rmarkdown-and-quarto",
    "href": "posts/2022-09-27-creating-an-analysis-using-targets/index.html#notes-about-rmarkdown-and-quarto",
    "title": "Creating an Analysis With a targets Workflow",
    "section": "Notes About Rmarkdown and Quarto",
    "text": "Notes About Rmarkdown and Quarto\nIf you want to be able to interactively mess with your Rmarkdown / Quarto docs while under {targets}, then you need to change the setting Chunk Output Inline to Chunk Output in Console.\nIf you want to run a full render of the document outside of the {targets} workflow, then you have to add an option to the interactive (shell, console) calls to {rmarkdown} and {quarto} to either use the knit_root_dir or execute_dir arguments, respectively. Both of those should be in the top level directory of the {targets} project, which most often at the console can be gotten by using getwd(), as shown in the examples below.\n# for rmarkdown\nrmarkdown::render(\"doc/document.Rmd\", knit_root_dir = getwd())\n# for quarto\nquarto::quarto_render(\"doc/document.qmd\", execute_dir = getwd())\nThere is also currently an issue with using {gt} tables in Quarto -> Word documents within {targets} workflows that as of 2022-09-27, is not quite resolved.\nEdited 2022-11-30: Added a couple of notes on the use of {tflow} and subsequently tar_plan()"
  },
  {
    "objectID": "posts/2018-07-17-split-unsplit-anti-pattern/index.html",
    "href": "posts/2018-07-17-split-unsplit-anti-pattern/index.html",
    "title": "Split - Unsplit Anti-Pattern",
    "section": "",
    "text": "If you notice yourself using split -> unsplit / rbind on two object to match items up, maybe you should be using dplyr::join_ instead. Read below for concrete examples."
  },
  {
    "objectID": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#motivation",
    "href": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#motivation",
    "title": "Split - Unsplit Anti-Pattern",
    "section": "Motivation",
    "text": "Motivation\nI have had a lot of calculations lately that involve some sort of normalization or scaling a group of related values, each group by a different factor.\nLets setup an example where we will have 1e5 values in 10 groups, each group of values being normalized by their own value.\n\nlibrary(microbenchmark)\nlibrary(profvis)\nset.seed(1234)\nn_point <- 1e5\nto_normalize <- data.frame(value = rnorm(n_point), group = sample(seq_len(10), n_point, replace = TRUE))\n\nnormalization <- data.frame(group = seq_len(10), normalization = rnorm(10))\n\nFor each group in to_normalize, we want to apply the normalization factor in normalization. In this case, I’m going to do a simple subtraction."
  },
  {
    "objectID": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#match-them",
    "href": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#match-them",
    "title": "Split - Unsplit Anti-Pattern",
    "section": "Match Them!",
    "text": "Match Them!\nMy initial implementation was to iterate over the groups, and use %in% to match each group from the normalization factors and the data to be normalized, and modify in place. Don’t do this!! It was the slowest method I’ve used in my real package code!\n\nmatch_normalization <- function(normalize_data, normalization_factors){\n  use_groups <- normalization_factors$group\n  \n  for (igroup in use_groups) {\n    normalize_data[normalize_data$group %in% igroup, \"value\"] <- \n      normalize_data[normalize_data$group %in% igroup, \"value\"] - normalization_factors[normalization_factors$group %in% igroup, \"normalization\"]\n  }\n  normalize_data\n}\n\nmicro_results <- summary(microbenchmark(match_normalization(to_normalize, normalization)))\nknitr::kable(micro_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\n\n\n\n\nmatch_normalization(to_normalize, normalization)\n49.75035\n53.45793\n57.42852\n54.73885\n57.50818\n115.9432\n100\n\n\n\n\nNot bad for the test data. But can we do better?"
  },
  {
    "objectID": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#split-them",
    "href": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#split-them",
    "title": "Split - Unsplit Anti-Pattern",
    "section": "Split Them!",
    "text": "Split Them!\nMy next thought was to split them by their groups, and then iterate again over the groups using purrr::map, and then unlist them.\n\nsplit_normalization <- function(normalize_data, normalization_factors){\n  split_norm <- split(normalization_factors$normalization, normalization_factors$group)\n  \n  split_data <- split(normalize_data, normalize_data$group)\n  \n  out_data <- purrr::map2(split_data, split_norm, function(.x, .y){\n    .x$value <- .x$value - .y\n    .x\n  })\n  do.call(rbind, out_data)\n}\n\nmicro_results2 <- summary(microbenchmark(match_normalization(to_normalize, normalization),\n               split_normalization(to_normalize, normalization)))\nknitr::kable(micro_results2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\n\n\n\n\nmatch_normalization(to_normalize, normalization)\n54.09506\n69.0412\n75.46899\n73.44315\n78.03191\n160.8331\n100\n\n\nsplit_normalization(to_normalize, normalization)\n103.18837\n133.9091\n141.51288\n142.00616\n148.73655\n197.9781\n100"
  },
  {
    "objectID": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#join-them",
    "href": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#join-them",
    "title": "Split - Unsplit Anti-Pattern",
    "section": "Join Them!",
    "text": "Join Them!\nMy final thought was to join the two data.frame’s together using dplyr, and then they are automatically matched up.\n\njoin_normalization <- function(normalize_data, normalization_factors){\n  normalize_data <- dplyr::right_join(normalize_data, normalization_factors,\n                                      by = \"group\")\n  \n  normalize_data$value <- normalize_data$value - normalize_data$normalization\n  normalize_data[, c(\"value\", \"group\")]\n}\n\nmicro_results3 <- summary(microbenchmark(match_normalization(to_normalize, normalization),\n               split_normalization(to_normalize, normalization),\n               join_normalization(to_normalize, normalization)))\nknitr::kable(micro_results3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexpr\nmin\nlq\nmean\nmedian\nuq\nmax\nneval\n\n\n\n\nmatch_normalization(to_normalize, normalization)\n56.04426\n68.50138\n77.23205\n74.45665\n82.25683\n184.9358\n100\n\n\nsplit_normalization(to_normalize, normalization)\n106.96651\n135.82308\n148.88644\n145.27503\n154.41702\n284.8173\n100\n\n\njoin_normalization(to_normalize, normalization)\n11.60424\n13.26513\n21.29059\n14.18384\n15.38526\n488.3713\n100"
  },
  {
    "objectID": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#conclusions",
    "href": "posts/2018-07-17-split-unsplit-anti-pattern/index.html#conclusions",
    "title": "Split - Unsplit Anti-Pattern",
    "section": "Conclusions",
    "text": "Conclusions\nSo on my computer, the split and match implementations are mostly comparable, although on my motivating real world example, I actually got a 3X speedup by using the split method. That may be because of issues related to DataFrame and matching elements within that structure. The join method is 10-14X faster than the others, which is what I’ve seen in my motivating work. I also think it makes the code easier to read and reason over, because you can see what is being subtracted from what directly in the code."
  },
  {
    "objectID": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html",
    "href": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html",
    "title": "Using Academicons in Your Quarto Blog",
    "section": "",
    "text": "Install the academicons extension in your Quarto project, and then use the “text” option."
  },
  {
    "objectID": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html#academicons",
    "href": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html#academicons",
    "title": "Using Academicons in Your Quarto Blog",
    "section": "Academicons??",
    "text": "Academicons??\nAcademicons are an awesome set of academic project themed icons in the same spirit as the Font-Awesome icon collection. In particular, they include a variety of icons related to various academic projects and software. The one I really wanted access to, was the ORCID icon, ."
  },
  {
    "objectID": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html#using-in-quarto",
    "href": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html#using-in-quarto",
    "title": "Using Academicons in Your Quarto Blog",
    "section": "Using in Quarto",
    "text": "Using in Quarto\nYou can download the academicons for use in Latex, HTML, and also in Quarto projects.\nFor quarto in particular, you need to install the extension in the particular project!\n\n\nI was using the instructions on the academicons site, and didn’t see that I needed to cd into my blog directory first to have the extension installed into the project. I do appreciate that the default for quarto is on a project basis.\ncd myblog\nquarto install extension schochastics/academicons\nThen including any of the academicons in your document is easily accomplished using the short codes, like we can include the ORCID icon  using:\n{{< source icon >}}\nwhere source = ai and icon = orcid or whatever other icon you want to use."
  },
  {
    "objectID": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html#including-the-icon-in-navbar",
    "href": "posts/2022-12-03-using-academicons-in-your-quarto-blog/index.html#including-the-icon-in-navbar",
    "title": "Using Academicons in Your Quarto Blog",
    "section": "Including the Icon in Navbar",
    "text": "Including the Icon in Navbar\nIt’s often nice to include various icons in the navigation bar (navbar) or footer of your site. If you want to use it in the navbar, you will have to use\n# this would be an item under the navbar\n  - text: \"{{< source icon >}}\"\n    href: link\nI hope this helps someone else!"
  },
  {
    "objectID": "posts/2022-01-31-heatmap-colormaps/index.html",
    "href": "posts/2022-01-31-heatmap-colormaps/index.html",
    "title": "Heatmap Colormaps!",
    "section": "",
    "text": "Nine times out of ten, you probably need just one of the {viridis} colormaps for values that are increasing in one direction. For example, correlation values that are all positive, or abundances from RNA-Seq, etc. If you want to use them with {ComplexHeatmap}, you do so like this:\n\n# correlation example for 0 - 1\n# imagine cor_vals is a set of correlation values\nlibrary(ComplexHeatmap)\ncor_start = 0\ncor_end = 1\nn_break = 20\nviridis_map = circlize::colorRamp2(seq(cor_start, \n                                       cor_end, \n                                       length.out = n_break),\n                                   viridis::viridis(n_break))\n\nHeatmap(cor_vals, \n        col = viridis_map, \n        ...)"
  },
  {
    "objectID": "posts/2022-01-31-heatmap-colormaps/index.html#divergent-data",
    "href": "posts/2022-01-31-heatmap-colormaps/index.html#divergent-data",
    "title": "Heatmap Colormaps!",
    "section": "Divergent Data",
    "text": "Divergent Data\nIn this case, you have values where a central value is a natural point that you want to highlight things above and below that value. Examples include correlations that include negative and positive correlations, log-fold-changes above and below zero. In this case, I prefer the {scico} package with the vik colormap.\n\ncor_start = -1\ncor_end = 1\nn_break = 20\n\nscico_map = circlize::colorRamp2(seq(cor_start\n                                     cor_end,\n                                     length.out = n_break),\n                                 scico::scico(n_break, \n                                              palette = \"vik\"))\n\nHeatmap(cor_vals, \n        col = scico_map,\n        ...)"
  },
  {
    "objectID": "posts/2022-01-31-heatmap-colormaps/index.html#other-colormaps",
    "href": "posts/2022-01-31-heatmap-colormaps/index.html#other-colormaps",
    "title": "Heatmap Colormaps!",
    "section": "Other Colormaps?",
    "text": "Other Colormaps?\nDo you have other suggestions for colormaps, both incremental and divergent? Please let me know in the comments!"
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html",
    "title": "Writing Papers Using R Markdown",
    "section": "",
    "text": "I have been watching the activity in RStudio and knitr for a while, and have even been using Rmd (R markdown) files in my own work as a way to easily provide commentary on an actual dataset analysis. Yihui has proposed writing papers in markdown and posting them to a blog as a way to host a statistics journal, and lots of people are now using knitr as a way to create reproducible blog posts that include code (including yours truly).\nThe idea of writing a paper that actually includes the necessary code to perform the analysis, and is actually readable in its raw form, and that someone else could actually run was pretty appealing. Unfortunately, I had not had the time or opportunity to actually try it, until recently our group submitted a conference paper that included a lot of analysis in R that seemed like the perfect opportunity to try this. (I will link to the paper here when I hear more, or get clearance from my PI). Originally we wrote the paper in Microsoft(r) Word, but after submission I decided to see what it would have taken to write it as an Rmd document that could then generate markdown or html.\nIt turned out that it was not that hard, but it did force me to do some things differently. This is what I want to discuss here."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#advantages",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#advantages",
    "title": "Writing Papers Using R Markdown",
    "section": "Advantages",
    "text": "Advantages\nI actually found it much easier to have the text with the analysis (in contrast to having to be separate in a Word document), and upon doing the conversion, discovered some possible numerical errors that crept in because of having to copy numerical results separately (that is the nice thing about being able to insert variable directly into the text). In addition, the Word template for the submission didn’t play nice with automatic table and figure numbering, so our table and figure numbering got messed up in the submission. So overall, I’d say it worked out better with the Rmd file overall, even with the having to create functions to handle table and figure numbering properly myself (see below)."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#tables-and-figures",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#tables-and-figures",
    "title": "Writing Papers Using R Markdown",
    "section": "Tables and Figures",
    "text": "Tables and Figures\nAs I’m sure most of you know, Word (and other WYSIWYG editors) have ability to keep track of your object numbers, this is especially nice for keeping your figure and table numbers straight. Of course, there is no such ability built into a static text file, but I found it was easy to write a couple of functions for this. The way I came up with is to have a variable that contains a label for the figure or table, a function that increments the counter when new figures or tables are added, and a function that prints the associated number for a particular label. This does require a bit of forethought on the part of the writer, because you may have to add a table or figure label to the variable long before you actually create it, but as long as you use sane (i.e. descriptive) labels, it shouldn’t be a big deal. Let me show you what I mean.\n\nCounting\n\nincCount <- function(inObj, useName){\n    nObj <- length(inObj)\n    useNum <- max(inObj) + 1\n    inObj <- c(inObj, useNum)\n    names(inObj)[nObj+1] <- useName\n    inObj\n}\nfigCount <- c(\"_\"=0)\ntableCount <- c(\"_\"=0)\n\nThe incCount function is very simple, it takes an object, checks the maximum count, and then adds an incremental value with the supplied name. In this example, I initialized the figCount and tableCount objects with a non-sensical named value of zero.\nNow in the process of writing, I decide I’m going to need a table on the amount of time spent by post-docs writing blog posts in different years of their post-doc training. Lets call this t.blogPostDocs. Notice that this is a fairly descriptive name. We can assign it a number like so:\n\ntableCount <- incCount(tableCount, \"t.blogPostDocs\")\ntableCount\n\n             _ t.blogPostDocs \n             0              1 \n\n\n\n\nInserting\nSo now we have a variable with a named number we can refer to. But how do we insert it into the text? We are going to use another function that will let us insert either the text with a link, or just the text itself.\n\npasteLabel <- function(preText, inObj, objName, insLink=TRUE){\n    objNum <- inObj[objName]\n    \n    useText <- paste(preText, objNum, sep=\" \")\n    if (insLink){\n        useText <- paste(\"[\", useText, \"](#\", objName, \")\", sep=\"\")\n    }\n    useText\n}\n\nThis function allows us to insert the table number like so:\n\nr I(pasteLabel(\"Table\", tableCount, \"t.blogPostDocs\"))\n\nThis would be inserted into a normal inline code block. The I makes sure that the text will appear as normal text, and not get formatted as a code block. The default behavior is to insert as a relative link, thereby enabling the use of relative links to link where a table / figure is mentioned to its actual location. For example, we can insert the anchor link like so:\n<a id=\"t.blogPostDocs\"></a>\n\n\nMarkdown Tables\nFollowed by the actual table text. This brings up the subject of markdown tables. I also wrote a function (thanks to Yihui again) that transforms a normal R data.frame to a markdown table.\n\ntableCat <- function(inFrame){\n    outText <- paste(names(inFrame), collapse=\" | \")\n    outText <- c(outText, paste(rep(\"---\", ncol(inFrame)), collapse=\" | \"))\n    invisible(apply(inFrame, 1, function(inRow){\n        outText <<- c(outText, paste(inRow, collapse=\" | \"))\n    }))\n    return(outText)\n}\n\nLets see it in action.\n\npostDocBlogs <- data.frame(PD=c(\"p1\", \"p2\", \"p3\"), NBlog=c(4, 10, 2), Year=c(1, 4, 2))\npostDocBlogs\n\n  PD NBlog Year\n1 p1     4    1\n2 p2    10    4\n3 p3     2    2\n\npostDocInsert <- tableCat(postDocBlogs)\npostDocInsert\n\n[1] \"PD | NBlog | Year\" \"--- | --- | ---\"   \"p1 |  4 | 1\"      \n[4] \"p2 | 10 | 4\"       \"p3 |  2 | 2\"      \n\n\nTo actually insert it into the text, use a code chunk with results='asis' and echo=FALSE.\ncat(postDocInsert, sep=\"\\n\")\n\n\n\nPD\nNBlog\nYear\n\n\n\n\np1\n4\n1\n\n\np2\n10\n4\n\n\np3\n2\n2\n\n\n\nBefore inserting the table though, you might want an inline code with the table number and caption, like this:\nI(pasteLabel(\"Table\", tableCount, \"t.blogPostDocs\", FALSE)) This is the number of blog posts and year of training for post-docs.\nTable 1 This is the number of blog posts and year of training for post-docs.\nRemember for captions to set the insLink variable to FALSE so that you don’t generate a link from the caption.\n\n\nFigures\nOftentimes, you will have code that generates the figure, and then you want to insert the figure at a different point. This is accomplished by the judicious use of echo and include chunk options.\nFor example, we can create a ggplot2 figure and store it in a variable in one chunk, and then print it in a later chunk to actually insert it into the text body.\n\nplotData <- data.frame(x=rnorm(1000, 1, 5), y=rnorm(1000, 0, 2))\nplotKeep <- ggplot(plotData, aes(x=x, y=y)) + geom_point()\nfigCounts <- incCount(figCount, \"f.randomFigure\")\n\nAnd now we decide to actually insert it using print(plotKeep) with the option of echo=FALSE:\n\n\n\n\n\nFigure 1. A random figure."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#numerical-result-formatting",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#numerical-result-formatting",
    "title": "Writing Papers Using R Markdown",
    "section": "Numerical result formatting",
    "text": "Numerical result formatting\nWhen R prints a number, it normally likes to do so with lots of digits. This is not probably what you want either in a table or when reporting a number in a sentence. You can control that by using the format function. When generating a new variable, the number of digits to display when printing will be saved, and when used on a variable directly, only the defined number of digits will display."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#echo-and-include",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#echo-and-include",
    "title": "Writing Papers Using R Markdown",
    "section": "Echo and Include",
    "text": "Echo and Include\nThis brings up the issue of how to keep the code from appearing in the text body. I found depending on the particulars, either using echo=FALSE or include=FALSE would do the job. This is meant to be a paper, a reproducible one, but a paper nonetheless, and therefore the code should not end up in the text body."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#references",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#references",
    "title": "Writing Papers Using R Markdown",
    "section": "References",
    "text": "References\nOne thing I haven’t done yet is convert all the references. I am planning to try using the knitcitations package. I will probably post on that experience."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#html-generation",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#html-generation",
    "title": "Writing Papers Using R Markdown",
    "section": "HTML generation",
    "text": "HTML generation\nBecause I use RStudio, I set up a modified function For generating a full html version of the paper, changing the default RStudio markdown render options like so:\nhtmlOptions <- markdownHTMLOptions(defaults=TRUE)\nhtmlOptions <- htmlOptions[htmlOptions != \"hard_wrap\"]\nmarkdownToHTML(inputFile, outputFile, options = htmlOptions)\nThis should be added to a .Rprofile file either in your home directory or in the directory you start R in (this is especially useful for modification on a per project basis).\nI do this because when I write my documents, I want the source to be readable online. If this is a github hosted repo, that means being displayed in the github file browser, which does not do line wrapping. So I set up a 120 character line in my editor, and try very hard to stick to that."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#function-source",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#function-source",
    "title": "Writing Papers Using R Markdown",
    "section": "Function source",
    "text": "Function source\nYou can find the previously mentioned functions in a github gist here."
  },
  {
    "objectID": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#post-source",
    "href": "posts/2012-10-09-writing-papers-using-r-markdown/index.html#post-source",
    "title": "Writing Papers Using R Markdown",
    "section": "Post source",
    "text": "Post source\nThe source files for this blog post can be found at: Rmd, md, and html.\nPosted on October 9, 2012, at http://robertmflight.blogspot.com/2012/10/writing-papers-using-r-markdown.html\nEdit: added section on formatting numerical results\nEdit: added session info\n\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Pop!_OS 22.04 LTS\n\nMatrix products: default\nBLAS:   /rmflight_stuff/software/R-4.2.1/lib/libRblas.so\nLAPACK: /rmflight_stuff/software/R-4.2.1/lib/libRlapack.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.6\n\nloaded via a namespace (and not attached):\n [1] compiler_4.2.1    pillar_1.8.1      tools_4.2.1       digest_0.6.29    \n [5] jsonlite_1.8.0    evaluate_0.16     lifecycle_1.0.2   tibble_3.1.8     \n [9] gtable_0.3.1      pkgconfig_2.0.3   rlang_1.0.5       DBI_1.1.3        \n[13] cli_3.4.0         rstudioapi_0.14   yaml_2.3.5        xfun_0.33        \n[17] fastmap_1.1.0     withr_2.5.0       stringr_1.4.1     dplyr_1.0.10     \n[21] knitr_1.40        generics_0.1.3    htmlwidgets_1.5.4 vctrs_0.4.1      \n[25] grid_4.2.1        tidyselect_1.1.2  glue_1.6.2        R6_2.5.1         \n[29] fansi_1.0.3       rmarkdown_2.16    farver_2.1.1      purrr_0.3.4      \n[33] magrittr_2.0.3    scales_1.2.1      htmltools_0.5.3   assertthat_0.2.1 \n[37] colorspace_2.0-3  renv_0.15.5       labeling_0.4.2    utf8_1.2.2       \n[41] stringi_1.7.8     munsell_0.5.0"
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html",
    "title": "I was Part of the Problem",
    "section": "",
    "text": "With the recent charges of sexual harassment against some high-profile individuals, and so many women coming forward with #metoo (and the understanding that this is really something almost all women have faced), I realized that my younger self was #partoftheproblem. I think many other men are part of the problem, even though they might not think so. I didn’t think I was part of the problem either. I hope that other men might read this and critically evaluate if they are #partoftheproblem. I also hope and pray that my own sons will do better at this if I teach them right."
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html#how-could-i-be",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html#how-could-i-be",
    "title": "I was Part of the Problem",
    "section": "How Could I Be?",
    "text": "How Could I Be?\nLet me be up front. I have never sexually assaulted anyone, let alone considered such a thing. But that’s not really the problem, because the way I acted towards women, I think they may have been scared that I might, as I have put tons of unwanted attention on several women over the years, starting with when I was 12 years old, in the sixth grade.\nI also want to be clear, I was a horrible guy friend to women (even if they didn’t think so). If I knew a girl had a boyfriend, well then, I would not even consider trying to hit on or express interest in that girl, and I was “friends” with plenty of women over the course of my school years who had boyfriends. But in most cases I secretly hoped they might dump their boyfriends and go out with me instead. Also, if I knew they didn’t have a boyfriend, and I found them remotely attractive, then I would do all I could to try to become friends with them in the hope to eventually become their boyfriend. So, my sole reason for being friends with women, really, was to eventually become romantically involved. That was my primary motivation. Looking back on it now, it makes me sick.\nI’ve never had a woman tell me she was assaulted by anyone either, but given my past behavior, even if someone I knew had, I don’t think my actions made me someone that a woman would trust to tell.\nLet me give you some examples of my behavior."
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html#grade-school",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html#grade-school",
    "title": "I was Part of the Problem",
    "section": "Grade School",
    "text": "Grade School\nIn 6th grade, I decided that I wanted a girlfriend, and I picked out one girl in my class who I wanted to be my girlfriend. I am very sure I never asked her out, to be my girlfriend, but I made sure to spend tons of time with her, and if I recall correctly, she eventually got the gist of my interest, and told me very clearly she wasn’t interested. But her telling me no did not stop my unwanted advances or attention. I am sure that I made her very uncomfortable the rest of that grade.\nIn middle school (7-9 at the time), I pretty much continued this process unabated. I would latch onto a girl that I found attractive, and make her the target of my affections, and pour out my unwanted attention upon her, not taking no for an answer. I only stopped after long periods of continued rejection, or when that person acquired a significant other. Although not an excuse for my actions, my tactics and hopes were largely fueled by rampaging hormones, way too many romantic comedies where the nice guy always got the girl by virtue of sheer persistence (this was the 90’s), and nascent exposure to pornography.\nI would find out girls numbers and call them without being asked. I would know where these girls were at all times through the day, even during lunch and between classes. I would find any excuse to be near them. Every sock-hop (weekly lunch time dance on gym floor) I would ask these girls to dance with me. I would give them valentines cards, Christmas cards, etc, in the hopes that they would realize what a great guy I was and go out with me.\nJust so we are clear, none of this got me any dates in grade school."
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html#undergraduate",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html#undergraduate",
    "title": "I was Part of the Problem",
    "section": "Undergraduate",
    "text": "Undergraduate\nNow I’ve graduated high-school, I’m heading off to a local university, with lots of women. I made lots of friends with girls who had boyfriends, in fact I think my circle of friends had way more girls in it than guys. But, I was always finding one girl who I wanted to date, and would make sure to spend extra time around them, helping them whenever possible, etc, and dropping subtle and not so subtle hints that I wanted to be their boyfriend. And there was always the hope that someone would break-up with their current boyfriend and find me, the faithful friend, waiting to comfort them.\nOver the course of this time, I had three women agree to be my date. Two of those did not result in an actual date, because I started acting like a stalker after they said yes, and they wisely stayed away. In the third case, we went out twice, but me calling at random hours, and showing up at her house un-announced because I thought she was really sad freaked her out, and she stopped talking to my creepy, stalkerish, clingy self."
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html#post-graduate",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html#post-graduate",
    "title": "I was Part of the Problem",
    "section": "Post-Graduate",
    "text": "Post-Graduate\nSomehow, it seems, by the time I got to my PhD, I had mostly given up on finding a girlfriend, settling down and getting married (really, that was my goal). I say mostly. I don’t know if I hadn’t met my now spouse in the first couple of months of my PhD that I would not have continued making unwanted advances on the women in my PhD program. (By the way, I met my spouse outside of work, at a Church actually, and was introduced by a mutual friend. In the 13 years I’ve known her, there are only a handful of days we haven’t talked to each other since we went on our first date)."
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html#the-real-problem",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html#the-real-problem",
    "title": "I was Part of the Problem",
    "section": "The Real Problem",
    "text": "The Real Problem\nAnd this is the real problem. Too many men, my past self included, think women owe them something for being their friend, for being a nice guy. For giving them any kind of attention, or any kind of help. Too many men believe these things, and then use their power and prestige, to demand things of women. Guys, women don’t owe you anything. They definitely don’t owe you sex or reciprocated romantic interest because of something you did for them. They are another person worthy of respect, simply because they are a person.\nIn addition, real life is not a romantic comedy. Non-romantic friendships are a good thing, because we need other peoples perspectives in our lives. So, if a woman tells you no, she doesn’t want to date you, accept it, and move on. Don’t make it awkward, especially if you are in the same work environment. Don’t assume that a woman is romantically interested just because she is friendly. I know, radical thought. Maybe try being friends, colleagues, whatever with no romantic intentions, and no expectations of them either. Don’t be #partoftheproblem."
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html#solutions",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html#solutions",
    "title": "I was Part of the Problem",
    "section": "Solutions",
    "text": "Solutions\nTeach your children that they can be friends with people of the opposite sex without being romantically involved, especially as they hit puberty. Teach them that no means no, not no means maybe in 3 weeks, or no means maybe if I try hard enough. And if you see other men engaging in putting unwanted attention on women, call them out on it, whatever form it may take. I wish someone had said something to me."
  },
  {
    "objectID": "posts/2017-10-18-i-was-part-of-the-problem/index.html#caveats",
    "href": "posts/2017-10-18-i-was-part-of-the-problem/index.html#caveats",
    "title": "I was Part of the Problem",
    "section": "Caveats",
    "text": "Caveats\nI realize that our general culture is really #partoftheproblem, when we have highly sexualized advertising (especially of women to men), and the idea that boys will be boys, tell jokes about sexual assault, and propagate the idea that women want it, based on how they act or dress. Those are all wrong too, and our culture needs to change.\nI also realize that some of what I describe about myself is rather mild in comparison to much of what gets reported, but that’s not the point. It is still unwanted attention, and I didn’t know how to take no for an answer. Those women didn’t want my attention, and I couldn’t accept that. If I had a different temperament, I don’t know what I would have done. Enough people realized it that some friends in Undergrad stopped being around me, but no one ever told me that what I was doing was wrong, and my parents weren’t involved enough in my so-called love life to know what was going on. If they had, I think they would have told me to knock it off and stop being an idiot."
  },
  {
    "objectID": "posts/2022-01-06-cairo-and-xquartz-in-mac-github-actions/index.html",
    "href": "posts/2022-01-06-cairo-and-xquartz-in-mac-github-actions/index.html",
    "title": "Cairo and XQuartz in Mac GitHub Actions",
    "section": "",
    "text": "I’ve been using GitHub actions to check and test some of my R packages on multiple architectures, which is nice because I work primarily on Linux.\nOne recent package uses the {Cairo} package to generate images that are subsequently used for visualization (Urbanek and Horner 2020).\nInterestingly, {Cairo} will install fine on MacOS, and then fail as soon as you do library(Cairo), complaining about not being able to load the {cairo.so} file.\nThankfully, it seems that the GitHub actions MacOS VMs have homebrew (“Homebrew” 2022) installed, which means we can use the xquartz brew (“Xquartz Homebrew Formulae” 2022) to install it, and have {cairo.so} available.\nTo add this to your GitHub actions yml, you should add these lines, and double check the brew syntax with the official docs.\n    steps:\n      - name: Install X11 dependencies on MacOS\n        if: runner.os == 'macOS'\n        run: |\n          brew install --cask xquartz\n\n\n\n\nReferences\n\n“Homebrew.” 2022. https://brew.sh/.\n\n\nUrbanek, Simon, and Jeffrey Horner. 2020. Cairo: R Graphics Device Using Cairo Graphics Library for Creating High-Quality Bitmap (PNG, JPEG, TIFF), Vector (PDF, SVG, PostScript) and Display (X11 and Win32) Output. https://CRAN.R-project.org/package=Cairo.\n\n\n“Xquartz Homebrew Formulae.” 2022. https://formulae.brew.sh/cask/xquartz#default.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2022,\n  author = {Robert M Flight},\n  title = {Cairo and {XQuartz} in {Mac} {GitHub} {Actions}},\n  date = {2022-01-06},\n  url = {https://rmflight.github.io/posts/2022-01-06-cairo-and-xquartz-in-mac-github-actions},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2022. “Cairo and XQuartz in Mac GitHub\nActions.” January 6, 2022. https://rmflight.github.io/posts/2022-01-06-cairo-and-xquartz-in-mac-github-actions."
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html",
    "href": "posts/2013-06-05-tim-hortons-density/index.html",
    "title": "Tim Hortons Density",
    "section": "",
    "text": "Inspired by this post, I wanted to examine the locations and density of Tim Hortons restaurants in Canada. Using Stats Canada data, each census tract is queried on Foursquare for Tims locations."
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#setup",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#setup",
    "title": "Tim Hortons Density",
    "section": "Setup",
    "text": "Setup\n\noptions(stringsAsFactors=F)\nrequire(timmysDensity)\nrequire(plyr)\nrequire(maps)\nrequire(ggplot2)\nrequire(geosphere)"
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#statistics-canada-census-data",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#statistics-canada-census-data",
    "title": "Tim Hortons Density",
    "section": "Statistics Canada Census Data",
    "text": "Statistics Canada Census Data\nThe actual Statistics Canada data at the dissemination block level can be downloaded from here. You will want to download the Excel format, read it, and then save it as either tab-delimited or CSV using a non-standard delimiter, I used a semi-colon (;).\n\ncensusData <- read.table(\"../timmysData/2011_92-151_XBB_XLSX.csv\", header=F, sep=\";\", quote=\"\")\ncensusData <- censusData[,1:17]\nnames(censusData) <- c(\"DBuid\", \"DBpop2011\", \"DBtdwell2011\", \"DBurdwell2011\", \"DBarea\", \"DB_ir2011\", \"DAuid\", \"DAlamx\", \"DAlamy\", \"DAlat\",\n                       \"DAlong\", \"PRuid\", \"PRname\", \"PRename\", \"PRfname\", \"PReabbr\", \"PRfabbr\")\ncensusData$DBpop2011 <- as.numeric(censusData$DBpop2011)\ncensusData$DBpop2011[is.na(censusData$DBpop2011)] <- 0\n\ncensusData$DBtdwell2011 <- as.numeric(censusData$DBtdwell2011)\ncensusData$DBtdwell2011[is.na(censusData$DBtdwell2011)] <- 0\n\nFrom this data we get block level:\n\npopulations (DBpop2011)\ntotal private dwellings (DBtdwell2011)\nprivale dwellings occupied by usual residents (DBurdwell2011)\nblock land area (DBarea)\ndissemination area id (DAuid)\nrepresentative point x coordinate in Lambert projection (DAlamx)\nrep. point y coordinate in Lambert projection (DAlamy)\nrep. point latitude (DAlat)\nrep. point longitude (DAlong)\n\nThis should be everything we need to do the investigation we want."
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#dissemination-area-long.-and-lat.",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#dissemination-area-long.-and-lat.",
    "title": "Tim Hortons Density",
    "section": "Dissemination Area Long. and Lat.",
    "text": "Dissemination Area Long. and Lat.\nWe need to find the unique dissemination areas, and get out their latitudes and longitudes for querying in other databases. Note that the longitude and latitude provided here actually are weighted representative locations based on population. However, given the size of them, I don’t think using them will be a problem for Foursquare. Because areas are what we have location data for, we will summarize everything at the area level, summing the population counts for all the blocks within an area.\n\nuniqAreas <- unique(censusData$DAuid)\n\nsummarizeArea <- function(areaID){\n  areaData <- censusData[(censusData$DAuid == areaID),]\n  outData <- data.frame(uid=areaID, lamx=areaData[1,\"DAlamx\"], lamy=areaData[1,\"DAlamy\"], lat=areaData[1,\"DAlat\"], long=areaData[1,\"DAlong\"], pop=sum(areaData[,\"DBpop2011\"]), dwell=sum(areaData[,\"DBtdwell2011\"]), prov=areaData[1, \"PRename\"])\n  return(outData)\n}\nareaData <- adply(uniqAreas, 1, summarizeArea)\n.sessionInfo <- sessionInfo()\n.timedate <- Sys.time()\nwrite.table(areaData, file=\"../timmysData/areaData.txt\", sep=\"\\t\", row.names=F, col.names=T)\nsave(areaData, .sessionInfo, .timedate, file=\"../timmysData/areaDataFile.RData\", compress=\"xz\")"
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#run-queries-on-foursquare",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#run-queries-on-foursquare",
    "title": "Tim Hortons Density",
    "section": "Run queries on Foursquare",
    "text": "Run queries on Foursquare\n\nLoad up the data and verify what we have.\n\nload(\"../timmysData/areaDataFile.RData\")\nhead(areaData)\n\n\n\nGenerate queries and run\nFor each dissemination area (DA), we are going to use as the location for the query the latitude and longitude of each DA, as well as the search string “tim horton”.\nBecause Foursquare limits the number of userless requests to 5000 / hr. To make sure we stay under this limit, the runQueries function will only 5000 queries an hour.\n\nrunQueries(areaData, idFile=\"../timmysData/clientid.txt\", secretFile=\"../timmysData/clientsecret.txt\", outFile=\"../timmysData/timmysLocs2.txt\")\n\n\n\nClean up the results\nDue to the small size of the DAs, we have a lot of duplicate entries. Now lets remove all the duplicate entries.\n\ncleanUpResults(\"../timmysData/timmysLocs2.txt\")"
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#visualize-locations",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#visualize-locations",
    "title": "Tim Hortons Density",
    "section": "Visualize Locations",
    "text": "Visualize Locations\nFirst lets read in the data and make sure that we have Tims locations.\n\n# read in and clean up the data\ntimsLocs <- scan(file=\"../timmysData/timmysLocs2.txt\", what=character(), sep=\"\\n\")\ntimsLocs <- strsplit(timsLocs, \":\")\n\ntimsName <- sapply(timsLocs, function(x){x[1]})\ntimsLat <- sapply(timsLocs, function(x){x[2]})\ntimsLong <- sapply(timsLocs, function(x){x[3]})\n\nlocData <- data.frame(description=timsName, lat=as.numeric(timsLat), long=as.numeric(timsLong))\nhasNA <- is.na(locData[,\"lat\"]) | is.na(locData[,\"long\"])\nlocData <- locData[!(hasNA),]\n\ntimsStr <- c(\"tim hortons\", \"tim horton's\")\n\nhasTims <- (grepl(timsStr[1], locData$description, ignore.case=T)) | (grepl(timsStr[2], locData$description, ignore.case=T))\n\nlocData <- locData[hasTims,]\ntimsLocs <- locData\nrm(timsName, timsLat, timsLong, hasNA, locData, hasTims, timsStr)\n.timedate <- Sys.time()\n.sessionInfo <- sessionInfo()\nsave(timsLocs, .timedate, .sessionInfo, file=\"../timmysData/timsLocs.RData\", compress=\"xz\")\n\n\nPut them on a map\n\ndata(timsLocs)\ndata(areaDataFile)\ncanada <- map_data(\"world\", \"canada\")\n\np <- ggplot(legend=FALSE) +\n  geom_polygon( data=canada, aes(x=long, y=lat,group=group)) +\n  theme(panel.background = element_blank()) +\n  theme(panel.grid.major = element_blank()) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.text.x = element_blank(),axis.text.y = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  xlab(\"\") + ylab(\"\")\n\nsp <- timsLocs[1, c(\"lat\", \"long\")]\n\np2 <- p + geom_point(data=timsLocs[,c(\"lat\", \"long\")], aes(x=long, y=lat), colour=\"green\", size=1, alpha=0.5)\n\nprint(p2)\n\n\n\n\n\n\n\n\nHow far??\nAnd now lets also calculate the minimum distance of a given DA from Timmys locations.\n\nqueryLocs <- matrix(c(timsLocs$long, timsLocs$lat), nrow=nrow(timsLocs), ncol=2, byrow=F) # these are the tims locations\ndistLocs <- matrix(c(areaData$long, areaData$lat), nrow=nrow(areaData), ncol=2, byrow=F) # the census centers\nallDists <- apply(queryLocs, 1, function(x){\n  min(distHaversine(x, distLocs)) # only need the minimum value to determine \n})\n\nFrom the allDists variable above, we can determine that the maximum distance any census dissemination area (DA) is from a Tim Hortons is 51.5 km (31.9815 miles). This is based on distances calculated “as the crow flies”, but still, that is pretty close. Assuming roads, the furthest a Canadian should have to travel is less than an hour to get their Timmys fix.\n\ntotPopulation <- sum(areaData$pop, na.rm=T)\nlessDist <- seq(50, 51.6 * 1000, 50) # distances are in meters, so multiply by 1000 to get reasonable km\n\npercPop <- sapply(lessDist, function(inDist){\n  isLess <- allDists < inDist\n  sum(areaData$pop[isLess], na.rm=T) / totPopulation * 100\n})\n\nplotDistPerc <- data.frame(distance=lessDist, population=percPop, logDist=log10(lessDist))\nggplot(plotDistPerc, aes(x=logDist, y=population)) + geom_point() + xlab(\"Log10 Distance\") + ylab(\"% Population\")\n\n\n\n\n\n\nWhat gets really interesting, is how much of the population lives within a given distance of a Timmys. By summing up the percentage of the population within given distances. The plot above shows that 50% of the population is within 316.227766 meters of a Tim Hortons location.\nI guess Canadians really do like their Tim Hortons Coffee (and donuts!)."
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#replication",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#replication",
    "title": "Tim Hortons Density",
    "section": "Replication",
    "text": "Replication\nAll of the necessary processed data and code is available in the R package timmysDensity. You can install it using devtools. The original data files are linked in the relevant sections above.\n\nlibrary(devtools)\ninstall_github('timmysDensity', 'rmflight')\n\n\nCaveats\nI originally did this work based on a different set of data, that I have not been able to locate the original source for. I have not compared these results to that data to verify their accuracy. When I do so, I will update the package, vignette and blog post."
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#posted",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#posted",
    "title": "Tim Hortons Density",
    "section": "Posted",
    "text": "Posted\nThis work exists as the vignette of timmysDensity, on my web-blog, and independently as the front page for the GitHub repo."
  },
  {
    "objectID": "posts/2013-06-05-tim-hortons-density/index.html#disclaimer",
    "href": "posts/2013-06-05-tim-hortons-density/index.html#disclaimer",
    "title": "Tim Hortons Density",
    "section": "Disclaimer",
    "text": "Disclaimer\nTim Hortons was not involved in the creation or preparation of this work. I am not regularly updating the location information obtained from Foursquare, it is only valid for May 31, 2013. All code used in preparing these results was written by me, except in the case where code from other R packages was used. All opinions and conclusions are my own, and do not reflect the views of anyone else or any institution I may be associated with."
  },
  {
    "objectID": "posts/2022-06-22-compiling-against-pythonh/index.html",
    "href": "posts/2022-06-22-compiling-against-pythonh/index.html",
    "title": "Compiling Against Python.h",
    "section": "",
    "text": "Sometimes, the installed version of Python on your linux system isn’t what you need, and you can’t easily install a newer version system-wide. Of course, you can download, extract, and configure and then make Python that lives in it’s own directory. But then you don’t have all the pointers to the Include directories that are useful for linking stuff against. And you don’t want to change symlinks at the system level, that might break things, a lot of things.\nIf you are using cython to speed up your code, then it needs to know where exactly to find those extra header files. Unfortunately, it needs both the C++ and the C versions, it seems.\nLets go through the steps of setting up a new python, and then linking it for compiling cythonized programs."
  },
  {
    "objectID": "posts/2022-06-22-compiling-against-pythonh/index.html#download-and-make",
    "href": "posts/2022-06-22-compiling-against-pythonh/index.html#download-and-make",
    "title": "Compiling Against Python.h",
    "section": "Download and Make",
    "text": "Download and Make\nDownload python, and navigate to where you saved it. I’ll assume we downloaded version 10 to ~/software\ncd ~/software\ntar -xzf Python-3.10.5.tgz\ncd Python-3.10.5\n./configure --enable-optimizations\nmake"
  },
  {
    "objectID": "posts/2022-06-22-compiling-against-pythonh/index.html#setup-new-virtualenv-with-it",
    "href": "posts/2022-06-22-compiling-against-pythonh/index.html#setup-new-virtualenv-with-it",
    "title": "Compiling Against Python.h",
    "section": "Setup New Virtualenv With It",
    "text": "Setup New Virtualenv With It\nMake sure you have virtualenv installed, its necessary if you want to specify a specific version of python. Yes, you could probably get around this with conda or docker, but we are doing things the painful way.\nvirtualenv -p ~/software/Python-3.10.5/python ~/py10_venv\nsource ~/py10_venv/activate"
  },
  {
    "objectID": "posts/2022-06-22-compiling-against-pythonh/index.html#add-it-to-environment",
    "href": "posts/2022-06-22-compiling-against-pythonh/index.html#add-it-to-environment",
    "title": "Compiling Against Python.h",
    "section": "Add It to Environment",
    "text": "Add It to Environment\nexport C_INCLUDE_PATH=/home/rmflight/software/Python-3.10.5:$C_INCLUDE_PATH"
  },
  {
    "objectID": "posts/2022-06-22-compiling-against-pythonh/index.html#install-something-cython",
    "href": "posts/2022-06-22-compiling-against-pythonh/index.html#install-something-cython",
    "title": "Compiling Against Python.h",
    "section": "Install Something Cython",
    "text": "Install Something Cython\nWe are going to install a package that needs only a few python deps, our labs icikt package.\n# check that you are using the right version in the venv\npython3 --version\npython3 -m pip install docopt\npython3 -m pip install scipy\npython3 -m pip install cython\ncd ~/software\ngit clone git@github.com:MoseleyBioinformaticsLab/icikt.git\npython3 -m pip install --global-option=build_ext --global-option=\"-I/home/rmflight/software/Python-3.10.5/Include\" ~/software/icikt\nNotice we’ve done two very important things overall:\n\nAdded a path, C_INCLUDE_PATH\nAdded two --global-option in the call to build our local package\n\nFor some reason we need both the environment variable path, and the global option to get this to work.\nIf you forget one or the other, you will see error messages concerning python.h"
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "",
    "text": "A blog post on the Weecology group blog by Elita Baldridge on being a PhD student with fibromyalgia, and how they are working through that, caused me to pause and reflect on my experience as a PhD student and PostDoc with migraines. For those who haven’t read my blog, I do research in bioinformatics, specifically in transcriptomics and metabolomics. I spend almost all of my research hours in front of a computer writing code, generating plots, and trying to make sense of -omics level data."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#tldr",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#tldr",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "TL;DR",
    "text": "TL;DR\nBeing in academia with migraines is a challenge, but probably less challenging than some other fields or disabilities. Moving cities and discovering Excedrin has made my migraines more bearable."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#migraine",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#migraine",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "Migraine",
    "text": "Migraine\nFor those who don’t know, migraines are a rather unusual neurological event that result in a bunch of symptoms, the most well known generally being aura (frequently visual, although auditory and olfactory are also known) followed by what is described as intense, debilitating pain, which may or may not be accompanied by extreme nausea and / or vomiting and tiredness."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#my-migraines",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#my-migraines",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "My Migraines",
    "text": "My Migraines\nI have suffered from migraines since at least my early teens (possibly younger), but mine seem to be rather mild in comparison to what I have read about others experiences. In contrast to many other migraineurs, I do not experience visual aura, and in general my pain levels tend to be on the milder side. However, I do experience mood swings during my prodrome (period prior to pain), going from a generally nice guy to someone who considers killing you just for looking at me wrong; as well as confusion, and I also experience expressive aphasia, wherein I can no longer remember proper nouns, but can describe properties of the object in question (for example, instead of a pencil, I might say “the writing implement with a lead center”). In addition to the pain of the migraine, which is often a pounding localized to one location of my skull, I often also experience extreme sensitivity to touch, to the point of not being able to sleep on a few occasions; as well as nausea, however never to the point of actually throwing up. Less frequently I will experience sensitivity to light or sound.\nAs far as I can tell, my primary migraine trigger is changes in the weather, particularly large or frequent changes in barometric pressure. If the weather is stable (either nice or bad), the frequency and severity of my migraines are greatly reduced."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#phd",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#phd",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "PhD",
    "text": "PhD\nDuring my undergraduate and masters degrees, I actually didn’t notice that much of an effect from my migraines. At some point, after a lot of investigation of my headaches, and alternative therapies, I did finally get a diagnosis of migraine, and a prescription for Maxalt, a triptan used to treat migraines. I used this very sparingly, due to the side effects, and the cost.\nMy PhD was at Dalhousie University in Halifax, NS, Canada. If you don’t know, Halifax is a harbor city on the east coast of Nova Scotia, and as a result of its location has extremely variable weather, as it gets the west -> east weather from the rest of Canada, as well as weather systems going south -> north up the eastern seaboard of the United States. This results in a large amount of barometric pressure changes, and resulted in a lot of migraines for me. In general, my coping strategy when possible was to go home, take 2 tylenol, and go to bed for the rest of the day and sleep it off. As a PhD student in an analytical chemistry lab doing no wet lab work but only programming, this was often a viable option. My PhD supervisor was very supportive, in that as long as grades were good and project progress was being made, he was not particular about hours spent in the lab.\nOf course, as a PhD student, there were often times this was not possible. I took regular upper level classes in my first 2 years, and I never informed any of my instructors about my migraines, but often just suffered through classes when necessary. In addition, I TA’d first and second year chemistry lab. Given the other symptoms of my migraines, I actually feel sorry for the students I instructed. I know many times I had trouble resolving what were probably trivial mistakes by first year students, and losing my cool over something that was not that difficult to fix. Second year was a bit better in that the students had more experience in a lab setting, however I TA’d analytical chemistry, which essentially involved generating a series of standards, measuring the standards, and then measuring a sample with unknown concentration. There was generally fewer ways for any given lab experiment to fail in the 2nd year lab, putting a lighter cognitive load on me most days."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#seminars-and-conferences",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#seminars-and-conferences",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "Seminars and Conferences",
    "text": "Seminars and Conferences\nOf course, as a student I had to attend weekly departmental seminars, and present a couple of times as well. I also traveled to a few conferences and presented talks and posters. To my recollection, I have not suffered a severe migraine during my own talks at a conference. However, I have had different occasions where I have had to skip out on conference sessions due to a migraine, and have been even less likely to socialize and network than I usually would because of migraines. This is hard, because one of the primary purposes of attending conferences is exposure to new research and people."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#postdoc",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#postdoc",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "PostDoc",
    "text": "PostDoc\nMy first PostDoc was at the University of Louisville in Louisville, KY, USA. My move to the US coincided with two things: 1) The introduction of Excedrin Migraine, and 2) My wife giving birth to our son. 1 is important because Excedrin has allowed me to manage my migraines. The side effects from Maxalt were almost as bad as the migraine itself, and it was bloody expensive (even with my wifes great drug plan). As a PostDoc supporting my wife and son, I always stopped to think about whether I actually needed it. Excedrin Migraine worked quite well in alleviating the pain from my migraines without introducing new side effects. I should note at this time that I was not a regular tea or coffee drinker, so the effect is not likely due to a caffeine addiction.\nThen I discovered that the regular Excedrin worked about the same, and was available in a generic form at many big box stores at an extremely low price ($4 for 100 tablets). I haven’t taken any Maxalt in over 3 years, and probably buy the generic Excedrin 100 count for $4 at WalMart every few months. Is this good for my stomach?? Maybe not. Do I need to watch myself if I get a cut within a short amount of time after taking Excedrin (ASA inhibits clotting)?? Yep. But it seems to mess with my head a lot less than the triptans I have taken (I have also tried Zomig in addition to the Maxalt).\n2 is important because the arrival of our son resulted in a lot of increased stress (and don’t get me wrong, a lot of happiness and joy as well). Although not a primary trigger for my migraines, it definitely did not help. And having a migraine with a crying baby in the background is a special kind of torture (I was reminded of this again the other day with my 8 month old daughter).\nHowever, the move to the midwest US has resulted in a general decrease in the severity of my migraines, but I’m not sure about the frequency. Since moving here, I’ve only had to leave work because of a migraine once or twice a year, instead of the once a month that I used to in Nova Scotia.\nObviously, my productivity at work during a migraine is reduced compared to not being in a migraine. However, there are often periods in academia when things just need to get done (collaborator has a paper or grant deadline, etc), and going home for me means a combined hour of walking and public transit, and a very small likelyhood of getting anything done even after the migraine has passed (having a 4 year old and 8 month old contributes to this a lot, but I wouldn’t have it any other way)."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#challenges",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#challenges",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "Challenges",
    "text": "Challenges\nAs I previously said, my migraines appear to be rather mild based on the descriptions of other peoples migraines I have encountered (see the migraine subreddit for some painful descriptions from other migaineurs). However, they can make some interactions difficult. My normally cheerful and upbeat demeanor can quickly turn sour, and I have been acutely aware of some rather awkward social interactions during a migraine. Thankfully, many times I am able to simply avoid others at work when I have a migraine, and I generally take pains to think twice before speaking as well (which is a good thing to do in general). I don’t teach, and the number of meetings that I have to attend in a given week is rather small, thankfully (not just because of my migraines).\nThe pain and mental confusion from my migraines can make getting work done difficult, and although Excedrin seems to do a wonderful job of dulling the pain, nothing can help with the mental confusion. Having a migraine in the middle of working on a large data analysis with many possible directions, or managing many different projects takes an incredible amount of mental effort when I am in a migraine. I have had times when I sit in front of my computer completely stunned because I cannot figure out what needs to be done next. At these points I frequently occupy myself with mindless tasks that need to be done, sending meeting emails, downloading papers to read, and trying to stay off of social media time sucks like Facebook and Reddit, as I tend to get heavily engrossed in those more so than usual."
  },
  {
    "objectID": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#prophylactic",
    "href": "posts/2014-12-31-being-a-phd-student-and-post-doc-with-migraines/index.html#prophylactic",
    "title": "Being a PhD Student and Post-Doc with Migraines",
    "section": "Prophylactic",
    "text": "Prophylactic\nThis past year I finally tried CBD (cannibidiol) oil, an oil where only CBD has been extracted from the marijuana plant. Taking this twice a day, every day, has greatly reduced the number of migraines I have, and seems to reduce their intensity as well. It’s greatest effect is on the pain levels I encounter, and not so much the other neurological effects of the migraines.\nUpdated on 2017-12-28 with information on CBD oil."
  },
  {
    "objectID": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html",
    "href": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html",
    "title": "First Open Post-Publication Peer Review, with Credit!",
    "section": "",
    "text": "Reviewed Jason McDermott’s MDRPred paper on F1000Research!, where my review is posted along side the paper, with a DOI, completely in the open with my name attached. Was a pleasant experience, aided by the fact that Jason wrote a good paper."
  },
  {
    "objectID": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#f1000research",
    "href": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#f1000research",
    "title": "First Open Post-Publication Peer Review, with Credit!",
    "section": "F1000Research!",
    "text": "F1000Research!\nF1000Research! is a new publishing startup from F1000 that has a model of post-publication peer review, whereby upon submission the manuscript undergoes basic quality checks (no real editorial control), and then is published. Once the article is published, reviewers are invited to review, and they have 10 days to submit their review. Reviews are signed, and given a DOI. Reviewers are asked to assign one of Approved, Approved with Reservations, Not Approved status to the article.\nWhen the article gets two revieweres giving Approved status (likely after at least one round of review and resubmission), then it will be submitted to PubMed for indexing."
  },
  {
    "objectID": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#how-did-i-end-up-reviewing-this",
    "href": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#how-did-i-end-up-reviewing-this",
    "title": "First Open Post-Publication Peer Review, with Credit!",
    "section": "How Did I End Up Reviewing This??",
    "text": "How Did I End Up Reviewing This??\nLong story short, I’ve been following Jason on twitter for a while, and I happened to see him tweet and blog about trying to get an F1000Research article up as a citeable supporting publication for a grant going in. I thought it was a nifty idea, and although I actually at the time did not have much of an idea of what Jason’s research actually involved, communicated with him that when the article went live I would be willing to be recommended as a reviewer. Imagine my surprise that I actually had enough domain knowledge that I could actually review the article.\nThat is saying something, given that the paper has a neat combination of genetic algorithms, regular expressions, and protein function prediction (really, you should go give it a read). By the way, this is also the first paper I’ve reviewed in a long time that did not have serious methodological problems, or where claims are made with no substantiation, and I could not identify any serious statistical issues."
  },
  {
    "objectID": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#the-open-part",
    "href": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#the-open-part",
    "title": "First Open Post-Publication Peer Review, with Credit!",
    "section": "The Open Part",
    "text": "The Open Part\nAlthough I don’t do a lot of reviewing, I had to admit that this was one of the best reviewing assignments I’ve had in a while. Although I believe that peer review is essential to science, and it needs to be more open (I’ve started signing my reviews for other journals), this was the first time I knew my review was open (my name would be known to the authors’ by default) and public. I have to say that this likely improved the care and thoroughness in doing the actual review as I went through the article, given that both the authors and anyone else who comes across the article and my review can see if the issues I raise are real, or if I’m trying to make myself look good. And my name will be publicly associated with it!\nNote that this openness did not keep me from criticizing particular aspects of the paper. There are lots of things that need to be changed in the article before I will give it an Approved status, and I laid those out in my review."
  },
  {
    "objectID": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#postpublication",
    "href": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#postpublication",
    "title": "First Open Post-Publication Peer Review, with Credit!",
    "section": "PostPublication",
    "text": "PostPublication\nI really appreciated that I get to review a published article, because it means that the whole thing is typeset, figures and tables are in their logical place (not at the end of the document!!!), the line spacing is readable, etc. Note to other publishers, at least let authors put the figures in-line for review if you are not going to type-set the article before it goes to reviewers."
  },
  {
    "objectID": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#disclaimers",
    "href": "posts/2015-03-25-first-open-post-publication-peer-review-with-credit/index.html#disclaimers",
    "title": "First Open Post-Publication Peer Review, with Credit!",
    "section": "Disclaimers",
    "text": "Disclaimers\nI was given an F1000Research! t-shirt as a reward for commenting on a blog-post regarding incentives for peer-review. I like the shirt. Having done my review, I am also eligible to get a 50% discount on the article processing charges if I submit an article to F1000Research in the next 12 months."
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html",
    "title": "Zooming GGraph Plots",
    "section": "",
    "text": "I was recently working with a largish graph that I am using the ggraph package, and I needed to zoom into a sub-region of the graph."
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html#setup",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html#setup",
    "title": "Zooming GGraph Plots",
    "section": "Setup",
    "text": "Setup\n\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.width = 10, fig.height = 8)\nexample_df = structure(list(from = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 7L, 7L, 7L, \n                                     7L, 7L, 7L, 7L, 7L, 7L, 8L, 9L, 10L, 11L, 12L, 12L, 12L, 13L, \n                                     13L, 13L, 13L, 13L, 13L, 13L, 14L, 15L, 16L, 16L, 17L, 17L, 18L, \n                                     18L, 19L, 20L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, \n                                     21L, 21L, 22L, 22L, 6L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, \n                                     23L, 23L, 23L, 23L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, \n                                     24L, 24L, 24L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, \n                                     25L, 25L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, \n                                     26L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, \n                                     28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 29L, \n                                     29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 30L, 30L, \n                                     30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 31L, 31L, 31L, \n                                     31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 31L, 32L, 32L, 32L, 32L, \n                                     32L, 32L, 32L, 32L, 32L, 32L, 32L, 32L, 33L, 33L, 33L, 33L, 33L, \n                                     33L, 33L, 33L, 33L, 33L, 33L, 33L, 34L, 34L, 34L, 34L, 34L, 34L, \n                                     34L, 34L, 34L, 34L, 34L, 34L, 35L, 35L, 35L, 35L, 35L, 35L, 35L, \n                                     35L, 35L, 35L, 35L, 35L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, 36L, \n                                     36L, 36L, 36L, 36L, 37L, 37L, 37L, 37L, 37L, 37L, 37L, 37L, 37L, \n                                     37L, 37L, 37L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, 38L, \n                                     38L, 38L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, 39L, \n                                     39L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, 40L, \n                                     41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 41L, 42L, \n                                     22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, \n                                     22L, 22L, 22L, 22L, 43L, 43L, 44L, 44L, 45L, 45L, 46L, 46L, 47L, \n                                     47L, 48L, 48L, 49L, 49L, 50L, 50L, 51L, 52L, 52L, 53L, 53L, 54L, \n                                     54L, 55L, 55L, 56L, 57L, 57L, 58L, 58L, 59L, 60L, 61L, 61L, 62L, \n                                     16L, 17L, 18L, 63L, 64L, 65L, 66L, 67L, 68L, 69L, 70L, 71L, 72L, \n                                     73L, 74L, 75L, 76L, 20L, 77L, 77L, 78L, 79L, 79L, 80L, 81L, 82L, \n                                     7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 14L, 15L, 19L, 21L, 21L, \n                                     22L, 83L, 84L, 45L, 46L, 47L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, \n                                     56L, 57L, 58L, 59L, 85L, 48L, 48L, 48L, 48L, 48L, 46L, 46L, 46L, \n                                     46L, 86L, 86L, 13L, 13L, 22L, 7L, 55L, 55L, 55L, 55L, 55L, 51L, \n                                     51L, 51L, 58L, 58L, 58L, 58L, 53L, 53L, 53L, 53L, 49L, 49L, 49L, \n                                     49L, 87L, 87L), to = c(7L, 7L, 7L, 61L, 61L, 61L, 8L, 88L, 89L, \n                                                            90L, 91L, 92L, 93L, 94L, 95L, 96L, 9L, 7L, 9L, 9L, 16L, 17L, \n                                                            18L, 14L, 15L, 16L, 17L, 18L, 19L, 10L, 10L, 10L, 10L, 42L, 10L, \n                                                            42L, 10L, 42L, 10L, 12L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, \n                                                            54L, 55L, 57L, 58L, 6L, 9L, 9L, 45L, 46L, 47L, 48L, 49L, 50L, \n                                                            52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, \n                                                            53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, \n                                                            54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, \n                                                            55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, \n                                                            57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, \n                                                            58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, \n                                                            45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, \n                                                            46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, \n                                                            47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, \n                                                            48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, \n                                                            49L, 50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, \n                                                            50L, 52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, \n                                                            52L, 53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, \n                                                            53L, 54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, \n                                                            54L, 55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, \n                                                            55L, 57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, \n                                                            57L, 58L, 45L, 46L, 47L, 48L, 49L, 50L, 52L, 53L, 54L, 55L, 57L, \n                                                            58L, 9L, 97L, 98L, 99L, 100L, 101L, 4L, 5L, 102L, 10L, 103L, \n                                                            104L, 105L, 60L, 106L, 107L, 108L, 109L, 13L, 22L, 110L, 83L, \n                                                            21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 44L, \n                                                            21L, 44L, 21L, 44L, 21L, 44L, 21L, 44L, 44L, 21L, 44L, 21L, 44L, \n                                                            44L, 111L, 111L, 112L, 60L, 83L, 83L, 83L, 22L, 22L, 22L, 22L, \n                                                            22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 83L, 83L, 110L, \n                                                            83L, 83L, 110L, 83L, 83L, 22L, 22L, 113L, 114L, 115L, 116L, 117L, \n                                                            118L, 119L, 120L, 121L, 122L, 83L, 83L, 83L, 86L, 2L, 123L, 78L, \n                                                            21L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, 84L, \n                                                            84L, 84L, 10L, 79L, 124L, 86L, 125L, 84L, 79L, 124L, 86L, 125L, \n                                                            110L, 83L, 42L, 11L, 126L, 111L, 48L, 79L, 124L, 86L, 125L, 79L, \n                                                            86L, 125L, 79L, 124L, 86L, 125L, 79L, 124L, 86L, 125L, 79L, 124L, \n                                                            86L, 125L, 46L, 48L)), class = \"data.frame\", row.names = c(NA, \n                                                                                                                       -432L))\n\nlibrary(ggraph)\n\nLoading required package: ggplot2\n\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(igraph)\n\n\nAttaching package: 'igraph'\n\n\nThe following object is masked from 'package:tidygraph':\n\n    groups\n\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\n\nThe following object is masked from 'package:base':\n\n    union\n\nexample_graph = as_tbl_graph(example_df)"
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html#initial-view",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html#initial-view",
    "title": "Zooming GGraph Plots",
    "section": "Initial View",
    "text": "Initial View\nLet’s generate a plot of this to start.\n\nset.seed(1234)\nggraph(example_graph, \"graphopt\") +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point()\n\n\n\n\nYou can see the bottom left section of this graph looks like a mess. It sure would be awesome if we could zoom into that region, and replot it."
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html#find-those-nodes",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html#find-those-nodes",
    "title": "Zooming GGraph Plots",
    "section": "Find Those Nodes",
    "text": "Find Those Nodes\nWe will use igraphs community detection to break this into community sets and select the correct group.\n\nwalk_membership = cluster_walktrap(example_graph)\nwalk_communities = membership(walk_membership)\nsplit_comms = split(names(walk_communities), walk_communities)\nnames(split_comms) = NULL\n\nwhich_comm = split_comms[[which(purrr::map_lgl(split_comms, ~ \"58\" %in% .x))]]"
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html#initial-layout",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html#initial-layout",
    "title": "Zooming GGraph Plots",
    "section": "Initial Layout",
    "text": "Initial Layout\nOne way to make sure we can get the same layout is to create the layout, which is treated as a graph by ggraph.\n\n# we set the seed so its the same layout as previous\nset.seed(1234)\nexample_layout = create_layout(example_graph, \"graphopt\")\n\nggraph(example_layout) +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point()\n\n\n\n\nAwesome, looks the same as the previous."
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html#subset-nodes-and-get-range",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html#subset-nodes-and-get-range",
    "title": "Zooming GGraph Plots",
    "section": "Subset Nodes and Get Range",
    "text": "Subset Nodes and Get Range\n\nsub_layout = example_layout %>%\n  dplyr::filter(name %in% which_comm)\nsub_xlim = range(sub_layout$x)\nsub_ylim = range(sub_layout$y)"
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html#plot-subset",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html#plot-subset",
    "title": "Zooming GGraph Plots",
    "section": "Plot Subset",
    "text": "Plot Subset\nWe would think we should be able to just use the sub-layout to plot the subset of nodes. Lets try that first.\n\nggraph(sub_layout) +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point()\n\n\n\n\nAs we can see, this isn’t quite right. We are missing some of the edges."
  },
  {
    "objectID": "posts/2021-11-11-zooming-ggraph-plots/index.html#plot-using-subset-ranges",
    "href": "posts/2021-11-11-zooming-ggraph-plots/index.html#plot-using-subset-ranges",
    "title": "Zooming GGraph Plots",
    "section": "Plot Using Subset Ranges",
    "text": "Plot Using Subset Ranges\n\nggraph(example_layout) +\n  geom_edge_link(arrow = arrow(length = unit(2, 'mm')),\n                end_cap = circle(10, \"mm\")) +\n  geom_node_point() +\n  coord_cartesian(xlim = sub_xlim, ylim = sub_ylim)\n\n\n\n\nFinally! This is what we wanted. And we can see why it’s so overlapped in the full plot! It’s pretty much a hairball in that part of the graph."
  },
  {
    "objectID": "posts/2014-07-28-packages-vs-projecttemplate/index.html",
    "href": "posts/2014-07-28-packages-vs-projecttemplate/index.html",
    "title": "Packages vs ProjectTemplate",
    "section": "",
    "text": "Imposing a different structure than R packages for distributing R code is a bad idea, especially now that R package tools have gotten to the point where managing a package has become much easier."
  },
  {
    "objectID": "posts/2014-07-28-packages-vs-projecttemplate/index.html#projecttemplate",
    "href": "posts/2014-07-28-packages-vs-projecttemplate/index.html#projecttemplate",
    "title": "Packages vs ProjectTemplate",
    "section": "ProjectTemplate ??",
    "text": "ProjectTemplate ??\nMy last two posts (1, 2) provided an argument and an example of why one should use R packages to contain analyses. They were partly motivated by trends I had seen in other areas, including the appearance of the package ProjectTemplate. I was reminded about it thanks to Hadley Wickham:\n\nwould be interesting to see comparison to ProjectTemplate\n\nJohn Myles White has an R package, ProjectTemplate for handling analyses. In contrast to the package philosophy that I espoused previously, ProjectTemplate has a large, logical folder layout (you can read all about each folder and why it exists here). There is a folder for raw data, a folder for R scripts, a folder for reports, etc.\nHadley probably wanted me to do the same analysis using ProjectTemplate, but I don’t have the time for that. Instead I’m going to write below about the features ProjectTemplate has, and why I think packages are a better general solution. So, just to be clear, I have not done any analyses using ProjectTemplate, I have only the website descriptions to go on.\nNow, I think I understand the philosophy of ProjectTemplate, in that when writing a manuscript or report in an external editor such as LaTex, Word, LibreOffice (or even something else), you want a directory of outputs (graphs, tables, etc) resulting from applying a series of transformations on some data (R scripts applied to .txt or .csv, etc). In addition, a package is for storing general functions that will work across multiple projects."
  },
  {
    "objectID": "posts/2014-07-28-packages-vs-projecttemplate/index.html#differences",
    "href": "posts/2014-07-28-packages-vs-projecttemplate/index.html#differences",
    "title": "Packages vs ProjectTemplate",
    "section": "Differences",
    "text": "Differences\n\nKeeping Directories Straight\nI can understand this POV, and in the past would have largely agreed. However, with the development of devtools and other tools that make managing R packages rather easy, I think it makes more sense to use the R package mechanism instead of a custom format. In the ProjectTemplate you may write functions or code in the /lib, /munge or /src directory depending on their purpose (general, data munging, actual analysis, respectively). Keeping all of this straight seems to me a waste of time from doing the actual analysis.\nIn a package, functions are in the /R directory, and that is where they live. They may be organized into different files depending on their purpose, names, or methods and classes (not fun S4, not fun), but at least I know where they are.\n\n\nDocumentation\nThis for me is the kicker. With packages (and roxygen2 and devtools), I can document everything in such a way that the documentation is available without looking at the source file; (i.e. ?function) be reminded of the various arguments, or look up the properties of my documented data set.\n\n\nDependencies\nR packages naturally tell you what other packages they depend on, and will try to resolve those dependencies when you install them. Now, granted, this isn’t a perfect process (anyone trying to upgrade their full R package library knows this), and with the proliferation of packages on github is likely to get more difficult (hmmm, would be nice to have a central registry of github available packages), but in general it does work (for a different solution, check out packrat. This means that an analysis that is a package can naturally get it’s dependencies at install time (really try it, go ahead and install my example package without having stringr installed).\nIn contrast, ProjectTemplate assumes all of the required packages are already installed, and if you shared your analysis directory with someone else, they have to look at the list and install everything before they are able to run it (although this is probably not that bad)."
  },
  {
    "objectID": "posts/2014-07-28-packages-vs-projecttemplate/index.html#package-reproducibility",
    "href": "posts/2014-07-28-packages-vs-projecttemplate/index.html#package-reproducibility",
    "title": "Packages vs ProjectTemplate",
    "section": "Package Reproducibility",
    "text": "Package Reproducibility\nIn this day and age of seeking to provide reproducible analyses, having all of the entities (code, data, final report) in a container that knows how to install it’s dependencies, and that works within the language ecosystem to provide documentation on the functions used, seems really useful. I believe that R packages provide that better than most other solutions I have seen lately."
  },
  {
    "objectID": "posts/2014-07-28-packages-vs-projecttemplate/index.html#outputs",
    "href": "posts/2014-07-28-packages-vs-projecttemplate/index.html#outputs",
    "title": "Packages vs ProjectTemplate",
    "section": "Outputs",
    "text": "Outputs\n“But I don’t want to write my final report using Markdown or Latex” you cry! “How will I get my graphs or tables otherwise?” if using a package? I didn’t mention this in the previous posts, but it is possible to write the code in such a way to save graphics or write text files into a specific location (I would recommend /inst/extdata/output if it was me)."
  },
  {
    "objectID": "posts/2014-07-28-packages-vs-projecttemplate/index.html#final-thoughts",
    "href": "posts/2014-07-28-packages-vs-projecttemplate/index.html#final-thoughts",
    "title": "Packages vs ProjectTemplate",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nAs I said, I think I understand John’s thought process into designing ProjectTemplate, but given how easy it is to work with packages in the current R ecosystem, and that R packages are the built-in way to share things, I think it is more natural to use packages directly with prose of the analysis as a package vignette. But everyone works differently. If you are writing analyses in R, please use a sane directory setup (either packages, or ProjectTemplate or something else), and use version control (really, please learn how to use mercurial or git, your future self will thank you for it).\nRemember, when the reviewer (or your boss, or you) asks for something to be added, or to change something slightly in a figure, or add a new dataset, you want to be able to do it easily, and regenerate all of your results without manual intervention."
  },
  {
    "objectID": "posts/2013-07-11-r-interface-for-teaching/index.html",
    "href": "posts/2013-07-11-r-interface-for-teaching/index.html",
    "title": "R Interface for Teaching",
    "section": "",
    "text": "Kaitlin Thaney asked on Twitter last week about using Ramnath Vaidyanathan’s new interactive R notebook 1 2 for teaching.\nNow, to be clear up front, I am not trying to be mean to Ramnath, discredit his work, or the effort that went into that project. I think it is really cool, and has some rather interesting potential applications, but I don’t really think it is the right interface for teaching R. I would argue that the best interface for teaching R right now is RStudio. Keep reading to find out why."
  },
  {
    "objectID": "posts/2013-07-11-r-interface-for-teaching/index.html#ipython-notebook",
    "href": "posts/2013-07-11-r-interface-for-teaching/index.html#ipython-notebook",
    "title": "R Interface for Teaching",
    "section": "iPython Notebook",
    "text": "iPython Notebook\nFirst, I believe Ramnath when he says he was inspired by the iPython Notebook that makes it so very nice to do interactive, reproducible Python coding. Software Carpentry has been very successfully using them for helping to teach Python to scientists.\nHowever, the iPython Notebook is an interesting beast for this purpose. You are able to mix markdown blocks and code blocks. In addition, it is extremely simple to break up your calculations into units of related code, and re-run those units as needed. This is particularly useful when writing new functions, because you can write the function definition, and a test that displays output in one block, and then the actual computations in subsequent blocks. It makes it very easy to keep re-running the same block of code over and over until it is correct, which allows one to interactively explore changes to functions. This is awesome for learning Python and prototyping functions.\nIn addition to being able to repeatedly write -> run -> modify in a loop, you can also insert prose describing what is going on in the form of markdown. This is a nice lightweight syntax that generates html. So it becomes relatively easy to document the why of something."
  },
  {
    "objectID": "posts/2013-07-11-r-interface-for-teaching/index.html#r-notebook",
    "href": "posts/2013-07-11-r-interface-for-teaching/index.html#r-notebook",
    "title": "R Interface for Teaching",
    "section": "R Notebook",
    "text": "R Notebook\nUnfortunately, the R notebook that Ramnath has put up is not quite the same beast. It is an Ace editor window coupled to an R process that knits the markdown and displays the resultant html. This is really cool, and I think will be useful in many other applications, but not for teaching in an interactive environment."
  },
  {
    "objectID": "posts/2013-07-11-r-interface-for-teaching/index.html#rstudio-as-a-teaching-environment",
    "href": "posts/2013-07-11-r-interface-for-teaching/index.html#rstudio-as-a-teaching-environment",
    "title": "R Interface for Teaching",
    "section": "RStudio as a Teaching Environment",
    "text": "RStudio as a Teaching Environment\nLets think. We want something that lets us repeatedly write -> run -> modify on small code blocks in R, but would be great if it was some kind of document that could be shared, and re-run.\nI would argue that the editor environment in RStudio when writing R markdown (Rmd) files is the solution. R code blocks behave much the same as in iPython notebook, in that they are colored differently, set apart, have syntax highlighting, and can be easily repeatedly run using the code chunk menu. Outside of code blocks is assumed to be markdown, making it easy to insert documentation and explanation. The code from the code blocks is sent to an attached R session, where objects can be further investigated if required, and results are displayed.\nThis infrastructure supplies an interactive back and forth between editor and execution environment, with the ability to easily group together units of code.\nIn addition, RStudio has git integration baked in, so it becomes easy to get started with some basic version control.\nFinally, RStudio is cross-platform, has tab completion among other standard IDE goodies, and its free."
  },
  {
    "objectID": "posts/2013-07-11-r-interface-for-teaching/index.html#feedback",
    "href": "posts/2013-07-11-r-interface-for-teaching/index.html#feedback",
    "title": "R Interface for Teaching",
    "section": "Feedback",
    "text": "Feedback\nI’ve gotten some feedback on twitter about this, and I want to update this post to address it.\n\nHard to Install\nOne comment was that installing R, RStudio and necessary packages might be hard. True, it might be. However, I have done multiple installs of R, RStudio, Python, and iPython Notebook in both Linux and Windows, and I would argue that the level of difficulty is at least the same.\n\n\nMoving from Presentation to Coding\nI think this is always difficult, especially if you have a powerpoint, and your code is in another application. However, the latest dev version of RStudio (download) now includes the ability to view markdown based presentations in an attached window. This is probably one of the potentially nicest things for doing presentations that actually involve editing actual code.\nEdit: added download links for Rstudio preview"
  },
  {
    "objectID": "posts/2013-05-30-storing-package-data-in-custom-environments/index.html",
    "href": "posts/2013-05-30-storing-package-data-in-custom-environments/index.html",
    "title": "Storing Package Data in Custom Environments",
    "section": "",
    "text": "If you do R package development, sometimes you want to be able to store variables specific to your package, without cluttering up the users workspace. One way to do this is by modifying the global options. This is done by packages grDevices and parallel. Sometimes this doesn’t seem to work quite right (see this issue for example.\nAnother way to do this is to create an environment within your package, that only package functions will be able to see, and therefore read from and modify. You get a space to put package specific stuff, the user can’t see it or modify it directly, and you just need to write functions that do the appropriate things to that environment (adding variables, reading them, etc). This sounds great in practice, but I wasn’t clear on how to do this, even after reading the help page on environments, the R documentation, or even Hadley’s excellent writeup. From all these sources, I could glean that one can create environments, name them, modify them, etc, but wasn’t sure how to work with this within a package.\nI checked out the knitcitations package to see how it was done. When I looked, I realized that it was pretty obvious in retrospect. In zzz.R, initialize the environment, assigning it to a variable. When you need to work with the variables inside, this variable will be accessible to your package, and you simply use the get and assign functions like you would if you were doing anything on the command line.\nTo make sure I had it figured out, I created a very tiny package to create a custom environment and functions for modifying it. Please feel free to examine, download, install (using devtools]) and see for yourself.\nI have at least two projects where I know I will use this, and I’m sure others might find it useful as well.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2013,\n  author = {Robert M Flight},\n  title = {Storing {Package} {Data} in {Custom} {Environments}},\n  date = {2013-05-30},\n  url = {https://rmflight.github.io/posts/2013-05-30-storing-package-data-in-custom-environments},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2013. “Storing Package Data in Custom\nEnvironments.” May 30, 2013. https://rmflight.github.io/posts/2013-05-30-storing-package-data-in-custom-environments."
  },
  {
    "objectID": "posts/2014-08-08-my-career-goals/index.html",
    "href": "posts/2014-08-08-my-career-goals/index.html",
    "title": "My Career Goals",
    "section": "",
    "text": "I don’t want to be a PI because I enjoy spending time with my family, and don’t think I can handle the stress of juggling multiple grants, people, and deadlines. I want to be a staff member in a group that affords relative autonomy, while providing some security. If I’m lucky enough, my current position will enable that."
  },
  {
    "objectID": "posts/2014-08-08-my-career-goals/index.html#the-bad-news",
    "href": "posts/2014-08-08-my-career-goals/index.html#the-bad-news",
    "title": "My Career Goals",
    "section": "The Bad News",
    "text": "The Bad News\nIf you keep up with the news in academia, this is a horrible time to be a postdoc (post-doctoral) or PI (principal investigator). The amount of money available for grants is at an all time low, and the likelihood of getting a grant is dismal. If you want to be successful, then you will need to work incredibly hard, sacrifice everything, and it is likely that you still won’t get grants that allow you to have an independent research lab at a decent university.\nOf course, this is worst in the biomedical field, due to a glut of NIH research money and universities allowing researchers salary to be composed largely of grant money, with very little money from the university itself. In addition grant money has been allowed to be used to support trainees; undergraduate, graduate, and postdoctoral. The compensation for the trainees was relatively minimal, leading to a glut of highly trained, poorly paid people who hope to move up the academic ladder. The large numbers of which has likely contributed to the current funding situation."
  },
  {
    "objectID": "posts/2014-08-08-my-career-goals/index.html#the-good-news",
    "href": "posts/2014-08-08-my-career-goals/index.html#the-good-news",
    "title": "My Career Goals",
    "section": "The Good News",
    "text": "The Good News\nThe NIH and NSF are starting to encourage research labs to hire staff at reasonable pay, and support trainees from specific grants. Although I keep hearing about this in general, I don’t know of any specific changes in grant applications that are causing any real changes."
  },
  {
    "objectID": "posts/2014-08-08-my-career-goals/index.html#my-own-path",
    "href": "posts/2014-08-08-my-career-goals/index.html#my-own-path",
    "title": "My Career Goals",
    "section": "My Own Path",
    "text": "My Own Path\nDuring my senior undergraduate and masters work, I realized that I probably couldn’t be a student forever, but I had hopes. During my PhD, however, I saw many PI’s who sacrified their time with families for their teaching, research, and administrative duties. This was even more pronounced during my first PostDoc, and made me question my continued progress in academia. I actually had a job offer as a research programmer with a research hospital during my first PostDoc, which I turned down because I realized that it was likely in that position (at least as advertised) there would be limited opportunity to pursue my own research ideas. However, grant funding success rate was continuing to plummet (see above), and I was not able to push out a first author paper before finishing my first PostDoc, so being successful in a faculty search was looking grim.\nWhen I started my second PostDoc position (transitioning from transcriptomics to metabolomics), I was up front with my supervisor that I did not feel that I was the type of person who could manage their own lab with independent funding, but that I wanted to find a place where I could stay on as staff while contributing research. My PI was sympathetic, and we worked well together, so he was immediately amenable to that, as long as the funding situation would provide.\nFortunately, within a year of joining the lab, we and our collaborators landed a large center grant that provides stable funding for 5 years, and everyone involved (my PI, the other PI’s, myself) would be happy if I stayed on as staff after my time as a PostDoc. Already, as a PostDoc in this group I have a lot of latitude and freedom to pursue avenues of research, as long as they are somehow aligned with the overall goals of the lab. I am also finding myself involved in supervising various undergraduate and graduate projects, as well as proposing new avenues for research.\nI know the opportunity to stay on as staff in a research group is relatively rare in the current funding climate, and my job will be largely to develop novel methods and software to advance our understanding of metabolism in various biological states, as well as provide means to secure additional funding for the lab."
  },
  {
    "objectID": "posts/2021-12-21-reusing-ggplot2-colors/index.html",
    "href": "posts/2021-12-21-reusing-ggplot2-colors/index.html",
    "title": "Reusing ggplot2 Colors",
    "section": "",
    "text": "If you are using {ggplot2} a lot in your code, and don’t want to bother with a custom color scheme (I mean, there are lots of good options), but you are also using non-ggplot2 type plots, and want to reuse the colors.\nI frequently encounter this when I’m doing principal component analysis (PCA) combined with heatmaps plotted by {ComplexHeatmap}. I’ve colored sample groups in the PCA using {ggplot2} defaults, and now I want those same colors in the {ComplexHeatmap} row or column annotations.\nThe trick to this is extracting the palette from the color definition you want. For example, very commonly (in biology at least) we might have 2 classes of samples, cases and controls. So we need 2 colors.\nWhen we generated the {ggplot2} plot, we would do something like this:\n\nggplot(data_frame, aes(x = var1, y = var2, color = groups))\n\nTo generate the matching colors for something else, we can do:\n\ng_colors = scale_color_discrete()$palette(2)\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2021,\n  author = {Robert M Flight},\n  title = {Reusing Ggplot2 {Colors}},\n  date = {2021-12-21},\n  url = {https://rmflight.github.io/posts/2021-12-21-reusing-ggplot2-colors},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2021. “Reusing Ggplot2 Colors.” December\n21, 2021. https://rmflight.github.io/posts/2021-12-21-reusing-ggplot2-colors."
  },
  {
    "objectID": "posts/2020-02-25-using-group-by-instead-of-splitting/index.html",
    "href": "posts/2020-02-25-using-group-by-instead-of-splitting/index.html",
    "title": "Using group_by Instead of Splits",
    "section": "",
    "text": "It is relatively easy to use dplyr::group_by and summarise to find items that you might want to keep or remove based on a part_of the item or group in question. I used to use split and iterate, but group_by is much easier."
  },
  {
    "objectID": "posts/2020-02-25-using-group-by-instead-of-splitting/index.html#motivation",
    "href": "posts/2020-02-25-using-group-by-instead-of-splitting/index.html#motivation",
    "title": "Using group_by Instead of Splits",
    "section": "Motivation",
    "text": "Motivation\nI have some relatively large sets of data that fall naturally into groups of items. Often, I find that I want to remove a group that contains either any of or all of particular items. Let’s create some data as an example.\n\nlibrary(dplyr)\nset.seed(1234)\ngroups = as.character(seq(1, 1000))\ngrouped_data = data.frame(items = sample(letters, 10000, replace = TRUE),\n                          groups = sample(groups, 10000, replace = TRUE),\n                          stringsAsFactors = FALSE)\n\nknitr::kable(head(grouped_data))\n\n\n\n\nitems\ngroups\n\n\n\n\np\n891\n\n\nz\n646\n\n\nv\n795\n\n\ne\n49\n\n\nl\n19\n\n\no\n796\n\n\n\n\n\nIn this example, we have the 26 lowercase letters, that are part of one of groups 1-1000. Now, we might want to keep any groups that contain at least one “a”, for example.\nI would have previously used a split on the groups, and then purrr::map_lgl returning TRUE or FALSE to check if what we wanted to filter on was present, and then filter out the split groups, and finally put back together the full thing."
  },
  {
    "objectID": "posts/2020-02-25-using-group-by-instead-of-splitting/index.html#group-by",
    "href": "posts/2020-02-25-using-group-by-instead-of-splitting/index.html#group-by",
    "title": "Using group_by Instead of Splits",
    "section": "Group By",
    "text": "Group By\nWhat I’ve found instead is that I can use a combination of group_by, summarise and then filter to same effect, without splitting and iterating (yes, I know dplyr is doing it under the hood for me).\n\n# use group_by and summarize to find things we want\ngroups_to_keep = grouped_data %>% \n  group_by(groups) %>%\n  summarise(has_a = sum(items %in% \"a\") > 0) %>%\n  filter(has_a)\n\n# filter on original based on above\ngrouped_data2 = grouped_data %>%\n  filter(groups %in% groups_to_keep$groups)\n\nThis was a game changer for me in my thinking. As I’ve used group_by combined with summarise more and more, I’ve become amazed at what can be done without having to fully split the data apart to operate on it.\nThis combined with the use of dplyr::join_ in place of splits (see this other post for an example) is making my code faster, and often easier to reason over. I hope it helps you too!"
  },
  {
    "objectID": "posts/2022-01-06-dplyr-in-packages-and-global-variables/index.html",
    "href": "posts/2022-01-06-dplyr-in-packages-and-global-variables/index.html",
    "title": "Dplyr in Packages and Global Variables",
    "section": "",
    "text": "I was recently cleaning up a development package so it would actually pass checks so it could be hosted on our labs new r-universe (“The ’Moseleybioinformaticslab’ Universe,” n.d.), and was getting warnings about global variables due to using {dplyr} operations, without {enquoting} variables.\nThere are a few approaches to handling this.\n\nUsing {utils::globalvariables} in an R file somewhere.\nUsing {rlang::.data} and importing it into the package.\n\nI went with option 2, see (“How to Solve ’No Visible Binding for Global Variables’,” n.d.).\nIn the original version of {dplyr} and other packages like {ggplot2}, there was an option to use “string” arguments, normally by calling the {*_str} or {*_} version of a function name.\nThat has gone out of fashion, and the way to do it now is to use the .data pronoun (“Programming with Dplyr,” n.d.).\nSo now the code looks like this:\n# an example filtering operation\ndplyr::filter(.data$varname == \"value\")\nThe easiest way to include this correctly in your package, is by importing the {rlang::.data}.\n#' @importFrom rlang .data\nAnd all the warnings should go away during package checking.\n\n\n\n\nReferences\n\n“How to Solve ’No Visible Binding for Global Variables’.” n.d. https://community.rstudio.com/t/how-to-solve-no-visible-binding-for-global-variable-note/28887/3.\n\n\n“Programming with Dplyr.” n.d. https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html.\n\n\n“The ’Moseleybioinformaticslab’ Universe.” n.d. https://moseleybioinformaticslab.r-universe.dev/ui#builds.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2022,\n  author = {Robert M Flight},\n  title = {Dplyr in {Packages} and {Global} {Variables}},\n  date = {2022-01-06},\n  url = {https://rmflight.github.io/posts/2022-01-06-dplyr-in-packages-and-global-variables},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2022. “Dplyr in Packages and Global\nVariables.” January 6, 2022. https://rmflight.github.io/posts/2022-01-06-dplyr-in-packages-and-global-variables."
  },
  {
    "objectID": "posts/2017-12-27-linking-to-manually-inserted-images-in-hugo/index.html",
    "href": "posts/2017-12-27-linking-to-manually-inserted-images-in-hugo/index.html",
    "title": "Linking to Manually Inserted Images in Blogdown / Hugo",
    "section": "",
    "text": "Using blogdown for generating websites and blog-posts from Rmarkdown files with lots of inserted code and figures seems pretty awesome, but sometimes you want to include a figure manually, either because you want to generate something manually and convert it (say for going from SVG of lots of points to hi-res PNG), or because it is a figure from something else (like this figure from wikipedia)."
  },
  {
    "objectID": "posts/2017-12-27-linking-to-manually-inserted-images-in-hugo/index.html#where-to",
    "href": "posts/2017-12-27-linking-to-manually-inserted-images-in-hugo/index.html#where-to",
    "title": "Linking to Manually Inserted Images in Blogdown / Hugo",
    "section": "Where to??",
    "text": "Where to??\nTo do this, you want the text of your <img> tag to your image to be:\n<img src = \"/img/image_file.png\"></img>\nAnd then put the image itself in the directory /static/img/image_file.png\n\nBy M. W. Toews, CC BY 2.5, via Wikimedia Commons, source\nThis information is also mentioned in section 2.7 of the Blogdown book. Obviously I need to do more reading."
  },
  {
    "objectID": "posts/2019-10-16-comments-enabled-via-utterances/index.html",
    "href": "posts/2019-10-16-comments-enabled-via-utterances/index.html",
    "title": "Comments enabled via utterances",
    "section": "",
    "text": "Utterances is a lightweight commenting platform built on GitHub issues. So you have to have a GitHub account, but I expect most people who comment on this blog already have one."
  },
  {
    "objectID": "posts/2019-10-16-comments-enabled-via-utterances/index.html#why-utterances",
    "href": "posts/2019-10-16-comments-enabled-via-utterances/index.html#why-utterances",
    "title": "Comments enabled via utterances",
    "section": "Why Utterances",
    "text": "Why Utterances\nWhen I switched to blogdown, I lost my disqus comments. I had considered migrating them over, but never got around to it. I also thought that there had to be a way to link GitHub issues to blog posts, but didn’t investigate it much.\nThen, I came across Maëlle’s blog post about switching to utterances and I was sold. I had some free time last night, and dived into how to add it to my site that uses the hugo academic theme.\nI’m not expecting a lot of heavy commenting, but at least it’s now available!"
  },
  {
    "objectID": "posts/2019-10-16-comments-enabled-via-utterances/index.html#how",
    "href": "posts/2019-10-16-comments-enabled-via-utterances/index.html#how",
    "title": "Comments enabled via utterances",
    "section": "How",
    "text": "How\nFor blogdown, just replace everything in layouts/partials/comments.html with the code snippet from utterances."
  },
  {
    "objectID": "posts/2022-12-08-customizing-the-displayed-date-in-quarto-pubs/index.html",
    "href": "posts/2022-12-08-customizing-the-displayed-date-in-quarto-pubs/index.html",
    "title": "Customizing the Displayed Date in Quarto Pubs",
    "section": "",
    "text": "If you want to change how the date is displayed in your {quarto} documents, this is handy.\nOne, decide what date you want included:\n\nnow\ntoday\nlast-modified (this is my favorite, because I can tell if I’ve got the most recent version)\n\nThen, you can change how the date is displayed using the date-format option.\nMy personal preference is for something like 2022-12-08 13:42, to avoid any ambiguity when I’m generating reports for others.\n---\ndate: last-modified\ndate-format: YYYY-MM-DD HH:mm\n---\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2022,\n  author = {Robert M Flight},\n  title = {Customizing the {Displayed} {Date} in {Quarto} {Pubs}},\n  date = {2022-12-08},\n  url = {https://rmflight.github.io/posts/2022-12-08-customizing-the-displayed-date-in-quarto-pubs},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2022. “Customizing the Displayed Date in Quarto\nPubs.” December 8, 2022. https://rmflight.github.io/posts/2022-12-08-customizing-the-displayed-date-in-quarto-pubs."
  },
  {
    "objectID": "posts/2021-08-03-coloring-dendrogram-edges-with-ggraph/index.html",
    "href": "posts/2021-08-03-coloring-dendrogram-edges-with-ggraph/index.html",
    "title": "Coloring Dendrogram Edges with ggraph",
    "section": "",
    "text": "I wanted to color the dendrogram edges according to their class in ggraph, and I was getting stuck because of something that isn’t explicitly mentioned in the documentation, but is implied. You must use “node.” to access the data from the Node Data in the call to aes(...).\nLets set it up. We will borrow from the “Edges” vignette in the ggraph package (Pederson 2021).\n\nlibrary(ggraph)\n\nLoading required package: ggplot2\n\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(purrr)\nlibrary(rlang)\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\nset_graph_style(plot_margin = margin(1,1,1,1))\nhierarchy <- as_tbl_graph(hclust(dist(iris[, 1:4]))) %>% \n  mutate(Class = map_bfs_back_chr(node_is_root(), .f = function(node, path, ...) {\n    if (leaf[node]) {\n      as.character(iris$Species[as.integer(label[node])])\n    } else {\n      species <- unique(unlist(path$result))\n      if (length(species) == 1) {\n        species\n      } else {\n        NA_character_\n      }\n    }\n  }))\n\nhierarchy\n\n# A tbl_graph: 299 nodes and 298 edges\n#\n# A rooted tree\n#\n# Node Data: 299 × 5 (active)\n  height leaf  label members Class    \n   <dbl> <lgl> <chr>   <int> <chr>    \n1  0     TRUE  \"108\"       1 virginica\n2  0     TRUE  \"131\"       1 virginica\n3  0.265 FALSE \"\"          2 virginica\n4  0     TRUE  \"103\"       1 virginica\n5  0     TRUE  \"126\"       1 virginica\n6  0     TRUE  \"130\"       1 virginica\n# … with 293 more rows\n#\n# Edge Data: 298 × 2\n   from    to\n  <int> <int>\n1     3     1\n2     3     2\n3     7     5\n# … with 295 more rows\n\n\nAnd with this, we can create a dendrogram.\n\nggraph(hierarchy, \"dendrogram\", height = height) +\n  geom_edge_elbow()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nNice! But what if we want the leaves colored by which “Class” they belong to?\n\nggraph(hierarchy, \"dendrogram\", height = height) +\n  geom_edge_elbow2(aes(color = node.Class))\n\n\n\n\nNote the differences in this function call compared to the previous:\n\nUsing geom_edge_elbow2 instead of geom_edge_elbow\nUsing node.Class, not just Class.\n\nThe second point is really important! When you look at the hierarchy object printed above, the Class bit is part of the Node Data, which gets identified by ggraph by the prefix “node.”.\nIf we don’t use node.Class, here is the error:\n\nggraph(hierarchy, \"dendrogram\", height = height) +\n  geom_edge_elbow2(aes(color = Class))\n\nError in `geom_edge_elbow2()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `FUN()`:\n! object 'Class' not found\n\n\n\n\n\n\nReferences\n\nPederson, Thomas Lin. 2021. “Edges.” https://cran.r-project.org/web/packages/ggraph/vignettes/Edges.html.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2021,\n  author = {Robert M Flight},\n  title = {Coloring {Dendrogram} {Edges} with Ggraph},\n  date = {2021-08-03},\n  url = {https://rmflight.github.io/posts/2021-08-03-coloring-dendrogram-edges-with-ggraph},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2021. “Coloring Dendrogram Edges with\nGgraph.” August 3, 2021. https://rmflight.github.io/posts/2021-08-03-coloring-dendrogram-edges-with-ggraph."
  },
  {
    "objectID": "posts/2017-12-27-differences-in-posted-date-vs-sessioninfo/index.html",
    "href": "posts/2017-12-27-differences-in-posted-date-vs-sessioninfo/index.html",
    "title": "Differences in Posted Date vs sessionInfo()",
    "section": "",
    "text": "If you are a newcomer to my weblog, you may notice that some posts that are R tutorials generally include the output of Sys.time() at the end. If you look closeley at that time and the Posted on date, you may notice that some posts show disagreement between them. This is because I decided to move all of my old blog posts from blogspot to here, and keep the original posted dates.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2017,\n  author = {Robert M Flight},\n  title = {Differences in {Posted} {Date} Vs {sessionInfo()}},\n  date = {2017-12-27},\n  url = {https://rmflight.github.io/posts/2017-12-27-differences-in-posted-date-vs-sessioninfo},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2017. “Differences in Posted Date Vs\nsessionInfo().” December 27, 2017. https://rmflight.github.io/posts/2017-12-27-differences-in-posted-date-vs-sessioninfo."
  },
  {
    "objectID": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html",
    "href": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html",
    "title": "Creating an Analysis as a Package and Vignette",
    "section": "",
    "text": "Edit 2022-12-02: I don’t recommend this approach anymore.\nFollowing from my last post, I am going to go step by step through the process I use to generate an analysis as a package vignette. This will be an analysis of the tweets from the 2012 and 2014 ISMB conference (thanks to Neil and Stephen for compiling the data).\nI will link to individual commits so that you can see how things change as we go along."
  },
  {
    "objectID": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#setup",
    "href": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#setup",
    "title": "Creating an Analysis as a Package and Vignette",
    "section": "Setup",
    "text": "Setup\n\nInitialization\nTo start, we will initialize the package. devtools or rstudio make this rather easy:\n\nlibrary(devtools)\ncreate(\"~/Documents/projects/personal/ismbTweetAnalysis\")\n\nCreating package ismbTweetAnalysis in ~/Documents/projects/personal\nNo DESCRIPTION found. Creating with values:\n\nPackage: ismbTweetAnalysis\nTitle: What the package does (short line)\nVersion: 0.1\nAuthors@R: \"First Last <first.last@example.com> [aut, cre]\"\nDescription: What the package does (paragraph)\nDepends: R (>= 3.0.3)\nLicense: What license is it under?\nLazyData: true\nAdding Rstudio project file to ismbTweetAnalysis\nAlternatively, you can use File > New Project > New Directory > R Package in rstudio. Don’t forget to Create a git repository (or git init in the directory). Note that the devtools created package will pass CRAN tests, whereas the rstudio will not.\nOpen the DESCRIPTION file, and you will need to change the Title, Authors or Authors@R, Description, License, and add VignetteBuilder: knitr at the end. Here is what my initial setup looks like.\n\n\nRStudio Project Options\nIn addition, to make our life easier, we will change some options in the rstudio project.\nTools > Project Options > Build Tools, check Generate documentation with Roxygen, and select turn on all the options. We want to roxygenize when we Build & Reload especially, and have roxygen control the NAMESPACE file so we don’t worry about it.\nAlternatively, you can use document with reload=TRUE in devtools to update documentation and reload the functions.\nHaving this particular option of documenting and reloading the package every time I write a new function is what makes this easy. I write the new function, document/reload, and I can keep chugging along with my analysis document. And if I have to restart, I just run all chunks to get back to where I need to be."
  },
  {
    "objectID": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#data",
    "href": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#data",
    "title": "Creating an Analysis as a Package and Vignette",
    "section": "Data",
    "text": "Data\nNow we need some data. Neil’s data from 2012 uses a CSV format, however the tweets themselves have commas, so we will download the rdata file and use that, and also Stephen’s data from 2014. However, there are three separate files for 2014, so we will download all three files and combine them. Both initial data sets will go in the /inst/extdata folder, and we will clean them up.\nHere we have added our 4 data files.\n\nVignette\nWe are going to write this analysis as the vignette of the package, using R markdown as the language. To do that we need to create the file and add some boilerplate at the top so that the vignette gets generated properly. Here is the initial vignette, it is nothing but the engine and index definition, which are important."
  },
  {
    "objectID": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#preview-report",
    "href": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#preview-report",
    "title": "Creating an Analysis as a Package and Vignette",
    "section": "Preview Report",
    "text": "Preview Report\nTo preview the report, you can use the Knit HTML button in rstudio, or also use knitr directly. This will give you an html preview of the final report."
  },
  {
    "objectID": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#generate-vignette",
    "href": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#generate-vignette",
    "title": "Creating an Analysis as a Package and Vignette",
    "section": "Generate Vignette",
    "text": "Generate Vignette\nOnce happy with the report, you can use devtools::build_vignettes() to generate the vignette files that will be copied to the relevant locations."
  },
  {
    "objectID": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#commit-and-push-it-all",
    "href": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#commit-and-push-it-all",
    "title": "Creating an Analysis as a Package and Vignette",
    "section": "Commit and Push it ALL!",
    "text": "Commit and Push it ALL!\nAt this point, if you are happy with the package and analysis as a whole, you should commit all the package files to version control and make it available. In this case this means:\n\ninst/doc: the output vignette\nman: the function documentation\nDESCRIPTION: our description file\nNAMESPACE: the file documenting our namespace\n\nYou can see this commit here.\nNow your package can be installed by others using devtools::install_github(). You could also submit your package to CRAN or Bioconductor if so desired."
  },
  {
    "objectID": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#not-covered",
    "href": "posts/2014-07-28-creating-an-analysis-as-a-package-and-vignette/index.html#not-covered",
    "title": "Creating an Analysis as a Package and Vignette",
    "section": "Not Covered",
    "text": "Not Covered\nNow this was a simple example. Ideally I should have included tests for my functions, you can read up 1 2 on how to do that. In addition, none of my functions use methods (see why they are useful).\nI hope that you find this example useful, and will consider using packages more often even for simple analyses.\n\nReproducibility: One issue that may come up is how to make sure that you or someone else can directly reproduce the work in your package. Again, Hadley Wickham and the rstudio team have been thinking about this, and there is now the packrat package to make a project completely self-contained with all of it’s dependencies.\n\nEdit 2014-07-28 - added note on reproducibility at the end."
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "",
    "text": "I’ve been working on a manuscript for our newish correlation method, information-content-informed Kendall-tau (ICI-Kendalltau, package currently at (Flight and Moseley 2021)). As part of that manuscript, we wanted to highlight how well the correlation measure detects outlier samples. Two datasets we are using for that aspect are public, one from The Cancer Genome Atlas and the other from the Barton group. The Barton group and collaborators produced a highly replicated RNAseq yeast dataset, 48 replicates in two conditions, and have used it in various analyses (Gierliński et al. 2015; Schurch et al. 2016)."
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#so-what",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#so-what",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "So What",
    "text": "So What\nFor my manuscript, ideally I want to be able to comment on the outliers found in one of the Barton group manuscripts (Gierliński et al. 2015). To do that, I need access to one of:\n\nthe sample-sample correlations themselves,\nthe counts from each sample\nor be able to recreate counts from each sample.\n\nSeveral years ago I was asking about this dataset on twitter when I was using it for another project, because the project ID in the SRA didn’t have a mapping to condition. Dr. Geoff Barton pointed me to a metadata file available on figshare (Barton, Blaxter, Cole, Gharbi, Gierliński, Owen-Hughes, et al. 2015). This allowed me to generate read counts across biological replicates (which is what I was interested in at the time). However, if I had poked around the figshare project a bit more, I would have likely seen both of the files with read counts for each replicate already available (Barton, Blaxter, Cole, Gharbi, Gierliński, Schofield, et al. 2015a, 2015b). It really sucks that I hadn’t noticed these other files years ago, it would have saved me the effort in remapping and generating gene counts myself, as well as getting really weird correlation values compared to (Gierliński et al. 2015)."
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#my-correlations",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#my-correlations",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "My Correlations",
    "text": "My Correlations\nFrom the demultiplexed read data and the metadata file I found several years ago, I had run RNA-seq mapping software to generate read counts for each sample in each lane, and summed them across lanes. I know how to do these kinds of things, even though I will be the first to admit it is not my personal bread and butter analysis (I normally get involved after count generation). With the (few) hints from the manuscript about how (Gierliński et al. 2015) did the correlation calculation (see more below), I don’t get anything close to the range of median correlation values within each class of samples. Which I thought was very, very weird. I implemented a variety of transformation methods and inclusion of missing values for my correlation calculations:\n\nlogged values (log and log1p)\nraw values\nremoval and inclusion of missing (0) values\n\nEven with all of these variations, I could not come close to the same values I found presented in their manuscript. Here, in panel (a) of Figure 2 from (Gierliński et al. 2015), the median correlations range from 0.7 to 1.\n\n\n\n\n\nFigure 2(a) from (Gierliński et al. 2015). Identifying bad RNA-seq replicates in the WT (left) and Δsnf2 (right) data. The top three panels (a–c) show individual criteria for identifying ‘bad’ replicates which are combined into a quality score (d) in order to identify ‘bad’ replicates in each condition. The identified ‘bad’ replicates are shown as numbered points in each panel. The individual criteria are (a) median correlation coefficient, ri∼⁠, for each replicate i against all other replicates, (b) outlier fraction, fi⁠, calculated as a fraction of genes where the given replicate is more than five-trimmed standard deviations from the trimmed mean and (c) median reduced χ2 of pileup depth, χ∼2i⁠, as a measure of the non-uniformity of read distribution within genes (see also Fig. 3)\n\n\n\n\nMy ranges, however, were much different.\n\nlibrary(dplyr)\nout_ranges = readRDS(here::here(\"data_files\", \"ranges.rds\"))\nknitr::kable(out_ranges$rmf_ranges, digits = 2, caption = 'RMF median correlation ranges. Which is \"which\" method was used to calculate the correlations.')\n\n\nRMF median correlation ranges. Which is “which” method was used to calculate the correlations.\n\n\nwhich\nsample_class\nhigh\nlow\n\n\n\n\nlog\nSNF2\n0.99\n0.96\n\n\nlog\nWT\n0.97\n0.89\n\n\nlog_no0\nSNF2\n0.99\n0.95\n\n\nlog_no0\nWT\n0.96\n0.87\n\n\nraw\nSNF2\n0.98\n0.96\n\n\nraw\nWT\n0.97\n0.67\n\n\nraw_no0\nSNF2\n0.98\n0.96\n\n\nraw_no0\nWT\n0.97\n0.67\n\n\n\n\n\nWe can see in this table, that my median correlation ranges are not even close to what we can see in the figure.\nAnd the lowest values and samples didn’t seem to be right either.\n\nout_ranges$rmf_medians %>%\n  dplyr::filter(which %in% \"raw_no0\") %>%\n  dplyr::group_by(sample_class) %>%\n  dplyr::arrange(med_cor) %>%\n  dplyr::slice_head(n = 6) %>%\n  knitr::kable(., digits = 2, caption = \"Lowest median correlation values using my own counts.\")\n\n\nLowest median correlation values using my own counts.\n\n\nsample_id\nmed_cor\nsample_class\nwhich\n\n\n\n\nSNF2.7\n0.96\nSNF2\nraw_no0\n\n\nSNF2.33\n0.96\nSNF2\nraw_no0\n\n\nSNF2.27\n0.96\nSNF2\nraw_no0\n\n\nSNF2.8\n0.96\nSNF2\nraw_no0\n\n\nSNF2.48\n0.96\nSNF2\nraw_no0\n\n\nSNF2.23\n0.97\nSNF2\nraw_no0\n\n\nWT.32\n0.67\nWT\nraw_no0\n\n\nWT.14\n0.75\nWT\nraw_no0\n\n\nWT.11\n0.80\nWT\nraw_no0\n\n\nWT.13\n0.83\nWT\nraw_no0\n\n\nWT.16\n0.87\nWT\nraw_no0\n\n\nWT.12\n0.91\nWT\nraw_no0"
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#finding-the-data",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#finding-the-data",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "Finding the Data",
    "text": "Finding the Data\nFinally, during the week of 2021-11-25, I happened across another manuscript on this dataset from 2016 (Schurch et al. 2016), that mentions a GitHub repo that lo and behold had copies of the preprocessed data to the level of gene counts per biological replicate (Cole et al. 2015). Awesome!"
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#new-correlation",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#new-correlation",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "New Correlation",
    "text": "New Correlation\nUsing that preprocessed data, I was finally able to get what amounted to identical values of correlation, based on comparing the lowest correlation values with the figure. Great.\n\nknitr::kable(out_ranges$barton, digits = 2, caption = 'Barton median correlation ranges. Which is \"which\" method was used to calculate the correlations.')\n\n\nBarton median correlation ranges. Which is “which” method was used to calculate the correlations.\n\n\nwhich\nsample_class\nhigh\nlow\n\n\n\n\nlog\nSnf2\n0.99\n0.95\n\n\nlog\nWT\n0.99\n0.97\n\n\nlog_no0\nSnf2\n0.99\n0.91\n\n\nlog_no0\nWT\n0.99\n0.94\n\n\nraw\nSnf2\n1.00\n0.71\n\n\nraw\nWT\n0.99\n0.77\n\n\nraw_no0\nSnf2\n1.00\n0.71\n\n\nraw_no0\nWT\n0.99\n0.77\n\n\n\n\n\nIn this table, for the raw correlations, we finally see median correlation ranges that match what is observed in the figure, especially at the low end. Importantly, the Snf2 lowest value is lower than the WT lowest value. So I’m pretty sure I’m getting the correct sample-sample correlations now.\nAs well, if I look at the lowest sample - sample correlations in each class, the sample IDs match what is in the figure as well, and so do the median sample-sample correlations for those samples!\n\nout_ranges$barton_medians %>%\n  dplyr::filter(which %in% \"raw_no0\") %>%\n  dplyr::group_by(sample_class) %>%\n  dplyr::arrange(med_cor) %>%\n  dplyr::slice_head(n = 6) %>%\n  knitr::kable(., digits = 2, caption = \"Lowest median correlation values using Barton counts.\")\n\n\nLowest median correlation values using Barton counts.\n\n\nsample_id\nmed_cor\nsample_class\nwhich\n\n\n\n\nSnf2.06\n0.71\nSnf2\nraw_no0\n\n\nSnf2.13\n0.83\nSnf2\nraw_no0\n\n\nSnf2.35\n0.96\nSnf2\nraw_no0\n\n\nSnf2.10\n0.99\nSnf2\nraw_no0\n\n\nSnf2.25\n0.99\nSnf2\nraw_no0\n\n\nSnf2.24\n0.99\nSnf2\nraw_no0\n\n\nWT.21\n0.77\nWT\nraw_no0\n\n\nWT.25\n0.86\nWT\nraw_no0\n\n\nWT.28\n0.89\nWT\nraw_no0\n\n\nWT.22\n0.94\nWT\nraw_no0\n\n\nWT.17\n0.98\nWT\nraw_no0\n\n\nWT.36\n0.98\nWT\nraw_no0"
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#why-no-log-transformation",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#why-no-log-transformation",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "Why No Log-Transformation?",
    "text": "Why No Log-Transformation?\nOne interesting thing about having the correct correlations is discovering that Gierlinski et al didn’t use log-transformed data in their Pearson correlation calculations. This seems unusual to me. All my career in -omics, the one thing I’ve had drilled into me is that doing a linear correlation on data that has proportional error component or variance is a very bad idea. Proportional error or variance means the variance increases with increasing mean values, which is definitely true of these count data.\nIf I had to guess why log-transformed values weren’t used, I think it is because of the analysis in the replicate paper about how well the un-transformed values fit a normal distribution vs a log-normal distribution. That figure and caption are provided here for reference.\n\n\n\n\n\nFigure 5 from (Gierliński et al. 2015). Goodness-of-fit test results for normal (top panels), log-normal (middle panels) and negative binomial (bottom panels) distributions. Each panel shows the test P-value versus the mean count across replicates. Each dot represents equal-count normalized data from one gene. Panels on the left (a, b, e, f, i, j) show clean data with bad replicates rejected (42 and 44 replicates remaining in WT and Δsnf2, respectively). Panels on the right (c, d, g, h, k, l) show all available data (48 replicates in each condition). Due to the number of bootstraps performed, P-values for the negative-binomial test are limited to ∼10−2. Due to numerical precision of the software library used, P-values from the normal and log-normal tests are limited to ∼10−16. Below these limits data points are marked in orange (light gray in black and white) at the bottom of each panel. Horizontal lines show the Benjamini–Hochberg limit corresponding to the significance of 0.05 for the given dataset. The numbers in the right bottom corner of each panel indicate the number of genes with P-values below the significance limit and the total number of genes\n\n\n\n\nJust so you can see the difference using raw and log-space makes (besides the values in the above table), here are two of the replicate samples plotted against each other in raw and log-transformed values.\n\nlibrary(patchwork)\nlibrary(ggplot2)\ntheme_set(cowplot::theme_cowplot())\n\nraw_plot = readRDS(here::here(\"data_files\", \"raw_plot.rds\"))\nraw_p2 = raw_plot + \n  coord_equal() +\n  geom_abline(slope = 1, color = \"red\")\nlog_plot = readRDS(here::here(\"data_files\", \"log_plot.rds\"))\nlog_p2 = log_plot +\n  coord_equal() +\n  geom_abline(slope = 1, color = \"red\")\nraw_p2 + log_p2\n\n\n\n\nWe can summarize this behavior across all the replicate samples, looking at standard deviation (SD) and relative standard deviation (RSD, SD / mean). I took the counts from Barton group after removing outliers based on median correlations, and then calculate the mean, SD, and RSD across all replicates in each of Snf2 and WT.\n\ndata_summary = readRDS(here::here(\"data_files\", \"summary.rds\"))\n\ndata_summary %>%\n  dplyr::filter(!(type %in% \"diff\")) %>%\n  ggplot(aes(x = mean, y = var)) + \n  geom_point() +\n  facet_wrap(~ type, ncol = 1, scales = \"free_y\")\n\nWarning: Removed 494 rows containing missing values (`geom_point()`).\n\n\n\n\n\nAs we can see in the above figure, SD increases with increasing mean. Which is basically what we observe in the pairwise plot above. I’m pretty sure I’ve been taught not to do Pearson correlation on data with this structure. Interestingly, the RSD does become rather constant after a certain value in mean expression."
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#how-did-i-miss-the-original-data",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#how-did-i-miss-the-original-data",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "How Did I Miss the Original Data?",
    "text": "How Did I Miss the Original Data?\nBack to the question of finding the original data, it turns out I just didn’t look hard enough at the first place that Geoff Barton sent me to. When I started poking around the figshare repo’s from Geoff and others in the group and following links, it was easy to find a copy of the preprocessed data from each condition."
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#why-was-it-so-hard-to-reproduce-values",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#why-was-it-so-hard-to-reproduce-values",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "Why Was it So Hard to Reproduce Values?",
    "text": "Why Was it So Hard to Reproduce Values?\nAnother interesting thing about this endeavor was just how hard it was to reproduce the correlations. (Gierliński et al. 2015) was actually light on details of how the data was processed, and how the correlation was done. For example, the manuscript just says “Pearson correlation”, with basically no other details. Were raw counts used or log-transformed? Were missing (count of 0) values in either sample removed prior to correlation? It was only when I finally happened across the GitHub repo that I finally got the answers I really needed. And for some reason, that manuscript doesn’t mention the GitHub repo, or the data available on figshare. This points to the importance of citing and linking all the resources for a manuscript (or even a blog post!). And I’m not trying to knock on Marek, or Geoff, or Nick on this. In my experience, they post open things, and provide lots of data. But this highlights just how hard it becomes to recreate something if even a little piece of the data is missing."
  },
  {
    "objectID": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#data-files-and-scripts",
    "href": "posts/2022-01-15-recreating-correlation-values-from-another-manuscript/index.html#data-files-and-scripts",
    "title": "Recreating Correlation Values from Another Manuscript",
    "section": "Data Files and Scripts",
    "text": "Data Files and Scripts\nThe rds files and a processing script to calculate the correlations and generate the plots are all available on the blog directory on GitHub (Flight 2022)."
  },
  {
    "objectID": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html",
    "href": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html",
    "title": "Packages Don’t Work Well for Analyses in Practice",
    "section": "",
    "text": "Edit 2022-12-03: Don’t use {drake}, use {targets} now. Everything else still applies."
  },
  {
    "objectID": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#tldr",
    "href": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#tldr",
    "title": "Packages Don’t Work Well for Analyses in Practice",
    "section": "TL;DR",
    "text": "TL;DR\nI was wrong about using an R package to bundle an analysis. That happens a lot I suppose. But I needed to own up here."
  },
  {
    "objectID": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#using-r-packages-for-analysis",
    "href": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#using-r-packages-for-analysis",
    "title": "Packages Don’t Work Well for Analyses in Practice",
    "section": "Using R Packages for Analysis",
    "text": "Using R Packages for Analysis\nA couple of years ago, I wrote at least three different blog posts describing how using R packages to bundle up an analysis was the best way to do things. I went into the benefits of this approach in general (Flight 2014a), how to do actually go through one in practice (Flight 2014b), and bashed on an alternative, ProjectTemplate (Flight 2014c). I was wrong. So, so very wrong."
  },
  {
    "objectID": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#what-happened-in-practice",
    "href": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#what-happened-in-practice",
    "title": "Packages Don’t Work Well for Analyses in Practice",
    "section": "What Happened in Practice?",
    "text": "What Happened in Practice?\nAlthough I was espousing this philosophy for doing the analysis as part of an R package, in practice, it was a pain in the butt to actually carry out. Write functions, update installed package, update vignette, hope it all runs. I honestly would end up with a ton of functions written in the report vignette, and hope that {knitr} caching would save me (or have to delete the cache because something went wrong).\nEventually, I started having project directories with a report, and throwing all my functions into the top of the rmarkdown, and if I was lucky I might write objects (tables, figures, etc) out into directories that could then be sent to collaborators."
  },
  {
    "objectID": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#a-dim-bulb",
    "href": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#a-dim-bulb",
    "title": "Packages Don’t Work Well for Analyses in Practice",
    "section": "A Dim Bulb",
    "text": "A Dim Bulb\nHowever, {drake} was becoming popular as a proper workflow manager for R, with some very nice capabilities, and I started using it to help out with some large caching things I wanted for analyses for a manuscript. I still didn’t use it for anything else.\nThe documentation for {drake} very much encourages a functional workflow, where one writes functions that take inputs from other functions and generate outputs for other functions (Will Landau 2020). And though Will encourages this in the documentation, for some reason I was not grokking how this worked in practice, and how it could work easily for all of my projects with some simple conventions around folder structure borrowed from other projects (or even from R packages).\n\nWill Landau recommends not using {drake} anymore and to switch to {targets}. Given development on drake has stopped, that’s probably a good idea. There still isn’t a {targets} flavor of {dflow} (see below) available yet, however."
  },
  {
    "objectID": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#a-little-brighter",
    "href": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#a-little-brighter",
    "title": "Packages Don’t Work Well for Analyses in Practice",
    "section": "A Little Brighter",
    "text": "A Little Brighter\nMiles McBain was talking about some of these things (around data workflows) intermittently on Twitter, and then posted about a functional approach to analyses using {drake} (McBain 2020a). When I read it, I was really, really impressed with it. Unfortunately, I still wasn’t really clear on how to implement and use his method with {drake}. This is not the fault of Miles’ post or {drake}’s documentation (see above). It is my fault for not reading it slowly, and taking the time to try and stop through it with the {dflow} implementation and {drake} documentation together.\n\nIf you haven’t read it yet, go read it to see if it might help you. Seriously, it’s really, really good, and well thought out."
  },
  {
    "objectID": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#lightbulb-goes-off",
    "href": "posts/2021-03-02-packages-dont-work-well-for-analyses-in-practice/index.html#lightbulb-goes-off",
    "title": "Packages Don’t Work Well for Analyses in Practice",
    "section": "Lightbulb Goes Off",
    "text": "Lightbulb Goes Off\nMiles’ post was at the end of April, 2020. In August 2020, he was a guest at the New York Open Statistical Programming Meetup virtual event where he presented his {dflow} workflow in That Feeling of Workflowing (Lander 2020) (McBain 2020b). Seeing and hearing Miles explain everything, and then actually work through an example that everything about his approach finally clicked for me. And really, any project based layout that combines a directory for R scripts, and inputs and outputs too (e.g. {ProjectTemplate} and others).\nNow, I know, that still sounds like an R package, with functions in /R, reports in /vignettes, and built in documentation. But it’s lighter weight than a package, and the functions are specific to just this one analysis.\n{dflow} includes some nice boilerplate code for keeping your list of packages in one place, and getting all of your functions into the workspace. I was so impressed that I installed {dflow} the next day and working out how to convert my current analysis project to use it. Shortly after experimenting with that one analysis, I was converted. Every new project I start is being done using {dflow}, and any old projects I’ve gone back to are being converted over to use it.\nI would now recommend using {drake} / {targets} / {dflow} or {ProjectTemplate} (or any other directory based system with caching) for R analysis projects. And if you have the discipline and find it works for you, then using R packages might still work (there is even a {drake} / {targets} way of working within packages that I hadn’t noticed before). However, I think the general project based ideas are just fine for many analyses, and packages are overkill and take too much work for the analysis authors to keep it up.\n{drake} (and now {targets}) is a modern make like engine specifically for R, with an R based caching system (saving things so they don’t have to be repeated again) for your outputs that really, really helps you keep track of things, organize your inputs and outputs, and work in a functional way, which are all good things. {dflow} is a very pretty wrapper for setting up {drake} analysis projects.\nEdited on 2021-03-03 to give more credence to Will Landau and the {drake} package itself, as Miles McBain felt I gave too much credence to him instead of {drake}."
  },
  {
    "objectID": "posts/2022-02-08-project-consultation-template/index.html",
    "href": "posts/2022-02-08-project-consultation-template/index.html",
    "title": "Project Consultation Template",
    "section": "",
    "text": "A little while ago, I was tweeting about the joy of collaborating with people who haven’t included you in their experimental design. One of the tweets was this:\n\n\n\n\n\nI decided that I should follow up with our actual project consultation template."
  },
  {
    "objectID": "posts/2022-02-08-project-consultation-template/index.html#consultations",
    "href": "posts/2022-02-08-project-consultation-template/index.html#consultations",
    "title": "Project Consultation Template",
    "section": "Consultations?",
    "text": "Consultations?\nA large part of my work is collaborative projects where I do various -omics analyses for members of the cancer center I’m part of. As I’m sure many of you know, it’s very important for us to capture the details of the experiments so we can figure out what actually needs to be done, and the most appropriate method.\nInternally, we use a self-hosted foswiki instance to keep lists of what needs to be done, document SOPs, and capture experimental details from collaborators.\nWith every project we get asked to work on, we have a meeting, and who is at that meeting is usually determined by what kind of analysis the potential collaborator has asked for."
  },
  {
    "objectID": "posts/2022-02-08-project-consultation-template/index.html#template",
    "href": "posts/2022-02-08-project-consultation-template/index.html#template",
    "title": "Project Consultation Template",
    "section": "Template",
    "text": "Template\nDuring that meeting, we fill in all the details of this template (which I translated to markdown from foswiki). Obviously, not all of the fields necessarily get filled out for each project, and sometimes other custom fields will get added. This template helps us capture the minimum information needed.\n## Meeting Description\n  * Date/Time: \n  * Length: 1 hour\n\n### Attendance\n  * \n  * \n\n### Meeting Purpose\n\n## Project Description\n\n### Overall Purpose\n\n### Implementation Timeline\n  * Design/Conceptualization:\n  * Experiment: \n    * Start:  ; End:\n  * Sample Collection and Preparation:\n  * Data Collection:\n  * Data Analysis:\n\n### Experimental Design\n\n### Specific Data Analysis Questions\n\n## Conclusions\n\n### TODO"
  },
  {
    "objectID": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html",
    "href": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html",
    "title": "Don’t do PCA After Statistical Testing!",
    "section": "",
    "text": "If you do a statistical test before a dimensional reduction method like PCA, the highest source of variance is likely to be whatever you tested statistically."
  },
  {
    "objectID": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#wait-why",
    "href": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#wait-why",
    "title": "Don’t do PCA After Statistical Testing!",
    "section": "Wait, Why??",
    "text": "Wait, Why??\nLet me describe the situation. You’ve done an -omics level analysis on your system of interest. You run a t-test (or ANOVA, etc) on each of the features in your data (gene, protein, metabolite, etc). Filter down to those things that were statistically significant, and then finally, you decide to look at the data using a dimensionality reduction method such as principal components analysis (PCA) so you can see what is going on.\nI have seen this published at least once (in a Diabetes metabolomics paper, if anyone knows it, please send it to me so I can link it), and have seen collaborators do this after coaching from others in non-statistical departments."
  },
  {
    "objectID": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#the-problem",
    "href": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#the-problem",
    "title": "Don’t do PCA After Statistical Testing!",
    "section": "The Problem",
    "text": "The Problem\nThe problem is that PCA is just looking at either feature-feature covariances or sample-sample covariances. If you have trimmed the data to those things that have statistically significant differences, then you have completely modified the covariances, and PCA is likely to pick up on that."
  },
  {
    "objectID": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#an-example",
    "href": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#an-example",
    "title": "Don’t do PCA After Statistical Testing!",
    "section": "An Example",
    "text": "An Example\nLet’s actually do an example where there are no differences initially, and then see if we can introduce an artificial difference.\n\nRandom Data\nWe start with completely random data, 10000 features, and 100 samples.\n\nn_feat = 10000\nn_sample = 100\nrandom_data = matrix(rnorm(n_feat * n_sample), nrow = n_feat, ncol = n_sample)\n\nNow we will do a t-test on each row, taking the first 50 samples as class 1 and the other 50 samples as class 2.\n\nt_test_res = purrr::map_df(seq(1, nrow(random_data)), function(in_row){\n  tidy(t.test(random_data[in_row, 1:50], random_data[in_row, 51:100]))\n})\n\nHow many are significant at a p-value of 0.05?\n\nfilter(t_test_res, p.value <= 0.05) %>% dim()\n\n[1] 514  10\n\n\nObviously, these are false positives, but they are enough for us to illustrate the problem.\nFirst, lets do PCA on the whole data set of 10000 features.\n\nsample_classes = data.frame(class = c(rep(\"A\", 50), rep(\"B\", 50)))\n\nall_pca = prcomp(t(random_data), center = TRUE, scale. = FALSE)\npca_scores = cbind(as.data.frame(all_pca$x), sample_classes)\nggplot(pca_scores, aes(x = PC1, y = PC2, color = class)) + geom_point()\n\n\n\n\nObviously, there is no difference in the groups, and the % explained variance is very low.\nSecond, lets do it on just those things that were significant:\n\nsig_pca = prcomp(t(random_data[which(t_test_res$p.value <= 0.05), ]), center = TRUE,\n                 scale. = FALSE)\nsig_scores = cbind(as.data.frame(sig_pca$x), sample_classes)\n\nggplot(sig_scores, aes(x = PC1, y = PC2, color = class)) + \n  geom_point() +\n  theme(legend.position = c(0.5, 0.5))\n\n\n\n\nAnd look at that! We have separation of the two groups! But …., this is completely random data, that didn’t have any separation, until we did the statistical test!"
  },
  {
    "objectID": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#take-away",
    "href": "posts/2018-09-14-don-t-do-pca-after-statistical-testing/index.html#take-away",
    "title": "Don’t do PCA After Statistical Testing!",
    "section": "Take Away",
    "text": "Take Away\nBe careful of the order in which you do things. If you want to do dimensionality reduction to look for issues with the samples, then do that before any statistical testing on the individual features."
  },
  {
    "objectID": "posts/2017-12-27-custom-deployment-script/index.html",
    "href": "posts/2017-12-27-custom-deployment-script/index.html",
    "title": "Custom Deployment Script",
    "section": "",
    "text": "Use a short bash script to do deployment from your own computer directly to your *.github.io domain."
  },
  {
    "objectID": "posts/2017-12-27-custom-deployment-script/index.html#why",
    "href": "posts/2017-12-27-custom-deployment-script/index.html#why",
    "title": "Custom Deployment Script",
    "section": "Why?",
    "text": "Why?\nSo Yihui recommends using Netlify, or even Travis-CI in the Blogdown book. I wasn’t willing to setup a custom domain yet, and some of my posts involve a lot of personally created packages, etc, that I don’t want to debug installation on Travis. So, I wanted a simple script I could call on my laptop that would copy the /public directory to the repo for my github.io site, and then push the changes."
  },
  {
    "objectID": "posts/2017-12-27-custom-deployment-script/index.html#the-script",
    "href": "posts/2017-12-27-custom-deployment-script/index.html#the-script",
    "title": "Custom Deployment Script",
    "section": "The Script",
    "text": "The Script\nHere is the simple script I ended up using:\n#!/bin/bash\norg_dir=`pwd`\ncd path/to/github.io/repo/\n#rm -rf *\ncp -Rfu path/to/blogdown/public/* .\n\ngit add *\ncommit_time=`date`\ngit commit -m \"update at $commit_time\"\ngit push origin master\n\ncd $org_dir\nIt changes directories, because to push from a git repo I’m pretty sure you need to be in the directory, so it also makes sure to go back there at the end. It then copies the contents of /public to the repo, adds all the files, and then uses the current time-stamp as the commit message, and finally pushes all the updates."
  },
  {
    "objectID": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html",
    "href": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html",
    "title": "Authentication of Key Resources for Data Analysis",
    "section": "",
    "text": "NIH recently introduced a reproducibility initiative, extending to including the “Authentication of Key Resources” page in grant applications from Jan 25, 2016. Seems to be intended for grants involving biological reagents, but we included it in our recent R03 grant developing new data analysis methods. We believe that this type of thing should become common for all grants, not just those that use biological/chemical resources."
  },
  {
    "objectID": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html#nih-and-reproducibility",
    "href": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html#nih-and-reproducibility",
    "title": "Authentication of Key Resources for Data Analysis",
    "section": "NIH and Reproducibility",
    "text": "NIH and Reproducibility\nThere has been a lot of things published recently about the reproducibility crisis in science (see refs). The federal funding agencies are starting to respond to this, and beginning with grants submitted after January 25, 2016, grants are supposed to address the reproducibility of the work proposed, including the presence of various confounding factors (i.e. sex of animals, the source of cell lines, etc). In addition to this, there is a new document that can be added to grants, the Authentication Plan, which as far as I can tell is intended specifically for:\n\nkey biological and/or chemical resources used in the proposed studies\n\nNow, this makes sense. Some sources of irreproducibility include, but are not limited to:\n\nunvalidated antibodies\ncell lines that are not what was thought\nimpure chemicals\n\nI think this is a good thing. What does it have to do with data analysis?"
  },
  {
    "objectID": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html#data-code-authentication",
    "href": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html#data-code-authentication",
    "title": "Authentication of Key Resources for Data Analysis",
    "section": "Data / Code Authentication",
    "text": "Data / Code Authentication\nWhen we were submitting a recent R03 proposal for developing novel data analysis methods and statistical tools, the grant management office asked us about the Authentication of Key Resources attachment, which we completely missed. Upon review of the guidelines, we initially determined that this document did not apply. However, we decided to go ahead and take some initiative.\n\nData Authentication?\nWhen dealing with multiple samples from high-throughput samples, there are frequently a few easy ways to examine the data quality, and although it can be hard to verify that the data is what the supplier says it is, which would be true authentication, there are some ways to verify that the various samples in the dataset are at least self-consistent within each sample class (normal and disease, condition 1 and condition 2).\nMy go-to for data self-consistency are principal components analysis (PCA) and correlation heat-maps. Correlation heat-maps involve calculating all of the pairwise sample to sample correlations using all of the non-zero sample features (those that are non-zero in the two pairs being compared). These heatmaps, combined with the sample class information, and clustering within each class, are a nice visual way to eyeball samples that have potential problems. A simple example for RNA-seq transcriptomics was shown in Gierliński et al., Statistical models for RNA-seq data derived from a two-condition 48-replicate experiment Bioinformatics (2015) 31 (22): 3625-3630, Figure 1.\n\nknitr::include_graphics(\"yeast_48rep_cor_heatmap.jpg\")\n\n\n\n\nThe other measures they used in this paper are also very nice, in plotting the median correlation of a sample against all other samples, and the fraction of outlier features in a given sample (see figure 2 of Gierlinkski et al). The final measure they propose is not generally applicable to all -omics data however.\nPCA on the data, followed by visualizing the scores on the first few principal components, and colored by sample class (or experimental condition) is similar in spirit to the correlation heat-map. In fact, it is very similar, because PCA is actually decomposing on the covariance of the samples, which is very related to the correlations (an early algorithm actually used the correlation matrix).\nBoth of these methods can highlight possible problems with individual samples, and make sure that the set of data going into the analysis is at least self-consistent, which is important when doing classification or differential abundance analyses.\n\n\nCode Authentication\nThe other thing we highlighted in the document was code authentication. In this case, we highlighted the use of unit-testing in the R packages that we are planning to develop. Even though this is software coming out of a research lab, we need to have confidence that the functions we write return the correct values given various inputs. In addition, code testing coverage helps evaluate that we are testing all of the functionality by checking that all of the lines in our code are run by the tests. Finally, we are also planning to write tests for core functions provided by others (i.e. functions in other R packages), in that they work as we expect, by returning correct values given specific inputs."
  },
  {
    "objectID": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html#conclusion",
    "href": "posts/2016-03-23-authentication-of-key-resources-for-data-analysis/index.html#conclusion",
    "title": "Authentication of Key Resources for Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nGoing forward, I think it would be a good thing if people writing research grants for data analysis methods would discuss how they are going to look at the data to assess it’s quality, and how they are going to do unit testing, and will have to start saying that they are going to do unit testing of their analysis method.\nI’d be interested in others’ thoughts on this as well."
  },
  {
    "objectID": "posts/2018-01-17-docopt-numeric-optoins/index.html",
    "href": "posts/2018-01-17-docopt-numeric-optoins/index.html",
    "title": "docopt & Numeric Options",
    "section": "",
    "text": "If you use the docopt package to create command line R executables that take options, there is something to know about numeric command line options: they should have as.double before using them in your script."
  },
  {
    "objectID": "posts/2018-01-17-docopt-numeric-optoins/index.html#setup",
    "href": "posts/2018-01-17-docopt-numeric-optoins/index.html#setup",
    "title": "docopt & Numeric Options",
    "section": "Setup",
    "text": "Setup\nLets set up a new docopt string, that includes both string and numeric arguments.\n\n\"\nUsage:\n  test_numeric.R [--string=<string_value>] [--numeric=<numeric_value>]\n  test_numeric.R (-h | --help)\n  test_numeric.R\n\nDescription: Testing how values are passed using docopt.\n\nOptions:\n  --string=<string_value>  A string value [default: Hi!]\n  --numeric=<numeric_value>   A numeric value [default: 10]\n\n\" -> doc\n\n\nlibrary(methods)\nlibrary(docopt)\n\nscript_options <- docopt(doc)\n\nscript_options\n\n## List of 8\n##  $ --string : chr \"Hi!\"\n##  $ --numeric: chr \"10\"\n##  $ -h       : logi FALSE\n##  $ --help   : logi FALSE\n##  $ string   : chr \"Hi!\"\n##  $ numeric  : chr \"10\"\n##  $ h        : logi FALSE\n##  $ help     : logi FALSE\n## NULL\nIt is very easy to see here, that the numeric argument is indeed a string, and if you want to use it as numeric, it should first be converted using as.double, as.integer, or even as.numeric."
  },
  {
    "objectID": "posts/2018-01-17-docopt-numeric-optoins/index.html#cant-you-easily-tell-its-character",
    "href": "posts/2018-01-17-docopt-numeric-optoins/index.html#cant-you-easily-tell-its-character",
    "title": "docopt & Numeric Options",
    "section": "Can’t You Easily Tell It’s Character?",
    "text": "Can’t You Easily Tell It’s Character?\nI just bring this up because I recently used docopt to provide interfaces to three executables scripts, and I spent a lot of time printing the doc strings, and I somehow never noticed that the numeric values were actually character and needed to be converted to a numeric first. Hopefully this will save someone else some time in that regard."
  },
  {
    "objectID": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html",
    "href": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html",
    "title": "R, RStudio, and Release and Dev Bioconductor",
    "section": "",
    "text": "Update 2021-02-18: Now I would just use the r-docker image and the RStudio interface. I actually just did this recently to test updates to my package.\nI have one Bioconductor package that I am currently responsible for. Each bi-annual release of Bioconductor requires testing and squashing errors, warnings and bugs in a given package. Doing this means being able to work with multiple versions of R and multiple versions of Bioconductor libraries on a single system (assuming that you do production work and development on the same machine, right?).\nI really, really like RStudio as my working R environment, as some of you have read before. So how do we get RStudio on our Linux system to respect which version of R and libraries we want to use?"
  },
  {
    "objectID": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html#setup",
    "href": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html#setup",
    "title": "R, RStudio, and Release and Dev Bioconductor",
    "section": "Setup",
    "text": "Setup\nThis assumes that you have your R installs set somewhere properly, and a normal library for production level packages. You should install whichever Bioconductor packages you want into the normal library, and then make a copy of that. This copy will be your development library.\ncp -R productionLibrary developmentLibrary\nI also assume that you are using a local (i.e. sits in your home directory) .Renviron file to control where R installs the packages."
  },
  {
    "objectID": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html#changing-versions",
    "href": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html#changing-versions",
    "title": "R, RStudio, and Release and Dev Bioconductor",
    "section": "Changing Versions",
    "text": "Changing Versions\nRStudio really needs the environment variable RSTUDIO_WHICH_R set to know where R is, and R looks at R_LIBS in the .Renviron file. So I simply create two shell scripts that get sourced.\n\nuseRDev.sh\n#!/bin/sh\nexport RSTUDIO_WHICH_R=/pathToDevR/bin/R\necho \"R_LIBS=pathtoDevLibs\" > .Renviron\nThen I can simply do source useRDev.sh when I need to use the development R and library. Note that you will need to start RStudio from the shell for it to respect this environment variable. RStudio generally seems to install to /usr/lib/rstudio/bin/rstudio.\n\n\nresetR.sh\n#!/bin/sh\nexport RSTUDIO_WHICH_R=/pathtoReleaseR/bin/R\necho \"R_LIBS=pathtoReleaseLibs\" > .Renviron\nThis resets my variables by doint source resetR.sh."
  },
  {
    "objectID": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html#bioconductor-dev-version",
    "href": "posts/2013-09-17-r-rstudio-and-release-and-dev-bioconductor/index.html#bioconductor-dev-version",
    "title": "R, RStudio, and Release and Dev Bioconductor",
    "section": "Bioconductor Dev Version",
    "text": "Bioconductor Dev Version\nTo setup Bioconductor to use the develoment version, simply:\nsource useRDev.sh\nrstudio\n\n# once in RStudio\nlibrary(BiocInstaller)\nuseDev()\nbiocLite(ask=F) # this will update all the installed bioconductor packages\nI know this is not the most ideal situation, because I am rewriting over files, but it is working for me, and I thought it might help somone else."
  },
  {
    "objectID": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html",
    "href": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html",
    "title": "Open vs Closed Analysis Languages",
    "section": "",
    "text": "I think data scientists should choose to learn open languages such as R and python because they are open in the sense that anyone can obtain them, use them and modify them for free, and this has lead to large, robust groups of users, making it more likely that packages exist that you can use, and others can easily build on your own work."
  },
  {
    "objectID": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#why-the-debate",
    "href": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#why-the-debate",
    "title": "Open vs Closed Analysis Languages",
    "section": "Why the debate?",
    "text": "Why the debate?\nThis was sparked by a comment on twitter suggesting that data scientists and analysts need to be polyglots, that they should know more than one programming language or analysis framework (the full conversation of tweets can be found here)\n\n\nData Scientists need to be Polyglots - know 2 or more of #python #rstats #sas #spss #matlab #julia #octave http://t.co/LtzIOzZ4XH\n\n— Gregory Piatetsky (@kdnuggets) October 8, 2013\n\n\nThe commenter suggested knowing at least two of:\n\nR\nPython\nMatlab\nSPSS\nSAS\nJulia\nOctave\n\nMy comment back was that one should really evaluate whether SPSS, SAS and Matlab should be on this list, as they are closed languages, not open, or free.\nI want to expand on why I made that comment. Let me be forthright, I have not used SPSS, nor SAS, but I have programmed in MatLab and R extensively, and dabbled in Python.\nI also think it is a good thing for data scientists to know more than one language. Just to be clear, I am NOT arguing that point."
  },
  {
    "objectID": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#closed",
    "href": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#closed",
    "title": "Open vs Closed Analysis Languages",
    "section": "Closed",
    "text": "Closed\nWhat is a closed analysis language? I would say that there are three types of closed languages:\n\nthose that are not free (but still may their source code openly available)\nthose where the underlying engine is closed source\nthose where one cannot write their own functions to expand on what is already available\n\nNow, MatLab fits the first two categories. It is rather expensive to get a license for, the license can be restrictive (I know they have had a lot of abuse of licenses in the past, and they are trying to avoid that), and you are not expected to poke around in the internals of the MatLab engine. Oh, and if you want more than the base engine, expect to pay heavily for add-on packages.\nHowever, it is possible to write add-on’s for MatLab. I have previously written a few.\n\nClosed problem: Checking results\nSo why are closed languages a problem? A closed language that does not make it possible to examine the underlying functionality of the analysis engine has two problems:\n\nsurety that calculations are done correctly\nthe ability of others to run and check results\n\nBoth of these are issues that are really important in science. I would consider a data scientist to be doing actual science, so others should be able to scrutinize their work. The best scrutiny, is for others to be able to actually run their code. If I can’t run your code, then how do I know what you did is right?? If I can’t afford a copy of MatLab to run your code (assuming you made it available, you did provide the source for the analysis, right?), that is a bad thing.\n\n\nClosed problem: Re-using code\nOf course, the other problem is that with a closed language you have made it impossible for others to easily make use of your analysis. Sure, they could code it up in another language, but unless it is the be-all and end-all of analysis methods, I’m not going to bother. I don’t have a license for MatLab, or SPSS, or SAS, and I can’t afford it; therefore I’m not going to use your method / code nor give you a citation or credit."
  },
  {
    "objectID": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#solution-open-languages",
    "href": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#solution-open-languages",
    "title": "Open vs Closed Analysis Languages",
    "section": "Solution: Open languages",
    "text": "Solution: Open languages\nLanguages like R and python, they don’t have these problems. If I wonder how a function in the base distribution of R or python works, I can go look at the source. If I find a bug, I can suggest a fix, or fix it myself and tell others about it. In addition, if I write code to do an analysis, I can make it available and know that others should have the ability to examine it, including re-running it, in addition to using it for themselves, if it is licensed appropriately. This is the way science should work."
  },
  {
    "objectID": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#what-should-you-use",
    "href": "posts/2013-10-21-open-vs-closed-analysis-languages/index.html#what-should-you-use",
    "title": "Open vs Closed Analysis Languages",
    "section": "What should you use?",
    "text": "What should you use?\nSome would argue that you should use MatLab, SAS, and SPSS because they have been around for a while, and are the standard. I would argue that you should not use them because they are controlled by single corporate entities, who are only interested in what will get people to buy their product and use it. You should use software that others are using, and that others will be able to use, regardless of income.\nR is being used in lots of different places, by lots of different people for statistics, bioinformatics, visualization, and as a general functional language. Python is a great general purpose language that provides a lot of functional glue for doing lots of different things."
  },
  {
    "objectID": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html",
    "href": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html",
    "title": "Criticizing a Publication, and Lying About It",
    "section": "",
    "text": "Other researchers directly criticized a recent publication of ours in a “research article”. Although they raised valid points, they outright lied about the availability of our results. In addition, they did not provide access to their own results. We have published new work supporting our original results, and a direct rebuttal of their critique in a perspective article. The peer reviewers of their “research article” must have been asleep at the wheel to allow the major point, lack of access to our results, to stand."
  },
  {
    "objectID": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#original-publication",
    "href": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#original-publication",
    "title": "Criticizing a Publication, and Lying About It",
    "section": "Original Publication",
    "text": "Original Publication\nBack in the summer of 2015, I was second author on a publication (Yao et al., 2015, hereafter YS2015) describing an automated method to characterize zinc ion coordination geometries (CGs). Applying our automated method to all zinc sites in the worldwide Protein Data Bank (wwPDB), we found abberrant zinc CGs that don’t fit the canonical CGs. We were pretty sure that these aberrant CGs are real, and they have always existed, but had not been previously characterized because methods assumed that only the canonical geometries should be observed in biological systems, and were excluding the abberrant ones because they didn’t have good methods to detect and characterize them.\nAlso of note, the proteins with aberrant zinc geometries showed enrichment for different types of enzyme classifications than those with canonical zinc geometries.\nFor this publication, we made all of our code and results available in a tarball that could be downloaded from our website. This data went up while the paper was in review, on Dec 7, 2015 (with a correction on Dec 15). Recently, we’ve also put a copy of the tarball on FigShare. Every draft of the publication, from initial submission through to accepted publication, included the link to the tarball on the website."
  },
  {
    "objectID": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#critique",
    "href": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#critique",
    "title": "Criticizing a Publication, and Lying About It",
    "section": "Critique",
    "text": "Critique\nLess than a year later, Raczynska, Wlodawer, and Jaskolski (RJW2016) published a critique of YS2015 as a “research article”. In their publication, they questioned the existence of the abberrant sites completely, based on the examination and remodeling of four aberrant structures highlighted in YS2015. To be fair, they did have some valid criticisms of the methods, and Sen Yao did a lot of work in our latest paper to address them.\nAs part of the critique, however, they claimed that they could only evaluate the four structures listed in two figures because we didn’t provide all of our results. However, we had previously made our full results available as a tarball from our website. As you can see in the below figure, all of the results were really available in that tarball.\n<img src = “/img/ys2017_figure1.png”, width = “600”>\nIn addition, although RWJ2016 went to all the trouble to actually remodel those four structures by going back to the original X-ray density, they didn’t make any of their models available.\nFinally, no one from RWJ2016 ever contacted our research group to see if the results might be available."
  },
  {
    "objectID": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#response",
    "href": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#response",
    "title": "Criticizing a Publication, and Lying About It",
    "section": "Response",
    "text": "Response\n\nFollow-Up Paper on 5 Metals\nBy the time the critiques appeared in RJW2016, Sen was already hard at work showing that the previously developed methods could be modified and then applied to other metal ion CGs, and that they also contained aberrant CGs (see YS2017-1).\n\n\nCritique Direct Response\nIn addition to YS2017-1, we felt that the critique deserved separate response (YS2017-2). To that end, we began drafting a response, wherein we pointed out some of the problems with RJW2016, the first being that we did indeed provide the full set of results from YS2015, and therefore it was possible to evaluate our full work. We also addressed each of their other criticisms of YS2015, in many cases going beyond the original criticism, and explaining how it was being addressed in YS2017-1.\n\n\nOpen Results and Code\nA major part of the conclusions in YS2017-2 was also devoted to the idea that code and results in science need to be shared, highlighting the fact that RJW2016 did not share their models they used to try and discredit our work, lied about the fact that we did not share our own results, and pointing out some other projects in this research area that have shared well and others that have shared badly, and that the previous attitude of competition among research groups does not move science forward."
  },
  {
    "objectID": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#peer-review",
    "href": "posts/2017-03-29-criticizing-a-publication-and-lying-about-it/index.html#peer-review",
    "title": "Criticizing a Publication, and Lying About It",
    "section": "Peer Review",
    "text": "Peer Review\nLet’s just say that the peer-review of both of the papers was interesting. Both manuscripts had the same set of reviewers. YS2017-1, the five metal paper, had some rather rigorous peer review, and was definitely improved by the reviewer’s comments. YS2017-2, our perspective, in contrast, was attacked by one peer reviewer right from submission, and was questioned almost continually as to whether it should even be published. I am thankful that one reviewer saw the need for it to be published, and that the Editor ultimately decided that it should be published, and that we were able to rebut each of the reviewer’s criticisms.\nFinally, I really don’t know what happened in the peer review of RWJ2016. The first major claim was that our data wasn’t available, it should have taken a reviewer 10 minutes to verify and debunk that claim. I would have expected a much different critique from the authors had they actually examined our full data set. But, because of traditional closed peer review, that record is closed to us.\nOverall though, I’m very happy both of our publications are now out, and we can move on to new stages of our analyses. Looking forward to continuing to work with my co-authors to move the work forward.\n\nPapers Discussed\n\nOriginal Zinc CGs: Yao et al 2015\nCritique of Zinc CGs: Raczynska, Wlodawer & Jaskolski 2016, publisher, sci-hub\n5 Metal CGs: Yao et al 2017\nResponse to critique: Yao et al 2017, publisher, copy on figshare"
  },
  {
    "objectID": "posts/2014-02-10-installing-matlab-vs-installing-r/index.html",
    "href": "posts/2014-02-10-installing-matlab-vs-installing-r/index.html",
    "title": "Installing MatLab vs Installing R",
    "section": "",
    "text": "I retweeted this a few days ago:\n1. Open MATLAB for first time in a few years after using #rstats. 2. Site license doesn’t work right. 3. F*** MATLAB, I’ll try to do it in R\nAnd as I have started the process of installing MatLab on my own machine because I want to translate a published MatLab package into R, I am reminded of how painful the process can be.\nAs Andrew noted above, often times a university will have a site license for MatLab, and the license server will be local, and your installation must just contact the license server to know it is legit and run. However, if the server crashes (which they seem to be prone to in my experience), then you can’t run MatLab at all."
  },
  {
    "objectID": "posts/2014-02-10-installing-matlab-vs-installing-r/index.html#installing-matlab",
    "href": "posts/2014-02-10-installing-matlab-vs-installing-r/index.html#installing-matlab",
    "title": "Installing MatLab vs Installing R",
    "section": "Installing MatLab",
    "text": "Installing MatLab\nAt my university, the process for even getting and installing a copy of MatLab is rather tedious.\n\nProve you are someone who is in a position to be allowed to install it\nReceive email with license file and further instructions\nOpen an IT ticket to have an account created at MathWorks\nLogin to MathWorks and download MatLab\nInstall\nGive license key\nSave license file\nRun MatLab\n\nNow, I understand that MathWorks wants to protect their intellectual property, and I also understand that they have previously been ripped off rather badly in the past (having a company purchase a single license and run a hundred copies). But this process is not conducive to getting research done."
  },
  {
    "objectID": "posts/2014-02-10-installing-matlab-vs-installing-r/index.html#installing-r",
    "href": "posts/2014-02-10-installing-matlab-vs-installing-r/index.html#installing-r",
    "title": "Installing MatLab vs Installing R",
    "section": "Installing R",
    "text": "Installing R\n\nDownload appropriate R installer from http://r-project.org\nInstall\nInstall desired packages\n\nWhich one of these is going to make it easiest to actually get coding and getting stuff done?"
  },
  {
    "objectID": "posts/2013-05-10-writing-up-scientific-results-and-literate-programming/index.html",
    "href": "posts/2013-05-10-writing-up-scientific-results-and-literate-programming/index.html",
    "title": "Writing Up Scientific Results and Literate Programming",
    "section": "",
    "text": "As an academic researcher, my primary purpose is to find some new insight, and subsequently communicate this insight to the general public. The process of doing this is traditionally thought to be:\nAnd then repeat.\nThis is the way people envision it happening. And I would imagine that in some rare cases, this is what actually happens. However, I think many researchers would agree that this is not what normally happens. In the process of doing steps 3 and 4, your hypothesis in 1 will be modified, which modifies the experiments in 2, and so on and so forth. This makes the process of scientific discovery a very iterative process, often times right up to the report writing.\nFor some of this, it takes a long time to figure this out. I’ll never forget a professor during my PhD who suggested that you write the paper, and then figure out what experiments you should do to generate the results that would support or disprove the hypothesis you made in the paper. At the time I thought he was nuts, but when you start writing stuff, and looking at how all the steps of experiment and reporting can become intertwined, it doesn’t seem like a bad idea."
  },
  {
    "objectID": "posts/2013-05-10-writing-up-scientific-results-and-literate-programming/index.html#literate-programming",
    "href": "posts/2013-05-10-writing-up-scientific-results-and-literate-programming/index.html#literate-programming",
    "title": "Writing Up Scientific Results and Literate Programming",
    "section": "Literate programming",
    "text": "Literate programming\nWhat does this have to do with literate programming? For those who don’t know, literate programming is a way to mix code and prose together in one document (in R we use knitr & sweave, python now has the iPython notebook as an option). This literate programming paradigm (combined with markdown as the markup language instead of latex thanks to knitr) is changing how I actually write my papers and do research in general."
  },
  {
    "objectID": "posts/2013-05-10-writing-up-scientific-results-and-literate-programming/index.html#how-that-changes-writing",
    "href": "posts/2013-05-10-writing-up-scientific-results-and-literate-programming/index.html#how-that-changes-writing",
    "title": "Writing Up Scientific Results and Literate Programming",
    "section": "How that changes writing",
    "text": "How that changes writing\nAs I’ve previously described 12, RStudio makes the use of knitr and generation of literate documents using computations in R incredibly easy. Because my writing environment and programming environment are tightly coupled together, I can easily start writing what looks like a shareable, readable publication as soon as I start writing code. Couple this together with a CVS like git, and you have a way to follow the complete providence of a publication from start to finish.\nInstead of commenting my code to explain why I am doing something, I explain what I am doing in the prose, and then write the code to carry out that analysis. This changes my writing and coding style, and makes the interplay among the steps of writing scientific reports above much more explicit. I think it is a good thing, and will hopefully make my writing and research more productive.\n\nSources\nPublished 10.05.13 here\nThe source markdown for this document is available here"
  },
  {
    "objectID": "posts/2012-08-15-journal-club-2012-08-15/index.html",
    "href": "posts/2012-08-15-journal-club-2012-08-15/index.html",
    "title": "Journal Club 2012-08-15",
    "section": "",
    "text": "I just came back from our Bioinformatic group (a rather loose association of various researchers at UofL interested in and doing bioinformatics) journal club, where we discussed this recent paper:\nGoogle Goes Cancer: Improving Outcome Prediction for Cancer Patients by Network-Based Ranking of Marker Genes\nBesides the catchy title that makes one believe that perhaps Google is getting into cancer research (maybe they are and we don’t know it yet), there were some interesting aspects to this paper."
  },
  {
    "objectID": "posts/2012-08-15-journal-club-2012-08-15/index.html#premise",
    "href": "posts/2012-08-15-journal-club-2012-08-15/index.html#premise",
    "title": "Journal Club 2012-08-15",
    "section": "Premise",
    "text": "Premise\nThe premise is that they can combine gene expression data and network data to find better associations between gene expression data and a particular disease endpoint. The way this is carried out is through the use of the TRANSFAC transcription factor - gene target database for the network, the correlation of the gene expression with the disease status as the importance of a gene with the disease, and the Google PageRank as the means to transfer the network knowledge to the gene expression data. They call their method NetRank.\nNote that the general idea had already been tried in this paper on GeneRank."
  },
  {
    "objectID": "posts/2012-08-15-journal-club-2012-08-15/index.html#implementation",
    "href": "posts/2012-08-15-journal-club-2012-08-15/index.html#implementation",
    "title": "Journal Club 2012-08-15",
    "section": "Implementation",
    "text": "Implementation\nRank the genes with disease status (poor or good prognosis) using a method (SAM, t-test, fold-change, correlation, NetRank). Pick n top genes, and develop a predictive model using a support vector machine. Wash, rinse, repeat several times to find the best set, varying the number of top genes, and the number of samples used in the training set.\nFor NetRank, the top genes were decided by using a sub-optimization based on varying d, the dampening factor in the PageRank algorithm that determines how much information can be transferred to other genes. The best value of d determined in this study was 0.3.\nAll other methods used just the 8000 genes that passed filtering, but NetRank used all the genes on the array, with those that were filtered out had their initial correlations set to 0, so that they were still in the network representation.\n\n\n\nMonte Carlo cross-validation"
  },
  {
    "objectID": "posts/2012-08-15-journal-club-2012-08-15/index.html#did-it-work",
    "href": "posts/2012-08-15-journal-club-2012-08-15/index.html#did-it-work",
    "title": "Journal Club 2012-08-15",
    "section": "Did it work?",
    "text": "Did it work?\nFrom the paper, it appears to have worked. Using a monte-carlo cross-validation, they were able to achieve over 70% prediction rates. And this was better than any of the other methods they used to associate genes with the disease, including SAM, t-test, fold-change, and raw correlations.\n\n\n\nNetRank feature selection performance"
  },
  {
    "objectID": "posts/2012-08-15-journal-club-2012-08-15/index.html#issues",
    "href": "posts/2012-08-15-journal-club-2012-08-15/index.html#issues",
    "title": "Journal Club 2012-08-15",
    "section": "Issues",
    "text": "Issues\nAs we discussed the article, some questions did come up.\n\nWhat was the variation in d depending on the size of the training set?\nHow consistent were the genes that came out as biomarkers?\n\n\nIt would be nice to try this methodology on a series of independent, but related cancer datasets (ie breast or lung cancer) and see how consistent the lists are. This was done here.\n\n\nWhat happens if the genes that don’t pass filtering are removed from the network entirely?\nWere the problems reported with not-filtering genes due to having only two disease points (poor and good prognosis) to calculate a correlation of expression with?\nHow many iterations does it take to achieve convergence?\nThe list of genes they come up with are fairly well known cancer genes. We were kindof surprised that they didn’t seem to come up novel genes associated directly with pancreatic cancer.\nWhy is d so variable depending on the cancer examined?"
  },
  {
    "objectID": "posts/2012-08-15-journal-club-2012-08-15/index.html#things-to-try",
    "href": "posts/2012-08-15-journal-club-2012-08-15/index.html#things-to-try",
    "title": "Journal Club 2012-08-15",
    "section": "Things to try",
    "text": "Things to try\n\nCould we improve on this by instead of taking just the top-ranked genes, look for the top ranked cliques, i.e. take the top gene, remove anything in its immediate neighborhood, and then go to the next one?\nWhat would happen if we used a directed network based on connected Reactome or KEGG pathways?\n\nFind this post online at: http://robertmflight.blogspot.com/2012/08/journal-club-150812.html\nAuthored using Markdown, and the R Markdown package. Published on 15.08.12"
  },
  {
    "objectID": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html",
    "href": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html",
    "title": "Using IRanges for Non-Integer Overlaps",
    "section": "",
    "text": "The IRanges package implements interval algebra, and is very fast for finding overlaps of two ranges. If you have non-integer data, multiply values by a large constant factor and round them. The constant depends on how much accuracy you need."
  },
  {
    "objectID": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#iranges",
    "href": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#iranges",
    "title": "Using IRanges for Non-Integer Overlaps",
    "section": "IRanges??",
    "text": "IRanges??\nIRanges is a bioconductor package for interval algebra of integer ranges. It is used extensively in the GenomicRanges package for finding overlaps between various genomic features. For genomic features, integers make sense, because one cannot have fractional base locations.\nHowever, IRanges uses red-black trees as its data structure, which provide very fast searches of overlaps. This makes it very attractive for any problem that involves overlapping ranges."
  },
  {
    "objectID": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#motivation",
    "href": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#motivation",
    "title": "Using IRanges for Non-Integer Overlaps",
    "section": "Motivation",
    "text": "Motivation\nMy motivation comes from mass-spectrometry data, where I want to count the number of raw data points and / or the number of peaks in a large number of M/Z windows. Large here means on the order of 1,000,000 M/Z windows.\nGenerating the windows is not hard, but searching the list of points / peaks for which ones are within the bounds of a window takes a really long time. Long enough that I needed some other method."
  },
  {
    "objectID": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#iranges-to-the-rescue",
    "href": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#iranges-to-the-rescue",
    "title": "Using IRanges for Non-Integer Overlaps",
    "section": "IRanges to the Rescue!",
    "text": "IRanges to the Rescue!\nSo my idea was to use IRanges. But there is a problem, IRanges is for integer ranges. How do we use this for non-integer data? Simple, multiply and round the fractional numbers to generate integers.\nIt turns out that multiplying our mass-spec data by 20,000 gives us differences down to the 0.00005 place, which is more than enough accuracy for the size of the windows we are interested in. If needed, IRanges can handle 1600 * 1e6, but currently will crash at 1600 * 1e7."
  },
  {
    "objectID": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#how-fast-is-it",
    "href": "posts/2018-06-23-iranges-for-non-integer-overlaps/index.html#how-fast-is-it",
    "title": "Using IRanges for Non-Integer Overlaps",
    "section": "How Fast Is It?",
    "text": "How Fast Is It?\nLets actually test differences in speed by counting how many overlapping points there are.\n\nlibrary(IRanges)\nlibrary(ggplot2)\nload(here::here(\"data_files/iranges_example_data.rda\"))\n\nhead(mz_points)\n\nIRanges object with 6 ranges and 1 metadata column:\n          start       end     width |        mz\n      <integer> <integer> <integer> | <numeric>\n  [1]   2970182   2970182         1 |   148.509\n  [2]   2970183   2970183         1 |   148.509\n  [3]   2970184   2970184         1 |   148.509\n  [4]   2970186   2970186         1 |   148.509\n  [5]   3000526   3000526         1 |   150.026\n  [6]   3000527   3000527         1 |   150.026\n\nhead(mz_windows)\n\nIRanges object with 6 ranges and 2 metadata columns:\n          start       end     width |  mz_start    mz_end\n      <integer> <integer> <integer> | <numeric> <numeric>\n  [1]   2960000   2960011        12 |       148   148.001\n  [2]   2960001   2960012        12 |       148   148.001\n  [3]   2960002   2960013        12 |       148   148.001\n  [4]   2960003   2960014        12 |       148   148.001\n  [5]   2960004   2960016        13 |       148   148.001\n  [6]   2960006   2960017        12 |       148   148.001\n\n\nI have some example data with 3447542 windows, and 991816 points. We will count how many point there are in each window using the below functions, with differing number of windows.\n\nFunctions\n\ncount_overlaps_naive <- function(mz_start, mz_end, points){\n  sum((points >= mz_start) & (points <= mz_end))\n}\n\niterate_windows <- function(windows, points){\n  purrr::pmap_int(windows, count_overlaps_naive, points)\n}\n\nrun_times_iterating <- function(windows, points){\n  t <- Sys.time()\n  window_counts <- iterate_windows(windows, points)\n  t2 <- Sys.time()\n  run_time <- difftime(t2, t, units = \"secs\")\n  run_time\n}\n\nrun_times_countoverlaps <- function(windows, points){\n  t <- Sys.time()\n  window_counts <- countOverlaps(points, windows)\n  t2 <- Sys.time()\n  run_time <- difftime(t2, t, units = \"secs\")\n  run_time\n}\n\n\n\nDefine Samples of Different Sizes\n\nset.seed(1234)\n\nsample_sizes <- c(10, 100, 1000, 10000, 50000, 100000)\n\nwindow_samples <- purrr::map(sample_sizes, function(x){sample(length(mz_windows), size = x)})\n\n\n\nRun It\n\niranges_times <- purrr::map_dbl(window_samples, function(x){\n  run_times_countoverlaps(mz_windows[x], mz_points)\n})\n\nwindow_frame <- as.data.frame(mcols(mz_windows))\n\nnaive_times <- purrr::map_dbl(window_samples, function(x){\n  run_times_iterating(window_frame[x, ], mz_points)\n})\n\n\n\nPlot Them\n\nall_times <- data.frame(size = rep(sample_sizes, 2),\n                        time = c(iranges_times, naive_times),\n                        method = rep(c(\"iranges\", \"naive\"), each = 6))\n\np <- ggplot(all_times, aes(x = log10(size), y = time, color = method)) + geom_point() + geom_line() + labs(y = \"time (s)\", x = \"log10(# of windows)\", title = \"Naive & IRanges Timings\") + theme(legend.position = c(0.2, 0.8))\np\n\n\n\np + coord_cartesian(ylim = c(0, 1))\n\n\n\n\nAs the two figures show, the naive solution, while a little faster under 1000 regions, is quickly outperformed by IRanges, whose time increases much more slowly."
  },
  {
    "objectID": "posts/2013-10-22-pubmedcommons-api/index.html",
    "href": "posts/2013-10-22-pubmedcommons-api/index.html",
    "title": "PubmedCommons API",
    "section": "",
    "text": "The announcements are out, Pubmed is introducing a commenting system pubmedcommons, theoretically providing a single location for true post-publication peer review. This is a really good idea, as NCBI is likely to be around for a lot longer than a given publisher, and the requirement for all NIH funded research to be deposited into Pubmed.\nThere are some detractors, and they may have some valid points link. However, the alternative, pubpeer, I had not heard about. F1000 puts the comments in one location, in a way that is no better than a single publisher site.\nWhat would truly let the pubmedcommons proliferate would be if it became the equivalent of disqus, but for scientific articles. For those who don’t know, disqus is essentially a remote commenting platform that provides commenting on other websites. For example, my blog is a static site, and comments are provided by disqus. This is enabled with an account on disqus and a javascript snippet on my website.\nIt would be neat if pubmedcommons became a similar platform for scientific articles, with a little bit more flexibility. i.e. someone would be able to include some javascript, with relevant pubmed IDs, and the comments for that article would be displayed, and others would be able to make comments on that site.\n\nLet me illustrate with an example.\nImagine I publish a paper in PLOS One (or any other journal, for that matter). Upon acceptance, the paper also gets a pubmed id. This instantly creates a forum for comments using pubmedcommons. In addition, PLOS One adds a little bit of javascript to their page for the paper that enables the same commenting system. Therefore, any comments made on the paper at pubmed or PLOS One appear in both places (maybe with a tag for where they originated?). In addition, I post a copy of the paper on my own blog, and add the same javascript, and readers will see the comments made at pubmed, PLOS One, and can comment on the paper on my own blog. In addition, someone else could do the same to display the comments on their own site, etc, etc.\nI think the above example might have the potential to revolutionize post-pub peer review, because the actual comments are hosted in a central location, but are accessible in many different locations where the publication might exist.\nNow, it could still go awry. They could take too long to open things up, they might make the policies regarding comments too restrictive, they may police the comments too heavily, etc, etc. But\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2013,\n  author = {Robert M Flight},\n  title = {PubmedCommons {API}},\n  date = {2013-10-22},\n  url = {https://rmflight.github.io/posts/2013-10-22-pubmedcommons-api},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2013. “PubmedCommons API.” October 22,\n2013. https://rmflight.github.io/posts/2013-10-22-pubmedcommons-api."
  },
  {
    "objectID": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#what",
    "href": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#what",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "section": "What?",
    "text": "What?\nFor those who don’t know, CDF files are chip definition format files that define which probes on an Affymetrix microarray chip belong together, and are necessary to use any of the standard summarization methods such as RMA, and others."
  },
  {
    "objectID": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#why",
    "href": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#why",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "section": "Why?",
    "text": "Why?\nBecause we can, and because custom definitions have been shown to be quite useful. See the information over at Brainarray."
  },
  {
    "objectID": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#why-not-somewhere-else",
    "href": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#why-not-somewhere-else",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "section": "Why not somewhere else?",
    "text": "Why not somewhere else?\nA lot of times other people create custom CDF files based on their own criteria, and make it subsequently available for others to use (see the Brainarray for an example of what some are doing, as well as PlandbAffy)\nYou have a really nifty idea for a way to reorganize the probesets on an Affymetrix chip to perform a custom analysis, but you don’t want to go to the trouble of actually creating the CDF files and Bioconductor packages normally required to do the analysis, and yet you want to test and develop your analysis method."
  },
  {
    "objectID": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#how",
    "href": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#how",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "section": "How?",
    "text": "How?\nIt turns out you are in luck. At least for AffyBatch objects in Bioconductor (created by calling ReadAffy), the CDF information is stored as an attached environment that can be easily hacked and modified to your hearts content. Environments in R are quite important and useful, and I wouldn’t have come up with this if I hadn’t been working in R for the past couple of years, but figured someone else might benefit from this knowledge."
  },
  {
    "objectID": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#the-environment",
    "href": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#the-environment",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "section": "The environment",
    "text": "The environment\nIn R, one can access an environment like so:\n\nget(\"objName\", envName) # get the value of object in the environment\nls(envName)\n\nWhat is also very cool, is that one can extract the objects in an environment to a list, and also create their own environment from a list using list2env. Using this methodology, we can create our own definition of probesets that can be used by standard Bioconductor routines to summarize the probes into probesets.\nA couple of disclaimers:\n\nI have only tried this on 3’ expression arrays\nThere might be a better way to do this, but I couldn’t find it (let me know in the comments)"
  },
  {
    "objectID": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#example",
    "href": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#example",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "section": "Example",
    "text": "Example\n\nrequire(affy)\n\nLoading required package: affy\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, append, as.data.frame, basename, cbind, colnames,\n    dirname, do.call, duplicated, eval, evalq, Filter, Find, get, grep,\n    grepl, intersect, is.unsorted, lapply, Map, mapply, match, mget,\n    order, paste, pmax, pmax.int, pmin, pmin.int, Position, rank,\n    rbind, Reduce, rownames, sapply, setdiff, sort, table, tapply,\n    union, unique, unsplit, which.max, which.min\n\n\nLoading required package: Biobase\n\n\nWelcome to Bioconductor\n\n    Vignettes contain introductory material; view with\n    'browseVignettes()'. To cite Bioconductor, see\n    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\nrequire(estrogen)\n\nLoading required package: estrogen\n\nrequire(hgu95av2cdf)\n\nLoading required package: hgu95av2cdf\n\n\nWarning: replacing previous import 'AnnotationDbi::tail' by 'utils::tail' when\nloading 'hgu95av2cdf'\n\n\nWarning: replacing previous import 'AnnotationDbi::head' by 'utils::head' when\nloading 'hgu95av2cdf'\n\n\n\n\ndatadir = system.file(\"extdata\", package=\"estrogen\")\n\npd = read.AnnotatedDataFrame(file.path(datadir, \"estrogen.txt\"), header=TRUE, sep=\"\", row.names=1)\npData(pd)\n\n             estrogen time.h\nlow10-1.cel    absent     10\nlow10-2.cel    absent     10\nhigh10-1.cel  present     10\nhigh10-2.cel  present     10\nlow48-1.cel    absent     48\nlow48-2.cel    absent     48\nhigh48-1.cel  present     48\nhigh48-2.cel  present     48\n\ncelDat = ReadAffy(filenames = rownames(pData(pd)), \n                  phenoData = pd,\n                  verbose=TRUE, celfile.path=datadir)\n\n1 reading /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/low10-1.cel ...instantiating an AffyBatch (intensity a 409600x8 matrix)...done.\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/low10-1.cel\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/low10-2.cel\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/high10-1.cel\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/high10-2.cel\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/low48-1.cel\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/low48-2.cel\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/high48-1.cel\nReading in : /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu/estrogen/extdata/high48-2.cel\n\n\nThis loads up the data, reads in the raw data, and gets it ready for us to use. Now, lets see what is in the actual CDF environment.\n\ntopProbes <- head(ls(hgu95av2cdf)) # get a list of probesets\ntopProbes\n\n[1] \"100_g_at\"  \"1000_at\"   \"1001_at\"   \"1002_f_at\" \"1003_s_at\" \"1004_at\"  \n\nexSet <- get(topProbes[1], hgu95av2cdf)\nexSet\n\n          pm     mm\n [1,] 175218 175858\n [2,] 356689 357329\n [3,] 227696 228336\n [4,] 237919 238559\n [5,] 275173 275813\n [6,] 203444 204084\n [7,] 357984 358624\n [8,] 368524 369164\n [9,] 285352 285992\n[10,] 304510 305150\n[11,] 159937 160577\n[12,] 223929 224569\n[13,] 282764 283404\n[14,] 270003 270643\n[15,] 303343 303983\n[16,] 389048 389688\n\n\nWe can see here that the first probe set 100_g_at has 16 perfect-match and mis-match probes in associated with it.\nLets summarize the original data using RMA.\n\nrma1 <- exprs(rma(celDat))\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma1)\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel low48-1.cel\n100_g_at     9.642896    9.741496     9.537036     9.353625    9.591697\n1000_at     10.398169   10.254362    10.003971     9.903528   10.374866\n1001_at      5.717613    5.881008     5.859563     5.954028    5.960540\n1002_f_at    5.512596    5.801807     5.571065     5.608132    5.390064\n1003_s_at    7.783927    8.007975     8.037999     7.835120    7.926487\n1004_at      7.289162    7.603670     7.488539     7.771506    7.521789\n          low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.570590     9.475796     9.530655\n1000_at     10.033520    10.345066     9.863321\n1001_at      6.020889     5.981080     6.285192\n1002_f_at    5.494511     5.508104     5.630107\n1003_s_at    8.138870     7.994937     8.233338\n1004_at      7.599544     7.456149     7.675171\n\n\nNow lets get the data as a list, and then create a new environment to be used for summarization.\n\nallSets <- ls(hgu95av2cdf)\nallSetDat <- mget(allSets, hgu95av2cdf)\n\nallSetDat[1]\n\n$`100_g_at`\n          pm     mm\n [1,] 175218 175858\n [2,] 356689 357329\n [3,] 227696 228336\n [4,] 237919 238559\n [5,] 275173 275813\n [6,] 203444 204084\n [7,] 357984 358624\n [8,] 368524 369164\n [9,] 285352 285992\n[10,] 304510 305150\n[11,] 159937 160577\n[12,] 223929 224569\n[13,] 282764 283404\n[14,] 270003 270643\n[15,] 303343 303983\n[16,] 389048 389688\n\nhgu2 <- list2env(allSetDat)\ncelDat@cdfName <- \"hgu2\"\n\nrma2 <- exprs(rma(celDat))\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma2)\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel low48-1.cel\n100_g_at     9.642896    9.741496     9.537036     9.353625    9.591697\n1000_at     10.398169   10.254362    10.003971     9.903528   10.374866\n1001_at      5.717613    5.881008     5.859563     5.954028    5.960540\n1002_f_at    5.512596    5.801807     5.571065     5.608132    5.390064\n1003_s_at    7.783927    8.007975     8.037999     7.835120    7.926487\n1004_at      7.289162    7.603670     7.488539     7.771506    7.521789\n          low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.570590     9.475796     9.530655\n1000_at     10.033520    10.345066     9.863321\n1001_at      6.020889     5.981080     6.285192\n1002_f_at    5.494511     5.508104     5.630107\n1003_s_at    8.138870     7.994937     8.233338\n1004_at      7.599544     7.456149     7.675171\n\n\nWhat about removing the MM columns? RMA only uses the PM, so it should still work.\n\nallSetDat <- lapply(allSetDat, function(x){\n  x[,1, drop=F]\n})\n\nallSetDat[1]\n\n$`100_g_at`\n          pm\n [1,] 175218\n [2,] 356689\n [3,] 227696\n [4,] 237919\n [5,] 275173\n [6,] 203444\n [7,] 357984\n [8,] 368524\n [9,] 285352\n[10,] 304510\n[11,] 159937\n[12,] 223929\n[13,] 282764\n[14,] 270003\n[15,] 303343\n[16,] 389048\n\nhgu3 <- list2env(allSetDat)\ncelDat@cdfName <- \"hgu3\"\nrma3 <-exprs(rma(celDat))\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma3)\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel low48-1.cel\n100_g_at     9.642896    9.741496     9.537036     9.353625    9.591697\n1000_at     10.398169   10.254362    10.003971     9.903528   10.374866\n1001_at      5.717613    5.881008     5.859563     5.954028    5.960540\n1002_f_at    5.512596    5.801807     5.571065     5.608132    5.390064\n1003_s_at    7.783927    8.007975     8.037999     7.835120    7.926487\n1004_at      7.289162    7.603670     7.488539     7.771506    7.521789\n          low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.570590     9.475796     9.530655\n1000_at     10.033520    10.345066     9.863321\n1001_at      6.020889     5.981080     6.285192\n1002_f_at    5.494511     5.508104     5.630107\n1003_s_at    8.138870     7.994937     8.233338\n1004_at      7.599544     7.456149     7.675171\n\n\nWhat if we only want to use the first 5 probesets?\n\nallSetDat <- allSetDat[1:5]\nhgu4 <- list2env(allSetDat)\ncelDat@cdfName <- \"hgu4\"\ncelDat\n\nAffyBatch object\nsize of arrays=640x640 features (22 kb)\ncdf=hgu4 (5 affyids)\nnumber of samples=8\nnumber of genes=5\nannotation=hgu95av2\nnotes=\n\nrma4 <- exprs(rma(celDat))\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nrma4\n\n          low10-1.cel low10-2.cel high10-1.cel high10-2.cel low48-1.cel\n100_g_at     9.463007    9.554665     9.449050     9.401976    9.447562\n1000_at     10.182753   10.009785    10.009785     9.970396   10.102424\n1001_at      5.943840    6.005177     5.944015     6.089531    6.237329\n1002_f_at    5.787166    5.846225     5.816964     5.814798    5.763175\n1003_s_at    7.750877    7.769401     7.913021     7.864052    7.860778\n          low48-2.cel high48-1.cel high48-2.cel\n100_g_at     9.457986     9.401366     9.431078\n1000_at     10.009785    10.197065     9.889555\n1001_at      6.147957     6.189200     6.206669\n1002_f_at    5.763175     5.740757     5.755085\n1003_s_at    7.917565     7.862614     7.928691\n\ndim(rma4)\n\n[1] 5 8"
  },
  {
    "objectID": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#custom-cdf",
    "href": "posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/index.html#custom-cdf",
    "title": "Creating Custom CDFs for Affymetrix Chips in Bioconductor",
    "section": "Custom CDF",
    "text": "Custom CDF\nTo generate our custom CDF, we are going to set our own names, and take random probes from all of the probes on the chip. The actual criteria of which probes should be together can be made using any method the author chooses.\n\nmaxIndx <- 640*640\n\ncustomCDF <- lapply(seq(1,100), function(x){\n  tmp <- matrix(sample(maxIndx, 20), nrow=20, ncol=1)\n  colnames(tmp) <- \"pm\"\n  return(tmp)\n})\n\nnames(customCDF) <- seq(1, 100)\n\nhgu5 <- list2env(customCDF)\ncelDat@cdfName <- \"hgu5\"\nrma5 <- exprs(rma(celDat))\n\nBackground correcting\nNormalizing\nCalculating Expression\n\nhead(rma5)\n\n    low10-1.cel low10-2.cel high10-1.cel high10-2.cel low48-1.cel low48-2.cel\n1      6.805156    6.993719     6.899264     6.932725    6.928347    6.957158\n10     6.441884    6.319232     6.297822     6.198441    6.274462    6.419477\n100    5.700887    5.712753     5.618964     5.717375    5.824081    5.741998\n11     5.243884    5.204840     5.156431     5.149247    5.455638    5.249618\n12     6.790776    6.660627     6.718511     6.576265    6.828149    7.009592\n13     5.666141    5.752682     5.697687     5.696507    5.622639    5.778334\n    high48-1.cel high48-2.cel\n1       6.915275     6.836921\n10      6.204771     5.928888\n100     5.944784     5.899380\n11      5.284089     5.298048\n12      6.744098     6.714799\n13      5.660630     5.666974\n\n\nI hope this information is useful to someone else. I know it made my life a lot easier.\n\nsessionInfo()\n\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Pop!_OS 22.04 LTS\n\nMatrix products: default\nBLAS:   /rmflight_stuff/software/R-4.2.1/lib/libRblas.so\nLAPACK: /rmflight_stuff/software/R-4.2.1/lib/libRlapack.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] hgu95av2cdf_2.18.0  estrogen_1.42.0     affy_1.74.0        \n[4] Biobase_2.56.0      BiocGenerics_0.42.0\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9             XVector_0.36.0         GenomeInfoDb_1.32.4   \n [4] compiler_4.2.1         BiocManager_1.30.19    bitops_1.0-7          \n [7] tools_4.2.1            zlibbioc_1.42.0        digest_0.6.29         \n[10] bit_4.0.4              jsonlite_1.8.0         evaluate_0.16         \n[13] RSQLite_2.2.19         memoise_2.0.1          preprocessCore_1.58.0 \n[16] png_0.1-7              rlang_1.0.5            cli_3.4.0             \n[19] DBI_1.1.3              rstudioapi_0.14        yaml_2.3.5            \n[22] xfun_0.33              fastmap_1.1.0          GenomeInfoDbData_1.2.8\n[25] httr_1.4.4             stringr_1.4.1          knitr_1.40            \n[28] Biostrings_2.64.1      IRanges_2.30.1         S4Vectors_0.34.0      \n[31] htmlwidgets_1.5.4      vctrs_0.4.1            stats4_4.2.1          \n[34] bit64_4.0.5            R6_2.5.1               AnnotationDbi_1.58.0  \n[37] rmarkdown_2.16         blob_1.2.3             magrittr_2.0.3        \n[40] htmltools_0.5.3        KEGGREST_1.36.3        renv_0.15.5           \n[43] stringi_1.7.8          RCurl_1.98-1.8         cachem_1.0.6          \n[46] crayon_1.5.1           affyio_1.66.0         \n\n\nOriginally published 2013/07/13, moved to http://rmflight.github.io on 2013/12/04."
  },
  {
    "objectID": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/index.html",
    "href": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/index.html",
    "title": "Finding Modes Using Kernel Density Estimates",
    "section": "",
    "text": "If you have a unimodal distribution of values, you can use R’s density or Scipy’s gaussian_kde to create density estimates of the data, and then take the maxima of the density estimate to get the mode. See below for actual examples in R and Python."
  },
  {
    "objectID": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/index.html#mode-in-r",
    "href": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/index.html#mode-in-r",
    "title": "Finding Modes Using Kernel Density Estimates",
    "section": "Mode in R",
    "text": "Mode in R\nFirst, lets do this in R. Need some values to work with.\n\nlibrary(ggplot2)\nset.seed(1234)\nn_point <- 1000\ndata_df <- data.frame(values = rnorm(n_point))\n\nggplot(data_df, aes(x = values)) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nggplot(data_df, aes(x = values)) + geom_density()\n\n\n\n\nWe can do a kernel density, which will return an object with a bunch of peices. One of these is y, which is the actual density value for each value of x that was used! So we can find the mode by querying x for the maxima in y!\n\ndensity_estimate <- density(data_df$values)\n\nmode_value <- density_estimate$x[which.max(density_estimate$y)]\nmode_value\n\n[1] -0.04599328\n\n\nPlot the density estimate with the mode location.\n\ndensity_df <- data.frame(value = density_estimate$x, density = density_estimate$y)\n\nggplot(density_df, aes(x = value, y = density)) + geom_line() + geom_vline(xintercept = mode_value, color = \"red\")"
  },
  {
    "objectID": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/index.html#python",
    "href": "posts/2018-07-19-finding-modes-using-kernel-density-estimates/index.html#python",
    "title": "Finding Modes Using Kernel Density Estimates",
    "section": "Python",
    "text": "Python\nLets do something similar in Python. Start by generating a set of random values.\n\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\nvalues = np.random.normal(size = 1000)\n\nplt.hist(values)\n\n(array([  1.,   7.,  44., 105., 217., 296., 207.,  87.,  27.,   9.]), array([-3.80368902, -3.10698209, -2.41027516, -1.71356823, -1.01686129,\n       -0.32015436,  0.37655257,  1.07325951,  1.76996644,  2.46667337,\n        3.16338031]), <BarContainer object of 10 artists>)\n\nplt.show()\n\n\n\n\nAnd then use gaussian_kde to get a kernel estimator of the density, and then call the pdf method on the original values.\n\nkernel = stats.gaussian_kde(values)\nheight = kernel.pdf(values)\n\nmode_value = values[np.argmax(height)]\nprint(mode_value)\n\n-0.08045792953113866\n\n\nPlot to show indeed we have it right. Note we sort the values first so the PDF looks right.\n\nvalues2 = np.sort(values.copy())\nheight2 = kernel.pdf(values2)\n\nplt.clf()\nplt.cla()\nplt.close()\n\n<string>:1: MatplotlibDeprecationWarning: The close_event function was deprecated in Matplotlib 3.6 and will be removed two minor releases later. Use callbacks.process('close_event', CloseEvent(...)) instead.\n\nplt.plot(values2, height2)\nplt.axvline(mode_value)\nplt.show()"
  },
  {
    "objectID": "posts/2014-06-19-researcher-discoverability/index.html",
    "href": "posts/2014-06-19-researcher-discoverability/index.html",
    "title": "Researcher Discoverability",
    "section": "",
    "text": "University of Kentucky (UK) recently partnered with the discovery portal KNODE, for helping others to discover potential collaborators at UK. KNODE looks like a large corporate venture, that is probably costing a large amount of capital to the university (and other places that use it). I wonder if the universities money would be better spent on encouraging submission of preprints, a Github Enterprise/Education package and teaching researchers and faculty how to use social media like twitter. Here is why I think that."
  },
  {
    "objectID": "posts/2014-06-19-researcher-discoverability/index.html#preprints",
    "href": "posts/2014-06-19-researcher-discoverability/index.html#preprints",
    "title": "Researcher Discoverability",
    "section": "Preprints",
    "text": "Preprints\nSubmitting manuscripts to a preprint server such as arxiv, biorxiv or peerj gives a researchers work possible visibility, and can result in immediate feedback to improve a work prior to submission, or as part of the peer review."
  },
  {
    "objectID": "posts/2014-06-19-researcher-discoverability/index.html#github",
    "href": "posts/2014-06-19-researcher-discoverability/index.html#github",
    "title": "Researcher Discoverability",
    "section": "Github",
    "text": "Github\nMuch of our scientific output is generated as text. Whether it is lab notes, tables of data, draft manuscripts, or scientific scripts and programs, a lot of it is text. The Git version control system is well placed to handle iterations of text, and Github has become the defacto location for sharing version controlled text with others, and collaborating on it. Although I would imagine that it would take some encouragement, I think more researchers would use Git and Github if they knew about it, and had easy options to make Github repos private until they were ready to share their work.\nIn addition to putting general text under version control, more researchers should be encouraged to use version control on any analysis scripts / programs that they write. This is becoming important across disciplines, not just in STEM fields. There is also more and more encouragement from journals and funding bodies to make underlying data and the processing of the data available. By putting data, and code on Github, moving from private collaboration to fully shared can be changed with the flick of a switch. Furthermore, master branches of repos are crawled by Google, making your text discoverable, making webpages for a group and on a project basis is relatively painless using github-pages, and repos can also have a wiki associated with them to allow further documentation."
  },
  {
    "objectID": "posts/2014-06-19-researcher-discoverability/index.html#twitter",
    "href": "posts/2014-06-19-researcher-discoverability/index.html#twitter",
    "title": "Researcher Discoverability",
    "section": "Twitter",
    "text": "Twitter\nI’m going to use twitter as a proxy for social media in general (blogs, facebook, twitter, and others), although I believe twitter combined with a blog is probably the best combination for science outreach.\nHaving a blog to put announcements about when a new publication comes out, when a new presentation has been done (you are putting presentations on figshare or slideshare), with links to the work in question. Twitter (and other social media platforms) allow quick communication to a wide audience that your research group has a new research output, and can allow engagement with other researchers in the field as well as non-experts, all of which improve ones visibility and provide opportunities for collaboration.\nBy the way, github-pages as mentioned above, also makes it easy to set up a static blog (non database blog site) for rather easy blogging."
  },
  {
    "objectID": "posts/2014-06-19-researcher-discoverability/index.html#open-science",
    "href": "posts/2014-06-19-researcher-discoverability/index.html#open-science",
    "title": "Researcher Discoverability",
    "section": "Open Science",
    "text": "Open Science\nAll of the above encourages open science, sharing of results early and often. It does not have to, but as people get used to being able to share their work early and often, one would hope that this would become the norm, and it would lead to other researchers and possibly interested companies discovering applicable research for either collaboration, licensing, or simply re-use."
  },
  {
    "objectID": "posts/2014-06-19-researcher-discoverability/index.html#how-to-implement",
    "href": "posts/2014-06-19-researcher-discoverability/index.html#how-to-implement",
    "title": "Researcher Discoverability",
    "section": "How to implement",
    "text": "How to implement\nNow, if you’ve read this and you are not already doing these things, you might think this is all a lot of effort. Well, yes, it is. It takes time. This blog post didn’t write itself, you know. My twitter use sometimes gets a bit excessive and addictive. And some of what I have described is rather technical in nature, and might take training to implement. For example, best practices around code and version control. I’m sure Software Carpentry would love to do workshops at UK for researchers (disclaimer: I have done the SC training, but haven’t done a workshop yet). But overall, I know for me all of this (I’m speaking from experience and what I have observed in others) has been a plus to my research, not a minus.\nSo what say you, University of Kentucky? Why not try something different to make your researchers stand out to the world, and improve the scientific process?"
  },
  {
    "objectID": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html",
    "href": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "section": "",
    "text": "If you just want the hook scripts, check this gist. If you want to know some of the motivation behind writing them, and about the internals, then read on."
  },
  {
    "objectID": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#package-version-incrementing",
    "href": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#package-version-incrementing",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "section": "Package Version Incrementing",
    "text": "Package Version Incrementing\nA good practice to get into is incrementing the minor version number (i.e. going from 0.0.1 to 0.0.2) after each git commit when developing packages (this is recommended by the Bioconductor devs as well ). This makes it very easy to know what changes are in your currently installed version, and if you remembered to actually install the most recent version for testing.\nIf you are like me, this is a hard habit to get into, especially because I have to manually remember to go over to the DESCRIPTION file and up the version number when I make a commit. It seems I am not the only one according to this tweet from Kevin Ushey:\n\n\nIs there a git hook / alias that can auto-increment a package patch number (in DESCRIPTION) with each commit? #rstats #lazyweb\n\n— Kevin Ushey (@kevin_ushey) February 6, 2014\n\n\nI thought it sounded like a good idea, and got to work."
  },
  {
    "objectID": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#git-hooks",
    "href": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#git-hooks",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "section": "Git Hooks",
    "text": "Git Hooks\nFor those who don’t know, git allows you to have custom scripts (hooks) that are run at different steps in an overall git workflow. These are essentially executable scripts stored in the .git/hooks/ folder of an individual repo.\nFor our purposes, we would want something that runs either just before commit (pre-commit hook) or just after (post-commit hook), because we want to make modifications that are associated with commits themselves. The pre-commit hook is most appropriate, because we can make file modifications before our actual commit. However, if the user is already using a validation pre-commit hook, or wants the version change separate, a post-commit hook might be better. Both are available in the gist.\nI suppose this could have been done using an alias to run a script, but the nice thing about commit hooks is they are tied to the action of committing itself, instead of as a separate action."
  },
  {
    "objectID": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#rscript-read-and-write.dcf",
    "href": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#rscript-read-and-write.dcf",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "section": "Rscript & read and write.dcf",
    "text": "Rscript & read and write.dcf\nR already has some nice functionality for reading and writing the debian control format (dcf) DESCRIPTION files that are required at the root of an R package. So ideally we could write our script in R and not have to do parsing from the bash shell. Rscript lets us do this easily by defining in the shebang (#!) where to find the executable to run the script.\n#!/path/2/Rscript\nThe rest is simply reading the DESCRIPTION file (read.dcf), getting the current version, incrementing, writing a new DESCRIPTION file (write.dcf), and making the changes to the git commit.\n\ncurrDCF <- read.dcf(\"DESCRIPTION\")\ncurrVersion <- currDCF[1,\"Version\"]\nsplitVersion <- strsplit(currVersion, \".\", fixed=TRUE)[[1]]\nnVer <- length(splitVersion)\ncurrEndVersion <- as.integer(splitVersion[nVer])\nnewEndVersion <- as.character(currEndVersion + 1)\nsplitVersion[nVer] <- newEndVersion\nnewVersion <- paste(splitVersion, collapse=\".\")\ncurrDCF[1,\"Version\"] <- newVersion\nwrite.dcf(currDCF, \"DESCRIPTION\")\n\nIn order to add the new DESCRIPTION file (pre-commit), or do a subsequent commit, we do need a couple of system calls to git itself.\n\nsystem(\"git add DESCRIPTION\")\n\nWe also need to check that there are files to actually be commited (for the pre-commit) before we go ahead and increment the version. The git diff command will tell us if there is anything or not.\n\nfileDiff <- system(\"git diff HEAD --name-only\", intern=TRUE)"
  },
  {
    "objectID": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#overriding-increment",
    "href": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#overriding-increment",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "section": "Overriding Increment",
    "text": "Overriding Increment\nThere is two wrinkles to this process. What if you don’t want to increment the version of your package for a good reason, like you’ve just changed the major version number? Or, in the case of the post-commit, you don’t want to end up in an infinite loop of incrementing.\nBut you can’t pass arguments from the git call to the script. It turns out we can prepend an environment variable definition, and then get it using Sys.getenv. So our script checks for the environment variable, and if it is defined, sets it. Otherwise it uses the default that is initialized at the beginning of the script.\n\ndoIncrement <- TRUE # default\n \n# get the environment variable and modify if necessary\ntmpEnv <- as.logical(Sys.getenv(\"doIncrement\"))\nif (!is.na(tmpEnv)){\n  doIncrement <- tmpEnv\n}\n\nSo to change the default value of doIncrement, you make your commit like this:\ndoIncrement=FALSE git commit -m \"commit message\"\nThis same behavior is used to keep from making an infinite number of increments in the post-commit script."
  },
  {
    "objectID": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#installation",
    "href": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#installation",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "section": "Installation",
    "text": "Installation\nTo install the hooks, create the pre-commit or post-commit file in your .git/hooks directory, paste in the commands from the appropriate one in the gist, save the file, and make it executable (chmod on ’nix systems). I have been able to use both of these on ’nix and Windows systems. Also don’t forget to provide the path to your Rscript executable (either in the same directory as your R binary, or at /usr/bin/Rscript).\nJust in testing these scripts I have become surprised at the potential utility, and I hope you find them useful as well if you are developing R packages (which you should be if you are writing any analysis).\nIf you have comments, suggestions, or improvements feel free to modify the gist, or leave a comment, contact me on twitter, or send me an email (check the About link above)."
  },
  {
    "objectID": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#caveats",
    "href": "posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/index.html#caveats",
    "title": "Package Version Increment Pre- and Post-Commit Hooks",
    "section": "Caveats",
    "text": "Caveats\nThe script assumes the version separator is “.”, and that you always are incrementing the last value. If you don’t like it, feel free to change it."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "",
    "text": "Six years ago, we didn’t really look into what the free academic version of GitHub teams provided for private repositories, and instead decided that we should have a self-hosted GitLab instance. Outside of some weird issues around SSL certificates and reverse proxies (i.e. how our web-hosting worked), this actually worked pretty well. However, instead of making our internal projects public, we would frequently move them over to GitHub. I think this is partly because GitHub has become the de-facto location for open-source tool development. However, it seemed to cause some friction to push things between the two platforms. This may be due to our lab not using git as much as we should be, especially the collaborative aspects in issues and pull requests.\nAfter six years, our PI decided to look into the costs for GitHub teams, and discovered that academic teams get free private repos with rather generous amounts of storage and actions minutes. Therefore, we decided it was time to migrate our projects over to GitHub. The PI delegated this task to yours truly."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#getting-the-list-of-projects",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#getting-the-list-of-projects",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "Getting the List of Projects",
    "text": "Getting the List of Projects\nTo get the list of projects to migrate, I actually copied the project lists from the web interface. If I was doing this again, I would have just asked for the backup locations of our GitLab projects and used those to get the paths for each project. Some projects had changed their names over time and not updated the corresponding path in the database."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#users-to-ids",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#users-to-ids",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "Users to IDs",
    "text": "Users to IDs\nBecause I used lists from the web interface, I also had to convert the users to actual IDs that define the paths to each project. These weren’t too hard to look up."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#local-clones",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#local-clones",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "Local Clones",
    "text": "Local Clones\nWith a full list of projects as “User Name / Project”, and users to IDs, I could easily script converting users to IDs and the full project path, and then system calls to GIT_SSL_NO_VERIFY=TRUE git clone --mirror gitlab_url to make mirror clones of each project. GIT_SSL_NO_VERIFY=TRUE deals with our labs weird SSL certificate issues, while using --mirror means all the branches are pulled, not just the default branch. This is useful to maintain a history of branches over the course of the project development.\nNote that this means you get a directory with “projectName.git”, not just “projectName”. If you look inside the “projectName.git” folder, it’s actually all of the git contents, not the usual files you would see for a normal clone operation."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#duplicate-projects",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#duplicate-projects",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "Duplicate Projects",
    "text": "Duplicate Projects\nOne thing I didn’t think about when doing this is that users on GitLab can fork each others projects, resulting in duplicate projects. If I had been smart, I would have cloned the projects into user specific directories. Thankfully, we only had a few duplicate projects where I had to do a second clone to a user specific directory."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#empty-projects",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#empty-projects",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "Empty Projects",
    "text": "Empty Projects\nSome of the projects are actually empty, either because we thought we were going to do something and didn’t, or someone just wanted issue tracking (that would be me). These we don’t want to push back up to GitHub. However, empty projects with no commits appear to have less than two files in the objects directory, and zero files in the objects/pack directory."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#creating-github-repos",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#creating-github-repos",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "Creating GitHub Repos",
    "text": "Creating GitHub Repos\nI used the GitHub command-line-interface tools (gh) to interface with our GitHub organization. After authorizing the gh tools, for each GitLab project I created a corresponding GitHub project, and then moved to the local clone, and did\ngithub_locs = \"/path/to/github/repos\"\npurrr::walk(gitlab_project_dirs, function(in_dir){\n  proj_name = basename(in_dir)\n  setwd(github_locs)\n  gh_command = paste0(\"gh repo create --confirm --private orgName/\", proj_name)\n  system(gh_command)\n  setwd(in_dir)\n  gh_loc = paste0(\"git@github.com/orgName/\", proj_name)\n  git_push = paste0(\"git push --mirror \", gh_loc)\n  system(git_push)\n})\nYou can see here I move back and forth between directories a lot, and used system to run gh and git push. It feels a little hacky, but it worked just fine."
  },
  {
    "objectID": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#repos-later",
    "href": "posts/2021-11-25-migrating-self-hosted-gitlab-projects-to-github/index.html#repos-later",
    "title": "Migrating Self-Hosted GitLab Projects to GitHub",
    "section": "190 Repos Later",
    "text": "190 Repos Later\nI moved over 180 GitLab projects to GitHub repos. With the projects that already had GitHub repos, that means we have over 190 lab repos. That doesn’t count the stuff we left related to testing CI/CD, people learning how to work with GitLab, and 20 copies of the code we use for sys-admin. There were a subset I had to do by hand because of replication issues and not using user directories. I also discovered some missing repos due to a missed letter when copying one lab members user ID."
  },
  {
    "objectID": "posts/2013-10-17-pre-calculating-large-tables-of-values/index.html",
    "href": "posts/2013-10-17-pre-calculating-large-tables-of-values/index.html",
    "title": "Pre-Calculating Large Tables of Values",
    "section": "",
    "text": "I’m currently working on a project where we want to know, based on a euclidian distance measure, what is the probability that the value is a match to the another value. i.e. given an actual value, and a theoretical value from calculation, what is the probability that they are the same? This can be calculated using a chi-square distribution with one degree-of-freedom, easily enough by considering how much of the chi-cdf we are taking up.\nThe catch is, we want to do this a whole lot of times, in c++. We could use the boost library to calculate the chi-square each time we need it. Or we could generate a lookup table that is able to find the p-value simply based on the distance. This is especially attractive if we have a limit past which we consider the probability of a match as being zero, and if we use enough decimal points that we don’t suffer too much in precision.\nAlthough our goal is to implement this in c++, I also want to prototype, demonstrate and evaluate the approach in R."
  },
  {
    "objectID": "posts/2013-10-17-pre-calculating-large-tables-of-values/index.html#r",
    "href": "posts/2013-10-17-pre-calculating-large-tables-of-values/index.html#r",
    "title": "Pre-Calculating Large Tables of Values",
    "section": "R",
    "text": "R\n\nRandom number set\nWe are going to consider 25 (5 standard deviations squared) as our cutoff for saying the probability is zero. So to make sure we are doing all calculations using the exact same thing, we will pre-generate the values for testing on real data, in this case a set of 1000000 random numbers from zero to 25.\n\nnPoint <- 1000000\nrandomData <- abs(rnorm(nPoint, mean=5, sd=5)) # take absolute so we have only positive values\nrandomData[randomData > 25] <- 25\nhist(randomData, 100)\n\n\n\n\nWe will have three ways to do this in R:\n\nusing the pchisq function\nin a for loop\nas a lookup table\n\nI’m going to create these all as functions, and then time each one using microbenchmark.\n\npchisq_baser <- function(randomData) {\n  1 - pchisq(randomData, 1)\n}\n\npchisq_for <- function(randomData){\n  naiveRes <- numeric(length(randomData))\n  for (iP in 1:length(randomData)) {\n    naiveRes[iP] <- 1 - pchisq(randomData[iP], 1)\n  }\n  naiveRes\n}\n\n# creating the lookup table\nnDivision <- 10000\ndof <- 1\nnSD <- 25\nnElements <- nSD * nDivision\nchiVals <- seq(0, nElements, 1) / nDivision\npTable <- 1 - pchisq(chiVals, 1)\n\npchisq_lookup <- function(randomData, lookupTable, nDivision){\n  tableRes = numeric(length(randomData))\n  for (iP in 1:length(randomData)) {\n    tableRes[iP] <- lookupTable[(randomData[iP] * nDivision) + 1]\n  }\n  tableRes\n}\n\nbase_res = pchisq_baser(randomData)\nlookup_res = pchisq_lookup(randomData, pTable, nDivision)\n\nHow long do each of these take?\n\nres = microbenchmark::microbenchmark(\n  base = pchisq_baser(randomData),\n  for_loop = pchisq_for(randomData),\n  lookup = pchisq_lookup(randomData, pTable, nDivision),\n  times = 50\n)\n\n\nggplot2::autoplot(res)\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\nWhat about any loss in precision of the values returned?\n\nlookupRawPrecision <- abs(lookup_res - base_res) / base_res * 100\n\nprecTable <- data.frame(org = base_res, table = lookup_res, percError = lookupRawPrecision)\nggplot(precTable, aes(x=org, y=table)) + geom_point()\n\n\n\nggplot(precTable, aes(x=org, y=percError)) + geom_point()\n\n\n\n\nSo, according to this, we are only introducing error at 0.7905138%, which isn’t much. And the values look like the are well correlated, so we should be good.\nNow, how do these approaches compare when using c++?"
  },
  {
    "objectID": "posts/2013-10-17-pre-calculating-large-tables-of-values/index.html#c",
    "href": "posts/2013-10-17-pre-calculating-large-tables-of-values/index.html#c",
    "title": "Pre-Calculating Large Tables of Values",
    "section": "C++",
    "text": "C++\nSo it’s a fair comparison, the code below actually writes the c++ program we are going to use, with the random numbers for the p-value calculation stored as part of the code file.\nA couple of notes:\n\nTo be fair, both versions of the code have the set of random numbers and the lookup table as float variables, so that there is no difference in each for memory allocation.\n\n\nNeither one stores the results of the calculation, we don’t need it for this demonstration.\n\n\nRaw calculations\n\ncppRaw <- c('#include <iostream>',\n               '#include <boost/math/distributions/chi_squared.hpp>',\n               'int nVal = 1000000;',\n               'double dof = 1.0;',\n               'int i;',\n               paste('float randVals[1000000] = {', paste(as.character(randomData), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               paste('float pTable[250001] = {', paste(as.character(pTable), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               'int main() {',\n               'using boost::math::chi_squared_distribution;',\n               'chi_squared_distribution<> myChi(dof);',\n               'for (i = 0; i < nVal; i++){',\n               '1 - cdf(myChi, randVals[i]);',\n               '};',\n               'return(0);',\n               '};')\ncat(cppRaw, sep=\"\\n\", file=\"cppRaw.cpp\")\n\nsystem2(command = \"g++\", args = \"cppRaw.cpp -o cppRaw.out\")\nsystem2(command = \"time\", args = \"./cppRaw.out\", stderr = \"raw_results.txt\")\nreadLines(\"raw_results.txt\", n = 1)\n\n[1] \"0.45user 0.00system 0:00.45elapsed 100%CPU (0avgtext+0avgdata 7604maxresident)k\"\n\n\n\ncppLookup <- c('#include <iostream>',\n               '#include <boost/math/distributions/chi_squared.hpp>',\n               'int nVal = 1000000;',\n               'double dof = 1.0;',\n               'int i;',\n               paste('float randVals[1000000] = {', paste(as.character(randomData), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               paste('float pTable[250001] = {', paste(as.character(pTable), sep=\"\", collapse=\", \"), '};', sep=\"\", collapse=\"\"),\n               'int main() {',\n               'using boost::math::chi_squared_distribution;',\n               'chi_squared_distribution<> myChi(dof);',\n               'for (i = 0; i < nVal; i++){',\n               'pTable[(int(randVals[i] * nVal))];',\n               '};',\n               'return(0);',\n               '};')\ncat(cppLookup, sep=\"\\n\", file=\"cppLookup.cpp\")\nsystem2(\"g++\", args = \"cppLookup.cpp -o cppLookup.out\")\nsystem2(\"time\", args = \"./cppLookup.out\", stderr = \"lookup_results.txt\")\nreadLines(\"lookup_results.txt\", n = 1)\n\n[1] \"0.00user 0.00system 0:00.00elapsed 100%CPU (0avgtext+0avgdata 3528maxresident)k\"\n\n\nSo bypassing boost in this case is a good thing, we get some extra speed, and reduce a dependency. We have to generate the lookup table first, but the cpp file can be generated once, with a static variable in a class that is initialized to the lookup values. We do have some error, but in our case we can live with it, as the relative rankings should still be pretty good.\nEdit 2022-12-02: When I originally did this, the lookup R version was in between the for loop and the base R equivalent, and now it seems the lookup version is actually faster, by a bunch, actually. Or I was blinding myself because I wasn’t using microbenchmark?"
  },
  {
    "objectID": "posts/2018-06-20-turn-roberts-beard-purple/index.html",
    "href": "posts/2018-06-20-turn-roberts-beard-purple/index.html",
    "title": "Turn Robert’s Beard Purple!",
    "section": "",
    "text": "If I raise $100 by August 25ths The Walk to End Alzheimer’s, I will have my beard dyed purple in support of Alzheimer’s awareness.\nIf you are in another country, donate to your local Alzheimer’s charity and email me with the subject walk so I count it towards my total.\nLinks:\n\nMy donation page (Charity report on Alzheimer’s Association)\nFacebook Fundraising Page (if you want to share it!)\nAlzheimer Society Canada\nAlzheimer’s Society UK"
  },
  {
    "objectID": "posts/2018-06-20-turn-roberts-beard-purple/index.html#updates",
    "href": "posts/2018-06-20-turn-roberts-beard-purple/index.html#updates",
    "title": "Turn Robert’s Beard Purple!",
    "section": "Updates!",
    "text": "Updates!\n\nJuly 10\nThanks to everyone on Facebook, we’ve raised $5 for the American Alzheimer’s Association, and $30 for the Alzheimer Society in Canada!\nBack on June 25, I went in to Bak 4 More Studio and the awesome Brittany B did some testing to see how hard it would be to lighten my beard so it can be colored. She was great, and thinks it will be able to be done all in one session. I am currently booked to go in on Monday, August 20th to get the coloring done. Thanks again to Helue and Brittany for their help in this!"
  },
  {
    "objectID": "posts/2018-06-20-turn-roberts-beard-purple/index.html#video",
    "href": "posts/2018-06-20-turn-roberts-beard-purple/index.html#video",
    "title": "Turn Robert’s Beard Purple!",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "posts/2018-06-20-turn-roberts-beard-purple/index.html#why-a-purple-beard",
    "href": "posts/2018-06-20-turn-roberts-beard-purple/index.html#why-a-purple-beard",
    "title": "Turn Robert’s Beard Purple!",
    "section": "Why a Purple Beard??",
    "text": "Why a Purple Beard??\nI decided to participate in the Walk To End Alzheimer’s this year, coming up on August 25th here in Lexington. I will be walking the 2 miles in support of my Mom, who was diagnosed with early onset Alzheimer’s some time ago (she will turn 58 this summer).\nWhy am I walking?? Because I’ve seen a little bit of what Alzheimer’s does, and although my Mom won’t benefit from new treatments, we need to find effective treatments for this devastating disease.\nAs an incentive to donate, if I reach my fundraising goal of $100, I will have my beard dyed Alzheimer’s Association purple! The wonderful folks at Bak 4 More studios have agreed to help me dye my beard purple before the Walk date. I will make sure to take lots of video and pictures of the process, which I am told will probably involve two trips to the studio, one to lighten my beard and a second to actually apply the purple color.\nUsing the magic of computers, I’ve tried to show here what that might look like. I’m sure it will look 100X better than these, this is just to give a possible idea.\n\nI also realize that not everyone may be able to donate to the American Alzheimer’s Association, so if you want to support my fundraising for the Walk (and turn my beard purple) by donating to your local Alzheimer’s charity, just send me an email with the subject walk with the amount you donated and to which charity, and I will count your donation towards your local charity towards my goal. I will also respond to your email so that you know I’ve counted it.\nI will NOT be trimming my beard between now and the Walk date, so there will be even more purple beard to go around, with 9 weeks between now and then.\nThank you for helping me raise funds for the Walk to End Alzheimer’s, and feel free to spread this post far and wide."
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "",
    "text": "Way back in October 2020, I saw a tweet cross my feed by Esteban on making personal map art, and I was struck by their map (Moro 2020). I was also looking for an idea for my spouses birthday that was coming up in November, and I decided to do one of these maps for my lovely wife.\nIf you want to create one of these, you should definitely check out Esteban’s post (Moro 2020) for how they did it. Esteban also has figures for how things look as they go together. I’m not that organized, unfortunately.\nI’m detailing my process here, because I tried to organize it in a bit of a different way, putting as much stuff into functions as I could so I can reuse code where possible.\nYou can check out the code on GitHub (Flight 2020) and see how it’s organized.\nA short list of what you need for this project:\n\nA region of interest with mapped roads in OpenStreetMap\nA list of starting locations and ending locations that the shortest route is easily findable.\n\nEsteban and I used Google Maps takeout data.\nMy spouse and I have location data from when we started using Google phones back in August of 2016."
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#decide-on-a-size",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#decide-on-a-size",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "Decide on a Size",
    "text": "Decide on a Size\nIdeally before embarking on this project, decide what size of a print you want. If you are in the USA, WalMart’s prices are actually pretty decent, and I’ve found their quality to be good. I was very happy with the canvas I got from them back in November. The size of the print defines the ratio of the bounding box you are going to want to use and how you want it to look. I ultimately decided on a 16 in high by 20 in wide."
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#find-your-bounding-box",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#find-your-bounding-box",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "Find Your Bounding Box",
    "text": "Find Your Bounding Box\nThis is the part that takes some interactive work unless you just want to work within a particular city limits.\nI used the export function on OpenStreetMap to create my bounding box of the area I was interested in.\nI used {drake} to define a workflow for this project so any fetching of data from OpenStreetMap would only have to be done once.\n\nthe_plan <-\n  drake_plan(\n    lexington_bbx = list(min_lon = -84.7533,\n                         max_lon = -84.2143,\n                         min_lat = 37.9358,\n                         max_lat = 38.1775),\n\n    lexington_map = get_map(lexington_bbx),\n    lexington_counties = get_counties(lexington_bbx, \"KY\"),\n    lexington_water = get_us_water(lexington_bbx,\n                                   lexington_counties,\n                                   \"KY\"),\n    lexington_counties_water = combine_counties_uswater(lexington_counties, lexington_water),\n    sarah_locations = get_takeout_locations(\"saraheflight/saraheflight_takeout\"),\n    sarah_routes = get_sarah_routes(sarah_locations)\n)\n\nThis plan has the bounding box defined, fetches the map data, counties data, and any water data, merges it together, and then grabs the locations, and determines the routes."
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#fetching-map",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#fetching-map",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "Fetching Map",
    "text": "Fetching Map\nWe fetch the map data for the bounding box. Note that we fetch a ton of the road data, because that is what is likely to make it look nice. We also classify the highways and streets so that they can have a different weight in the final map.\n\nget_map = function(bbx_list){\n  bbx = rbind(x=c(bbx_list$min_lon, bbx_list$max_lon),y=c(bbx_list$min_lat, bbx_list$max_lat))\n  colnames(bbx) = c(\"min\",\"max\")\n\n  highways = bbx %>%\n    opq() %>%\n    add_osm_feature(key = \"highway\",\n                    value=c(\"motorway\", \"trunk\",\n                            \"primary\",\"secondary\",\n                            \"tertiary\",\"motorway_link\",\n                            \"trunk_link\",\"primary_link\",\n                            \"secondary_link\",\n                            \"tertiary_link\")) %>%\n    osmdata_sf()\n  streets = bbx %>%\n    opq() %>%\n    add_osm_feature(key = \"highway\",\n                    value = c(\"residential\", \"living_street\",\n                              \"service\",\"unclassified\",\n                              \"pedestrian\", \"footway\",\n                              \"track\",\"path\")) %>%\n    osmdata_sf()\n\n  list(highways = highways,\n       streets = streets)\n}"
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#adding-county-state-features-water",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#adding-county-state-features-water",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "Adding County / State Features & Water",
    "text": "Adding County / State Features & Water\nWe also want to have the data for the county and waterways (there are probably lakes and rivers or coastlines near you).\n\nget_counties = function(bbx_list, state = \"KY\"){\n\n  counties_state = counties(state=state, cb=T, class=\"sf\")\n  counties_state = st_crop(counties_state,\n                         xmin = bbx_list$min_lon, xmax = bbx_list$max_lon,\n                         ymin = bbx_list$min_lat, ymax = bbx_list$max_lat)\n  counties_state\n}\n\nget_us_water = function(bbx_list, counties_list, state){\n  get_water = function(county_GEOID, state = state){\n    area_water(state, county_GEOID, class = \"sf\")\n  }\n  water = do.call(rbind,\n                   lapply(counties_list$COUNTYFP, get_water, state))\n  water = st_crop(water,\n                   xmin = bbx_list$min_lon, xmax = bbx_list$max_lon,\n                   ymin = bbx_list$min_lat, ymax = bbx_list$max_lat)\n  water\n}\n\ncombine_counties_uswater = function(counties_state, counties_water){\n  st_difference(counties_state, st_union(counties_water))\n}\n\nThis gives us a decent image with the highways, streets, county level features, and the waterways."
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#location-data",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#location-data",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "Location Data",
    "text": "Location Data\nAs I said previously, I used the Google Maps location data from Google Takeout. I asked my spouse for the data. Be advised, it can take Google a little bit to prepare this data depending on how much there is.\nIf you didn’t have an automated source of data, you could probably set up a set of destinations replicated by how often you think you traveled there to get relative weights.\nFor this, we will parse through the takeout data and get all of the destinations.\nThe function below goes through all of the files (they are organized by year and month) and grabs the locations, and puts them into a data.frame to iterate through.\n\nget_takeout_locations = function(takeout_dir){\n\n  file2 = file.path(takeout_dir, \"Takeout\", \"Location History\", \"Semantic Location History\")\n  files = list.files(file2, pattern = \"*.json\", recursive = TRUE, full.names = TRUE)\n  get_locations = function(file, .progress = NULL){\n    knitrProgressBar::update_progress(.progress)\n    data = jsonlite::fromJSON(file)\n    tl_obj = data$timelineObjects$placeVisit\n    loc = cbind(tl_obj$location, tl_obj$duration)\n    tt = as.numeric(loc$startTimestampMs)/1000\n    loc$time=as.POSIXct(tt,origin = \"1970-01-01\")\n    #conver longitude & latitude from E7 to GPS\n    loc$lat = loc$latitudeE7 / 1e7\n    loc$lon = loc$longitudeE7 / 1e7\n    loc = data.frame(loc)\n    loc = loc[, c(\"placeId\", \"time\", \"lat\", \"lon\")]\n    loc = dplyr::filter(loc, !is.na(lon))\n    loc\n  }\n  locs_df = purrr::map_df(files, get_locations)\n  locs_df\n}"
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#routes",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#routes",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "Routes",
    "text": "Routes\nThen we have to work out the routes. For this project, it was complicated by the fact that we’ve lived in two different locations since we moved here. So this function sets two different home locations, and switches between them depending on the date of the trip.\nWe also assume that every trip is a trip between home and the destination. The locations are organized by day, so we have to do some transformations to make every trip start at home and end at the destination. Obviously that’s not how we actually travel, but otherwise I’d have to try and extract the route level data from the takeout, and that would be more of a pain. And for the kind of map we are trying to generate, this works well enough.\n\nget_sarah_routes = function(locs_df){\n  old_home = list(lat = 37.9898308, lon = -84.5054868)\n  new_home = list(lat = 37.982469, lon = -84.506552)\n  locs_df$day = lubridate::floor_date(locs_df$time, unit = \"day\")\n  locs_df = tibble::as_tibble(locs_df)\n  locs_df = dplyr::mutate(locs_df, home = dplyr::case_when(\n    day <= as.POSIXct(\"2018-03-14\") ~ list(old_home),\n    TRUE ~ list(new_home)\n  ))\n\n  split_day = split(locs_df, locs_df$day)\n\n  day_routes = purrr::map(split_day, daily_routes)\n  day_routes = do.call(rbind, day_routes)\n}\n\ndaily_routes = function(day_locations){\n  home_loc = day_locations$home[[1]]\n  use_locs = day_locations[, c(\"lat\", \"lon\")]\n  use_locs2 = rbind(data.frame(lat = home_loc$lat, lon = home_loc$lon),\n                    use_locs,\n                    data.frame(lat = home_loc$lat, lon = home_loc$lon))\n  route = NULL\n  for(irow in 2:nrow(use_locs2)){\n    p1 = c(use_locs2$lon[irow - 1], use_locs2$lat[irow - 1])\n    p2 = c(use_locs2$lon[irow], use_locs2$lat[irow])\n    oo = osrmRoute(src = p1, dst = p2, returnclass = \"sf\",\n                    overview = \"full\")\n    route <- rbind(route, oo)\n  }\n  route\n}"
  },
  {
    "objectID": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#plot-it",
    "href": "posts/2021-03-21-creating-a-map-of-routes-weighted-by-travel/index.html#plot-it",
    "title": "Creating a Map of Routes Weighted by Travel",
    "section": "Plot It!",
    "text": "Plot It!\nFinally, we put everything together into an image that can be plotted!\nThis ended up in a script because I was doing a lot of playing around with it, and when I finally got the image, I just saved the final script. You can also see here that after the fact I was trying to mess with the bounding box to get the correct aspect ratio. Don’t be like me, do it up front and figure it out.\n\nsource(\"packages.R\")\nloadd(lexington_bbx)\nloadd(lexington_map)\nnames(lexington_map)\nloadd(lexington_counties_water)\nloadd(sarah_routes)\n\nlexington_bbx = list(min_lon = -84.7533,\n                     max_lon = -84.355,\n                     min_lat = 37.9358,\n                     max_lat = 38.1775)\n\n(lexington_bbx$max_lat - lexington_bbx$min_lat) / (lexington_bbx$max_lon - lexington_bbx$min_lon)\n\ncolor_roads <- rgb(0.42,0.449,0.488)\nfinal_map = ggplot() +\n  geom_sf(data = lexington_counties_water,\n          inherit.aes = FALSE,\n          lwd= 0.0, fill = rgb(0.203,0.234,0.277)) +\n  geom_sf(data = lexington_map$streets$osm_lines,\n          inherit.aes = FALSE,\n          color=color_roads,\n          size = .4,\n          alpha = .65) +\n  geom_sf(data = lexington_map$highways$osm_lines,\n          inherit.aes = FALSE,\n          color=color_roads,\n          size = .6,\n          alpha = .65) +\n  geom_sf(data = st_geometry(sarah_routes),\n          inherit.aes = FALSE, col = \"orange\", alpha = 0.2) +\n  coord_sf(xlim = c(lexington_bbx$min_lon, lexington_bbx$max_lon),\n           ylim = c(lexington_bbx$min_lat, lexington_bbx$max_lat),\n           expand = FALSE) +\n  theme(legend.position = \"none\") + theme_void() +\n  theme(panel.background=\n          element_rect(fill = \"white\"))\n\nggsave(final_map,\n       filename = \"sarah_lexington.png\",\n       scale = 1,\n       width = 20,\n       height = 16,\n       units = \"in\",\n       bg = rgb(0.203,0.234,0.277),\n       dpi = 500)\n\nAnd what I got was this:\n\n\n\n\n\nAnd now it hangs on our wall as a canvas print:"
  },
  {
    "objectID": "posts/2022-01-06-pie-charts-in-rcy3/index.html",
    "href": "posts/2022-01-06-pie-charts-in-rcy3/index.html",
    "title": "Pie Charts in RCy3",
    "section": "",
    "text": "I have a package, {categoryCompare2} (Flight 2022) that I’ve been working on for a while, and recently wanted to make available on our labs r-universe (“The ’Moseleybioinformaticslab’ Universe,” n.d.).\nFor some of the current visualization, we use Cytoscape to examine annotation similarity graphs, coupling the R together with Cytoscape via {RCy3} (Gustavsen et al. 2019). In the previous iteration, we had used actual piechart images generated by R, and then used setNodeImageDirect to point the node images to local image files.\nHowever, the latest iteration of {RCy3} has essentially lost that functionality. However, there is a new visualization plugin that can do similar things, enhancedGraphics (Morris JH 2014).\nAlexander Pico, one of the primary {RCy3} developers, provided me with the guidance for a code solution (Pico 2022), which I’ve adapted below.\n\ngrp_matrix = matrix(c(1, 0,\n                      0, 1), nrow = 2, ncol = 2)\nn_grp <- nrow(grp_matrix)\nuse_color <- rainbow_hcl(ncol(grp_matrix), c = 100)\nn_color <- length(use_color)\nuse_color = color\n# defines how many pie segments are needed, common to all the pie-charts\npie_area <- rep(1 / n_color, n_color)\nnames(pie_area) <- rep(\"\", n_color)\n\n\ndesat_color <- desaturate(use_color)\nnames(desat_color) <- names(use_color)\npiechart_strings <- purrr::map_dfr(rownames(grp_matrix), function(i_grp){\n  tmp_logical <- grp_matrix[i_grp, ]\n  tmp_color <- use_color\n    \n  # add the proper desaturated versions of the colors\n  tmp_color[!tmp_logical] <- desat_color[!tmp_logical]\n    \n  out_str = paste0('piechart: attributelist=\"',\n                   paste(colnames(grp_matrix), collapse = ','),\n                   '\" ',\n                   'colorlist=\"',\n                   paste(tmp_color, collapse = ','),\n                   '\" ',\n                   'arcstart=-90 showlabels=false')\n  data.frame(colorlist = out_str)\n})\n\ntmp_matrix = as.data.frame(matrix(1, nrow = nrow(piechart_strings),\n                      ncol = ncol(grp_matrix)))\nnames(tmp_matrix) = colnames(grp_matrix) \npiechart_df = cbind(tmp_matrix, piechart_strings)\n\n# after merging this with the node information to\n# put the right things with the right node\n# this gives **node_vis_df** below\nRCy3::setNodeShapeDefault(\"ELLIPSE\")\nRCy3::setNodeSizeDefault(35)\nRCy3::loadTableData(node_vis_df, data.key.column = \"name\", table = \"node\", table.key.column = \"name\")\nRCy3::updateStyleMapping(\"default\",\n   RCy3::mapVisualProperty(\"NODE_CUSTOMGRAPHICS_1\", \"colorlist\", \"p\"))\n\n\n\n\n\nReferences\n\nFlight, Robert M. 2022. “categoryCompare2.” https://github.com/MoseleyBioinformaticsLab/categoryCompare2.\n\n\nGustavsen, Julia A., Pai, Shraddha, Isserlin, Ruth, Demchak, Barry, Pico, and Alexander R. 2019. “RCy3: Network Biology Using Cytoscape from Within r.” F1000Research. https://doi.org/10.12688/f1000research.20887.3.\n\n\nMorris JH, Ferrin TE, Kuchinsky A. 2014. “enhancedGraphics: A Cytoscape App for Enhanced Node Graphics.” F1000Research. https://doi.org/10.12688/f1000research.4460.1.\n\n\nPico, Alexander. 2022. “RCy3 GitHub Issue: Equivalent of setNodeImageDirect.” https://github.com/cytoscape/RCy3/issues/167#issuecomment-1004467137.\n\n\n“The ’Moseleybioinformaticslab’ Universe.” n.d. https://moseleybioinformaticslab.r-universe.dev/ui#builds.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2022,\n  author = {Robert M Flight},\n  title = {Pie {Charts} in {RCy3}},\n  date = {2022-01-06},\n  url = {https://rmflight.github.io/posts/2022-01-06-pie-charts-in-rcy3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2022. “Pie Charts in RCy3.” January 6,\n2022. https://rmflight.github.io/posts/2022-01-06-pie-charts-in-rcy3."
  },
  {
    "objectID": "posts/2021-03-26-highlighting-a-row-of-a-complexheatmap/index.html",
    "href": "posts/2021-03-26-highlighting-a-row-of-a-complexheatmap/index.html",
    "title": "Highlighting a Row of A ComplexHeatmap",
    "section": "",
    "text": "The ComplexHeatmap Bioconductor package (Gu, Eils, and Schlesner 2016; Gu 2021a, 2021b) has become my goto for visualizing sample-sample correlation heatmaps, which I use a lot. Recently, I had a report where I wanted to highlight a particular row of the heatmap. There is not an easy way that I could find to add something that wraps around a particular column. However, you can indicate that there is a grouping in the heatmap, and use that grouping to separate a sample or samples from the others.\nLets do an example:\n\nlibrary(ComplexHeatmap)\nset.seed(123)\nnr1 = 4; nr2 = 8; nr3 = 6; nr = nr1 + nr2 + nr3\nnc1 = 6; nc2 = 8; nc3 = 10; nc = nc1 + nc2 + nc3\nmat = cbind(rbind(matrix(rnorm(nr1*nc1, mean = 1,   sd = 0.5), nr = nr1),\n          matrix(rnorm(nr2*nc1, mean = 0,   sd = 0.5), nr = nr2),\n          matrix(rnorm(nr3*nc1, mean = 0,   sd = 0.5), nr = nr3)),\n    rbind(matrix(rnorm(nr1*nc2, mean = 0,   sd = 0.5), nr = nr1),\n          matrix(rnorm(nr2*nc2, mean = 1,   sd = 0.5), nr = nr2),\n          matrix(rnorm(nr3*nc2, mean = 0,   sd = 0.5), nr = nr3)),\n    rbind(matrix(rnorm(nr1*nc3, mean = 0.5, sd = 0.5), nr = nr1),\n          matrix(rnorm(nr2*nc3, mean = 0.5, sd = 0.5), nr = nr2),\n          matrix(rnorm(nr3*nc3, mean = 1,   sd = 0.5), nr = nr3))\n   )\nmat = mat[sample(nr, nr), sample(nc, nc)] # random shuffle rows and columns\nrownames(mat) = paste0(\"row\", seq_len(nr))\ncolnames(mat) = paste0(\"column\", seq_len(nc))\n\nHeatmap(mat, cluster_rows = FALSE, cluster_columns = FALSE)\n\n\n\n\nNow, lets suppose we just want to highlight row2.\nWe create a data.frame with a factor to represent the grouping:\n\nwhich_row2 = which(grepl(\"row2\", rownames(mat)))\nsplit = data.frame(x = c(rep(\"A\", which_row2 - 1), \"B\",\n                   rep(\"C\", nrow(mat) - which_row2)))\nHeatmap(mat, cluster_rows = FALSE, cluster_columns = FALSE,\n        row_split = split,\n        row_title = NULL)\n\n\n\n\nVoila! row2 is separated from the others to draw attention to it. It’s not perfect, but hopefully it’s useful to others. Note, that you can’t use clustering with this method. If you have actual dendrograms to display, this will fail, because ComplexHeatmap expects you to use a numeric argument to tell the cut height for dendrograms for splitting (Gu 2021c). Therefore, if you have dendrograms, reorder your columns and rows according to the dendrogram first and then add the splitting information and keep the clustering off.\n\n\n\n\nReferences\n\nGu, Zuguang. 2021a. “ComplexHeatmap.” https://doi.org/10.18129/B9.bioc.ComplexHeatmap.\n\n\n———. 2021b. “ComplexHeatmap Complete Reference.” https://jokergoo.github.io/ComplexHeatmap-reference/book/.\n\n\n———. 2021c. “ComplexHeatmap Complete Reference.” https://jokergoo.github.io/ComplexHeatmap-reference/book/a-single-heatmap.html#heatmap-split.\n\n\nGu, Zuguang, Roland Eils, and Matthias Schlesner. 2016. “Complex Heatmaps Reveal Patterns and Correlations in Multidimensional Genomic Data.” Bioinformatics.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2021,\n  author = {Robert M Flight},\n  title = {Highlighting a {Row} of {A} {ComplexHeatmap}},\n  date = {2021-03-26},\n  url = {https://rmflight.github.io/posts/2021-03-26-highlighting-a-row-of-a-complexheatmap},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2021. “Highlighting a Row of A\nComplexHeatmap.” March 26, 2021. https://rmflight.github.io/posts/2021-03-26-highlighting-a-row-of-a-complexheatmap."
  },
  {
    "objectID": "posts/2013-09-19-k-12-wants-scientists/index.html",
    "href": "posts/2013-09-19-k-12-wants-scientists/index.html",
    "title": "K-12 Wants Scientists!!",
    "section": "",
    "text": "It seems that the PostDoc committee here at UK has an interest in providing some information on alternative careers for PostDocs outside of academia. We all know that there are not enough PI slots at universities for all of the PhDs who are going on to do PostDocs, and many will end up in alternative (I use that term loosely) careers.\nYesterday (2013-09-18), Scott Diamond gave a seminar to PostDocs in the Medical Center at UK on getting involved in teaching science at the K-12 and college level. Scott was previously a professor here at UK, who got involved in science teaching at local schools, saw how miserable many students were in science classes, and wanted to change that. So he quit academia, got his teaching certification, and is now teaching science at a school for high-risk youth, The Learning Center, here in Fayette County.\nThis school takes 180 of the kids who are extremely truant, i.e. they like to skip school. Of course, we know that poverty is a big factor in the decision to skip school, but so is boredom from lack of engagement with the material. At The Learning Center they have been finding that if you find ways to make the material engaging (especially science), then the kids will want to be at school, learning, and they will still do well on the standard testing. Scott mentioned that these kids in regular school were truant at least 50% of the time, and at The Learning Center they have an attendance rate of 95%.\nAs scientists, we are out to make discoveries. We look at something happening in the natural world, make hypotheses, and test them. Even in bioinformatics, the goal is to use computers to analyze biological data so we can understand better how these biological systems work, and make better hypotheses and test them. This is not how science is taught in K-12 right now here in the US. In many school systems, it is merely recitation of science facts, with no actual “investigational” science going on.\nScott (and others) are working to change this. But to do it right, they need trained scientists in the system. So if you are thinking about doing something other than academia following your PostDoc, and want to help kids be truly interested in science and how it works, maybe you should consider going into teaching K-12??\nIf you want more info about The Learning Center, about their model, or about getting into teaching at this level, you can reach Scott at scott dot diamond near uky circular shape edu.\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2013,\n  author = {Robert M Flight},\n  title = {K-12 {Wants} {Scientists!!}},\n  date = {2013-09-19},\n  url = {https://rmflight.github.io/posts/2013-09-19-k-12-wants-scientists},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2013. “K-12 Wants Scientists!!” September\n19, 2013. https://rmflight.github.io/posts/2013-09-19-k-12-wants-scientists."
  },
  {
    "objectID": "posts/2019-11-06-narrower-kable-tables/index.html",
    "href": "posts/2019-11-06-narrower-kable-tables/index.html",
    "title": "Narrower PDF Kable Tables",
    "section": "",
    "text": "Don’t bother trying to roll your own function to make narrower kable tables in a PDF document, just use kableExtra."
  },
  {
    "objectID": "posts/2019-11-06-narrower-kable-tables/index.html#motivation",
    "href": "posts/2019-11-06-narrower-kable-tables/index.html#motivation",
    "title": "Narrower PDF Kable Tables",
    "section": "Motivation",
    "text": "Motivation\nI’ve been creating tables in a report where I really needed the table to fit, and because I am using PDF output, that means the tables can’t be any wider than the page. As I’m sure many readers might be aware, kable tables will gladly overrun the side of the page if they are too wide. I’ve previously used xtable tables when I’ve had this issue, but I really appreciate the simplicity of kable."
  },
  {
    "objectID": "posts/2019-11-06-narrower-kable-tables/index.html#first-solution-custom-function",
    "href": "posts/2019-11-06-narrower-kable-tables/index.html#first-solution-custom-function",
    "title": "Narrower PDF Kable Tables",
    "section": "First Solution: Custom Function",
    "text": "First Solution: Custom Function\nAfter some serious Googling, I discovered the \\tiny latex environment to change font sizes. Wrapping pandoc tables in this was a no go, but I discovered that it could be embedded within latex table output. This lead me to create a simple function that allowed me to modify latex formatted tables.\nsmaller_latex_table = function(kable_table, size = \"tiny\"){\n  split_table = strsplit(kable_table, \"\\n\", )[[1]]\n  centering_loc = grepl(\"centering\", split_table)\n  top_table = split_table[seq(1, which(centering_loc))]\n  bottom_table = split_table[seq(which(centering_loc)+1, length(split_table))]\n  new_table = c(top_table,\n                paste0(\"\\\\\", size),\n                bottom_table)\n  structure(new_table, format = \"latex\", class = \"knitr_kable\")\n}\nThis worked! And worked quite well. However, the downside to this is that because I had to explicity use latex tables, the tables didn’t stay in place anymore and floated wherever there was free space in the document. Everything I tried with this to get the tables to hold in place failed. So back to the drawing board."
  },
  {
    "objectID": "posts/2019-11-06-narrower-kable-tables/index.html#second-solution-kableextra",
    "href": "posts/2019-11-06-narrower-kable-tables/index.html#second-solution-kableextra",
    "title": "Narrower PDF Kable Tables",
    "section": "Second Solution: kableExtra",
    "text": "Second Solution: kableExtra\nBy this point I’ve spent a whole day’s worth of time trying to get this to work, just for some tables in my report. I had initially tried kableExtra on the suggestion of another StackOverflow post, but I had something odd in my latex environment, and odd things going on with tinyTex install that made some ugly tables. After re-installing tinyTex (no small feat to make it discoverable by apt installed RStudio on Linux), I finally got both smaller tables and held in place tables via kableExtra.\nTo make the tables fit the width of the page, we use latex_options = 'scale_down'.\nFor holding them to where they are declared, we use latex_options = 'HOLD_position'. However, this also requires the tex packages longtable and float, which should be declared in the yaml header.\nPutting it all together looks like this:\n## yaml header content\ntitle: \"Title\"\nauthor: \"Me\"\noutput: \n  pdf_document:\n    extra_dependencies: [\"longtable\", \"float\"]\n## table call\nknitr::kable(data.frame) %>%\n  kableExtra::kable_styling(latex_options = c(\"scale_down\", \"HOLD_position\"))\nNow the table will fit on the page, and stay where it was declared!\nI hope I can save someone else two days of trial and error and crazy Googling!"
  },
  {
    "objectID": "posts/2017-12-28-r-job-notifications-using-twitter/index.html",
    "href": "posts/2017-12-28-r-job-notifications-using-twitter/index.html",
    "title": "R Job Notifications Using Twitter",
    "section": "",
    "text": "There has been some interesting activity about getting R to send a notification somehow when a long running job is completed. The most notable entries I have seen in this category are RPushBullet for web notifications and pingr for audio notifications.\nAlthough RPushBullet looks really cool (and Dirk does great work), I wondered if there was a way to do this using a free service that I already had access to, namely twitter. See, twitter will notify you when someone else uses your handle in a tweet. If you have twitter notifications on your device, it should also appear on a mobile device, and if you are using tweetdeck on a desktop or laptop machine, you can also set up to get notifications there.\nHowever, the default method of authenticating and caching oauth tokens for twitteR does not seem to be really useful for job notifications. However, we will take some precautions so that it is not too big a deal, and still be really useful."
  },
  {
    "objectID": "posts/2017-12-28-r-job-notifications-using-twitter/index.html#how",
    "href": "posts/2017-12-28-r-job-notifications-using-twitter/index.html#how",
    "title": "R Job Notifications Using Twitter",
    "section": "How??",
    "text": "How??\nIn a nutshell, using a specific twitter account and app that have credentials stored in an R package. Below are the steps I used to make this happen. You can see my job notification twitter user rmf_notifier and the package I created.\n\nInstall twitteR\nBefore you begin, you should have a modern version of R (all of this was initially done using v 3.0.1), devtools, and the twitteR package from github.\n\nlibrary(devtools)\ninstall_github('twitteR', username='goeffjentry')\n\n\n\nSetup a new package\nWe are going to create a personal package solely for sending twitter notifications and storing the API credentials.\nWarning: Hadley Wickham in the httr token caching documentation advises against storing token caches in a package, but this is part of the reason we are creating a twitter user and app solely for this purpose (this makes it easier to revoke tokens, or remove the app, delete the user, etc if something goes wrong). This package also should never be published with the .rdata file included, and that file should never be under version control. If any of these things happen, someone else may be able to hijack this twitter account.\nYou should create your package (create a directory with DESCRIPTION, R directory, data directory, and NAMESPACE file). Look up how to do this if you are not sure. Set your working directory to be your package base directory for all further steps.\nAll following steps assume that you are working in the base package directory.\n\n\nTwitter Account\nWe are going to create a twitter account and app specifically for sending this one type of notification. You can register a new twitter account using a new email (if you use gmail you can add a dot (.) anywhere in your email address for a unique address that still reaches you) and setup a new user name.\nAfter setting up your new account, log in to https://apps.twitter.com, and create new app. Fill in all the necessary details, and when you have it up, modify app permissions to be Read and write. This will allow it to actually send messages on the accounts behalf.\n\n\nCredentials\nClick on the API Keys tab, and set up your api data by copying the values into the code below. Note if you dont see a token and a secret, try hitting test oauth to generate one.\n\napikey <- \"\" #API Key\napisecret <- \"\" #API Secret\ntoken <- \"\" #Access Token\ntokensecret <- \"\" #Access token secret\n\nAnd now we will authorize our app and make sure that it can tweet.\n\nlibrary(twitteR)\nsetup_twitter_oauth(apikey, apisecret, token, tokensecret)\ntweet(\"this is a test\") # make sure you can see a tweet\ntweet(\"@username this is a test\") # check that you see notifications, change @username to your own username\n\nAnd then save the cache for later use.\n\nlocal_cache <- get(\"oauth_tken\", twitteR:::oauth_cache) # saves the oauth token so we can reuse it\nsave(local_cache, file=\"data/oauth_cache.RData\")\n\n\n\nUsing saved credentials\nTo make sure that our package uses the saved credentials every time, we will include a .onLoad function that sets the oauth cache up properly. This should go in the file R/zzz.R\n\n.onLoad <- function(libname, pkgname){\n  cachedToken <- new.env()\n  dataFile <- system.file(\"data/oauth_cache.RData\", package=\"packageName\")\n  load(dataFile, cachedToken)\n  assign(\"oauth_token\", cachedToken$local_cache, envir=twitteR:::oauth_cache)\n  rm(cachedToken)\n}\n\nNotice that this function loads the credentials into a particular environment, and then sets the oauth_token variable in the twitteR:::oauth_cache environment to our saved credentials.\n\n\nOur Notifying Function\nFinally, we need a function that we can use to notify us when something happens. One could simply importFrom(twitteR, tweet) and export(tweet) in the namespace, but why should we have to type the username every time? This should go in R/packageFunction.R.\n\n#' notifies job status\n#' \n#' sends a tweet to rmflight the job status\n#' \n#' @param tweetText the text to include in the tweet\n#' @export\n#' @importFrom twitteR tweet\n#' @importFrom lubridate now\njobNotify <- function(tweetText, addnow=TRUE){\n  t <- \"\"\n  if (addnow){\n    t <- now()\n  }\n  fullTweet <- paste(\"@username\", tweetText, t, sep=\" \") # change @username to where you want to recieve notifications\n  tweet(fullTweet)\n}\n\n\nImportance of Including Time\nIn my initial implementation, I did not include the date-time string. Upon actual usage, I noticed that twitter would reject subsequent tweets with identical text. The simplest way to get around this is to add the date-time string to the tweet, thereby making it unique.\n\n\n\nTest it\nAnd there you go. After building and installing your new package and loading it, you should be able to do:\n\nlibrary(packageName) # change to the name of your own package\njobNotify(\"this is a test\")\n\nAnd get a notification. Now you can simply put the above code at the end of any long running jobs, and voila, you are getting notifications from twitter about your R jobs.\nYou are subject to twitters app limits, so be careful how you use this. If you had lots of mini jobs, you would want to put this after all of them were finished, not have each one call this."
  },
  {
    "objectID": "posts/2017-12-28-r-job-notifications-using-twitter/index.html#extensions",
    "href": "posts/2017-12-28-r-job-notifications-using-twitter/index.html#extensions",
    "title": "R Job Notifications Using Twitter",
    "section": "Extensions",
    "text": "Extensions\nI would like to find a way to extend this to watching jobs and sending a notice at particular levels of completion, and also be able to have a try-catch that catches an error and can tweet that the job error’d out. But this level is still rather useful I think."
  },
  {
    "objectID": "posts/2017-12-28-r-job-notifications-using-twitter/index.html#edit",
    "href": "posts/2017-12-28-r-job-notifications-using-twitter/index.html#edit",
    "title": "R Job Notifications Using Twitter",
    "section": "Edit",
    "text": "Edit\nOn July 3, 2014 I modified the jobNotify function to also include a date-time string, and added an explanation of why that was necessary."
  },
  {
    "objectID": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html",
    "href": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html",
    "title": "Random Forest vs PLS on Random Data",
    "section": "",
    "text": "Partial least squares (PLS) discriminant-analysis (DA) can ridiculously over fit even on completely random data. The quality of the PLS-DA model can be assessed using cross-validation, but cross-validation is not typically performed in many metabolomics publications. Random forest, in contrast, because of the forest of decision tree learners, and the out-of-bag (OOB) samples used for testing each tree, automatically provides an indication of the quality of the model."
  },
  {
    "objectID": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#why",
    "href": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#why",
    "title": "Random Forest vs PLS on Random Data",
    "section": "Why?",
    "text": "Why?\nI’ve recently been working on some machine learning work using random forests (RF) Breimann, 2001 on metabolomics data. This has been relatively successful, with decent sensitivity and specificity, and hopefully I’ll be able to post more soon. However, PLS (Wold, 1975) is a standard technique used in metabolomics due to the prevalence of analytical chemists in metabolomics and a long familiarity with the method. Importantly, my collaborators frequently use PLS-DA to generate plots to show that the various classes of samples are separable.\nHowever, it has long been known that PLS (and all of it’s variants, PLS-DA, OPLS, OPLS-DA, etc) can easily generate models that over fit the data, and that over fitting of the model needs to be assessed if the model is going to be used in subsequent analyses."
  },
  {
    "objectID": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#random-data",
    "href": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#random-data",
    "title": "Random Forest vs PLS on Random Data",
    "section": "Random Data",
    "text": "Random Data\nTo illustrate the behavior of both RF and PLS-DA, we will generate some random data where each of the samples are randomly assigned to one of two classes.\n\nFeature Intensities\nWe will generate a data set with 1000 features, where each feature’s mean value is from a uniform distribution with a range of 1-10000.\n\nlibrary(ggplot2)\ntheme_set(cowplot::theme_cowplot())\nlibrary(fakeDataWithError)\nset.seed(1234)\nn_point <- 1000\nmax_value <- 10000\ninit_values <- runif(n_point, 0, max_value)\n\n\ninit_data <- data.frame(data = init_values)\nggplot(init_data, aes(x = data)) + geom_histogram() + ggtitle(\"Initial Data\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFor each of these features, their distribution across samples will be based on a random normal distribution where the mean is the initial feature value and a standard deviation of 200. The number of samples is 100.\n\nn_sample <- 100\nerror_values <- add_uniform_noise(n_sample, init_values, 200)\n\nJust for information, the add_uniform_noise function is this:\n\nadd_uniform_noise\n\nfunction (n_rep, value, sd, use_zero = FALSE) \n{\n    n_value <- length(value)\n    n_sd <- n_rep * n_value\n    out_sd <- rnorm(n_sd, 0, sd)\n    out_sd <- matrix(out_sd, nrow = n_value, ncol = n_rep)\n    if (!use_zero) {\n        tmp_value <- matrix(value, nrow = n_value, ncol = n_rep, \n            byrow = FALSE)\n        out_value <- tmp_value + out_sd\n    }\n    else {\n        out_value <- out_sd\n    }\n    return(out_value)\n}\n<bytecode: 0x557b6812fbf8>\n<environment: namespace:fakeDataWithError>\n\n\nI created it as part of a package that is able to add different kinds of noise to data.\nThe distribution of values for a single feature looks like this:\n\nerror_data <- data.frame(feature_1 = error_values[1,])\nggplot(error_data, aes(x = feature_1)) + geom_histogram() + ggtitle(\"Error Data\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAnd we will assign the first 50 samples to class_1 and the second 50 samples to class_2.\n\nsample_class <- rep(c(\"class_1\", \"class_2\"), each = 50)\nsample_class\n\n  [1] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n  [8] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [15] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [22] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [29] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [36] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [43] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [50] \"class_1\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [57] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [64] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [71] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [78] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [85] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [92] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [99] \"class_2\" \"class_2\""
  },
  {
    "objectID": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#pca",
    "href": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#pca",
    "title": "Random Forest vs PLS on Random Data",
    "section": "PCA",
    "text": "PCA\nJust to show that the data is pretty random, lets use principal components analysis (PCA) to do a decomposition, and plot the first two components:\n\ntmp_pca <- prcomp(t(error_values), center = TRUE, scale. = TRUE)\npca_data <- as.data.frame(tmp_pca$x[, 1:2])\npca_data$class <- as.factor(sample_class)\nggplot(pca_data, aes(x = PC1, y = PC2, color = class)) + geom_point(size = 4)"
  },
  {
    "objectID": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#random-forest",
    "href": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#random-forest",
    "title": "Random Forest vs PLS on Random Data",
    "section": "Random Forest",
    "text": "Random Forest\nLet’s use RF first, and see how things look.\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nrf_model <- randomForest(t(error_values), y = as.factor(sample_class))\n\nThe confusion matrix comparing actual vs predicted classes based on the out of bag (OOB) samples:\n\nknitr::kable(rf_model$confusion)\n\n\n\n\n\nclass_1\nclass_2\nclass.error\n\n\n\n\nclass_1\n21\n29\n0.58\n\n\nclass_2\n28\n22\n0.56\n\n\n\n\n\nAnd an overall error of 0.5760364."
  },
  {
    "objectID": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#pls-da",
    "href": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#pls-da",
    "title": "Random Forest vs PLS on Random Data",
    "section": "PLS-DA",
    "text": "PLS-DA\nSo PLS-DA is really just PLS with y variable that is binary.\n\nlibrary(caret)\n\nLoading required package: lattice\n\npls_model <- plsda(t(error_values), as.factor(sample_class), ncomp = 2)\npls_scores <- data.frame(comp1 = pls_model$scores[,1], comp2 = pls_model$scores[,2], class = sample_class)\n\nAnd plot the PLS scores:\n\nggplot(pls_scores, aes(x = comp1, y = comp2, color = class)) + geom_point(size = 4) + ggtitle(\"PLS-DA of Random Data\")\n\n\n\n\nAnd voila! Perfectly separated data! If I didn’t tell you that it was random, would you suspect it?"
  },
  {
    "objectID": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#cross-validated-pls-da",
    "href": "posts/2015-12-12-random-forest-vs-pls-on-random-data/index.html#cross-validated-pls-da",
    "title": "Random Forest vs PLS on Random Data",
    "section": "Cross-validated PLS-DA",
    "text": "Cross-validated PLS-DA\nOf course, one way to truly assess the worth of the model would be to use cross-validation, where a fraction of data is held back, and the model trained on the rest. Predictions are then made on the held back fraction, and because we know the truth, we will then calculate the area under the reciever operator curve (AUROC) or area under the curve (AUC) created by plotting true positives vs false positives.\nTo do this we will need two functions:\n\nGenerates all of the CV folds\nGenerates PLS-DA model, does prediction on hold out, calculates AUC\n\n\nlibrary(cvTools)\n\nLoading required package: robustbase\n\nlibrary(ROCR)\n\ngen_cv <- function(xdata, ydata, nrep, kfold){\n  n_sample <- length(ydata)\n  all_index <- seq(1, n_sample)\n  cv_data <- cvFolds(n_sample, K = kfold, R = nrep, type = \"random\")\n  \n  rep_values <- vapply(seq(1, nrep), function(in_rep){\n    use_rep <- cv_data$subsets[, in_rep]\n    cv_values <- vapply(seq(1, kfold), function(in_fold){\n      test_index <- use_rep[cv_data$which == in_fold]\n      train_index <- all_index[-test_index]\n      \n      plsda_cv(xdata[train_index, ], ydata[train_index], xdata[test_index, ],\n               ydata[test_index])\n    }, numeric(1))\n  }, numeric(kfold))\n}\n\nplsda_cv <- function(xtrain, ytrain, xtest, ytest){\n  pls_model <- plsda(xtrain, ytrain, ncomp = 2)\n  pls_pred <- predict(pls_model, xtest, type = \"prob\")\n  \n  use_pred <- pls_pred[, 2, 1]\n  \n  pred_perf <- ROCR::prediction(use_pred, ytest)\n  pred_auc <- ROCR::performance(pred_perf, \"auc\")@y.values[[1]]\n  return(pred_auc)\n}\n\nAnd now lets do a bunch of replicates (100).\n\ncv_vals <- gen_cv(t(error_values), factor(sample_class), nrep = 100, kfold = 5)\n\nmean(cv_vals)\n\n[1] 0.4260387\n\nsd(cv_vals)\n\n[1] 0.1188491\n\ncv_frame <- data.frame(auc = as.vector(cv_vals))\nggplot(cv_frame, aes(x = auc)) + geom_histogram(binwidth = 0.01)\n\n\n\n\nSo we get an average AUC of 0.4260387, which is pretty awful. This implies that even though there was good separation on the scores, maybe the model is not actually that good, and we should be cautious of any predictions being made.\nOf course, the PCA at the beginning of the analysis shows that there is no real separation in the data in the first place.\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       Pop!_OS 22.04 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US:en\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2022-12-02\n pandoc   2.19.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   assertthat          0.2.1      2019-03-21 [2] CRAN (R 4.2.1)\n P cachem              1.0.6      2021-08-19 [?] CRAN (R 4.2.1)\n   callr               3.7.2      2022-08-22 [2] CRAN (R 4.2.1)\n P caret             * 6.0-93     2022-08-09 [?] CRAN (R 4.2.1)\n P class               7.3-20     2022-01-13 [?] CRAN (R 4.2.1)\n   cli                 3.4.0      2022-09-08 [2] CRAN (R 4.2.1)\n   codetools           0.2-18     2020-11-04 [2] CRAN (R 4.2.1)\n   colorspace          2.0-3      2022-02-21 [2] CRAN (R 4.2.1)\n   cowplot             1.1.1      2020-12-30 [2] CRAN (R 4.2.1)\n   crayon              1.5.1      2022-03-26 [2] CRAN (R 4.2.1)\n P cvTools           * 0.3.2      2012-05-14 [?] CRAN (R 4.2.1)\n   data.table          1.14.2     2021-09-27 [2] CRAN (R 4.2.1)\n   DBI                 1.1.3      2022-06-18 [2] CRAN (R 4.2.1)\n   DEoptimR            1.0-11     2022-04-03 [2] CRAN (R 4.2.1)\n   devtools            2.4.4      2022-07-20 [2] CRAN (R 4.2.1)\n P digest              0.6.29     2021-12-01 [?] CRAN (R 4.2.1)\n   dplyr               1.0.10     2022-09-01 [2] CRAN (R 4.2.1)\n   ellipsis            0.3.2      2021-04-29 [2] CRAN (R 4.2.1)\n P evaluate            0.16       2022-08-09 [?] CRAN (R 4.2.1)\n   fakeDataWithError * 0.0.1      2022-12-02 [1] Github (rmflight/fakeDataWithError@ccd8714)\n   fansi               1.0.3      2022-03-24 [2] CRAN (R 4.2.1)\n   farver              2.1.1      2022-07-06 [2] CRAN (R 4.2.1)\n P fastmap             1.1.0      2021-01-25 [?] CRAN (R 4.2.1)\n   foreach             1.5.2      2022-02-02 [2] CRAN (R 4.2.1)\n P fs                  1.5.2      2021-12-08 [?] CRAN (R 4.2.1)\n   future              1.28.0     2022-09-02 [2] CRAN (R 4.2.1)\n P future.apply        1.10.0     2022-11-05 [?] CRAN (R 4.2.1)\n   generics            0.1.3      2022-07-05 [2] CRAN (R 4.2.1)\n P ggplot2           * 3.4.0      2022-11-04 [?] CRAN (R 4.2.1)\n   globals             0.16.1     2022-08-28 [2] CRAN (R 4.2.1)\n P glue                1.6.2      2022-02-24 [?] CRAN (R 4.2.1)\n P gower               1.0.0      2022-02-03 [?] CRAN (R 4.2.1)\n   gtable              0.3.1      2022-09-01 [2] CRAN (R 4.2.1)\n P hardhat             1.2.0      2022-06-30 [?] CRAN (R 4.2.1)\n P highr               0.9        2021-04-16 [?] CRAN (R 4.2.1)\n P htmltools           0.5.3      2022-07-18 [?] CRAN (R 4.2.1)\n   htmlwidgets         1.5.4      2021-09-08 [2] CRAN (R 4.2.1)\n   httpuv              1.6.6      2022-09-08 [2] CRAN (R 4.2.1)\n P ipred               0.9-13     2022-06-02 [?] CRAN (R 4.2.1)\n   iterators           1.0.14     2022-02-05 [2] CRAN (R 4.2.1)\n P jsonlite            1.8.0      2022-02-22 [?] CRAN (R 4.2.1)\n P knitr               1.40       2022-08-24 [?] CRAN (R 4.2.1)\n   labeling            0.4.2      2020-10-20 [2] CRAN (R 4.2.1)\n   later               1.3.0      2021-08-18 [2] CRAN (R 4.2.1)\n   lattice           * 0.20-45    2021-09-22 [2] CRAN (R 4.2.1)\n P lava                1.7.0      2022-10-25 [?] CRAN (R 4.2.1)\n P lifecycle           1.0.3      2022-10-07 [?] CRAN (R 4.2.1)\n   listenv             0.8.0      2019-12-05 [2] CRAN (R 4.2.1)\n   lubridate           1.8.0      2021-10-07 [2] CRAN (R 4.2.1)\n P magrittr            2.0.3      2022-03-30 [?] CRAN (R 4.2.1)\n   MASS                7.3-58.1   2022-08-03 [2] CRAN (R 4.2.1)\n   Matrix              1.4-1      2022-03-23 [2] CRAN (R 4.2.1)\n P memoise             2.0.1      2021-11-26 [?] CRAN (R 4.2.1)\n   mime                0.12       2021-09-28 [2] CRAN (R 4.2.1)\n   miniUI              0.1.1.1    2018-05-18 [2] CRAN (R 4.2.1)\n P ModelMetrics        1.2.2.2    2020-03-17 [?] CRAN (R 4.2.1)\n   munsell             0.5.0      2018-06-12 [2] CRAN (R 4.2.1)\n   nlme                3.1-159    2022-08-09 [2] CRAN (R 4.2.1)\n P nnet                7.3-18     2022-09-28 [?] CRAN (R 4.2.1)\n   parallelly          1.32.1     2022-07-21 [2] CRAN (R 4.2.1)\n   pillar              1.8.1      2022-08-19 [2] CRAN (R 4.2.1)\n   pkgbuild            1.3.1      2021-12-20 [2] CRAN (R 4.2.1)\n   pkgconfig           2.0.3      2019-09-22 [2] CRAN (R 4.2.1)\n   pkgload             1.3.0      2022-06-27 [2] CRAN (R 4.2.1)\n P pls                 2.8-1      2022-07-16 [?] CRAN (R 4.2.1)\n   plyr                1.8.7      2022-03-24 [2] CRAN (R 4.2.1)\n   prettyunits         1.1.1      2020-01-24 [2] CRAN (R 4.2.1)\n P pROC                1.18.0     2021-09-03 [?] CRAN (R 4.2.1)\n   processx            3.7.0      2022-07-07 [2] CRAN (R 4.2.1)\n P prodlim             2019.11.13 2019-11-17 [?] CRAN (R 4.2.1)\n   profvis             0.3.7      2020-11-02 [2] CRAN (R 4.2.1)\n   promises            1.2.0.1    2021-02-11 [2] CRAN (R 4.2.1)\n   ps                  1.7.1      2022-06-18 [2] CRAN (R 4.2.1)\n   purrr               0.3.4      2020-04-17 [2] CRAN (R 4.2.1)\n P R6                  2.5.1      2021-08-19 [?] CRAN (R 4.2.1)\n P randomForest      * 4.7-1.1    2022-05-23 [?] CRAN (R 4.2.1)\n   Rcpp                1.0.9      2022-07-08 [2] CRAN (R 4.2.1)\n P recipes             1.0.3      2022-11-09 [?] CRAN (R 4.2.1)\n   remotes             2.4.2      2021-11-30 [2] CRAN (R 4.2.1)\n   renv                0.15.5     2022-05-26 [1] CRAN (R 4.2.1)\n P reshape2            1.4.4      2020-04-09 [?] CRAN (R 4.2.1)\n P rlang               1.0.6      2022-09-24 [?] CRAN (R 4.2.1)\n P rmarkdown           2.16       2022-08-24 [?] CRAN (R 4.2.1)\n   robustbase        * 0.95-0     2022-04-02 [2] CRAN (R 4.2.1)\n P ROCR              * 1.0-11     2020-05-02 [?] CRAN (R 4.2.1)\n P rpart               4.1.19     2022-10-21 [?] CRAN (R 4.2.1)\n   rstudioapi          0.14       2022-08-22 [2] CRAN (R 4.2.1)\n   scales              1.2.1      2022-08-20 [2] CRAN (R 4.2.1)\n   sessioninfo         1.2.2      2021-12-06 [2] CRAN (R 4.2.1)\n   shiny               1.7.2      2022-07-19 [2] CRAN (R 4.2.1)\n P stringi             1.7.8      2022-07-11 [?] CRAN (R 4.2.1)\n P stringr             1.4.1      2022-08-20 [?] CRAN (R 4.2.1)\n P survival            3.4-0      2022-08-09 [?] CRAN (R 4.2.1)\n   tibble              3.1.8      2022-07-22 [2] CRAN (R 4.2.1)\n P tidyselect          1.2.0      2022-10-10 [?] CRAN (R 4.2.1)\n P timeDate            4021.106   2022-09-30 [?] CRAN (R 4.2.1)\n   urlchecker          1.0.1      2021-11-30 [2] CRAN (R 4.2.1)\n   usethis             2.1.6      2022-05-25 [2] CRAN (R 4.2.1)\n   utf8                1.2.2      2021-07-24 [2] CRAN (R 4.2.1)\n P vctrs               0.5.1      2022-11-16 [?] CRAN (R 4.2.1)\n   withr               2.5.0      2022-03-03 [2] CRAN (R 4.2.1)\n P xfun                0.33       2022-09-12 [?] CRAN (R 4.2.1)\n   xtable              1.8-4      2019-04-21 [2] CRAN (R 4.2.1)\n P yaml                2.3.5      2022-02-21 [?] CRAN (R 4.2.1)\n\n [1] /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu\n [2] /rmflight_stuff/software/R-4.2.1/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/2021-03-05-randomcodesnippets/index.html",
    "href": "posts/2021-03-05-randomcodesnippets/index.html",
    "title": "Random Code Snippets",
    "section": "",
    "text": "I’ve been organizing a bunch of random code that I find myself re-using time and time again in a Github Repo (Flight, n.d.). But the repo is not very searchable unless you are the one who maintains it, and some of the categories are not well defined.\nThis blog already contains a few posts in this vein, where I’ve previously described, including these for example:\n\nCreating custom Affymetrix CDF definitions (Flight 2012)\nCustom commit hooks to increment R package version numbers (Flight 2014)\nWriting a custom blog deployment script (Flight 2017)\n\nSo, in that vein, I’ve decided to organize those posts here, and move more of the code from the github repo from there to here, where it should be easier to find and search, as they can be organized under the one page, but have multiple tags to make it easier to find what you (or really myself) are looking for.\nThese will all share the category random-code-snippets, and be listed at Random Code Snippets. I hope others find these as useful as I do.\n\n\n\n\nReferences\n\nFlight, Robert M. 2012. “Deciphering Life: One Bit at a Time: Creating Custom CDFs for Affymetrix Chips in Bioconductor.” https://rmflight.github.io/posts/2012-07-13-creating-custom-cdf-s-for-affymetrix-chips-in-bioconductor/.\n\n\n———. 2014. “Deciphering Life: One Bit at a Time: Package Version Increment Pre- and Post-Commit Hooks.” https://rmflight.github.io/posts/2014-02-07-package-version-increment-pre-and-post-commit-hooks/.\n\n\n———. 2017. “Deciphering Life: One Bit at a Time: Custom Deployment Script.” https://rmflight.github.io/posts/2017-12-27-custom-deployment-script/.\n\n\n———. n.d. “Resources.” https://github.com/rmflight/resources.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2021,\n  author = {Robert M Flight},\n  title = {Random {Code} {Snippets}},\n  date = {2021-03-05},\n  url = {https://rmflight.github.io/posts/2021-03-05-randomcodesnippets},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2021. “Random Code Snippets.” March 5,\n2021. https://rmflight.github.io/posts/2021-03-05-randomcodesnippets."
  },
  {
    "objectID": "posts/2013-09-23-portable-peronal-packages/index.html",
    "href": "posts/2013-09-23-portable-peronal-packages/index.html",
    "title": "Portable, Personal Packages",
    "section": "",
    "text": "ProgrammingR had an interesting post recently about keeping a set of R functions that are used often as a gist on Github, and sourceing that file at the beginning of R analysis scripts. There is nothing inherently wrong with this, but it does end up cluttering the user workspace, and there is no real documentation on the functions, and no good way to implement unit testing.\nHowever, the best way to have sets of R functions is as a package, that can then be installed and loaded by anyone. Normally packages are compiled and hosted on CRAN, R-forge, or Bioconductor. However, Github is becoming a common place to host packages, and thanks to Hadley Wickham’s install_github function in the devtools package, it is rather easy to install a package directly from Github. This does require that you have r-tools installed if you are using Windows (I know that can take a bit of work, but it’s not impossible), and do some extra work to create a proper package, but the overhead is probably worth it if you are using these functions all the time.\nOnce you have the package created and hosted on Github, it is simple to install it once, and load it when needed. If there is a particular version of the package that is required, it is even possible to tell install_github to install a particular version based on the commit, or a tag.\nSome examples of this type of package can be found on Github: 1 2 3\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2013,\n  author = {Robert M Flight},\n  title = {Portable, {Personal} {Packages}},\n  date = {2013-09-23},\n  url = {https://rmflight.github.io/posts/2013-09-23-portable-peronal-packages},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2013. “Portable, Personal Packages.”\nSeptember 23, 2013. https://rmflight.github.io/posts/2013-09-23-portable-peronal-packages."
  },
  {
    "objectID": "posts/2021-08-30-random-forest-classification-using-parsnip/index.html",
    "href": "posts/2021-08-30-random-forest-classification-using-parsnip/index.html",
    "title": "Random Forest Classification Using Parsnip",
    "section": "",
    "text": "I’ve been working on a “machine learning” project, and in the process I’ve been learning to use the tidymodels framework (“Tidymodels” 2021), which helps save you from leaking information from testing to training data, as well as creating workflows in a consistent way across methods.\nHowever, I got tripped up recently by one issue. When I’ve previously used Random Forests (“Random Forest Wiki Page” 2021), I’ve found that for classification problems, the out-of-bag (OOB) error reported is a good proxy for the area-under-the-curve (AUC), or estimate of how good any other machine learning technique will do (see (Flight 2015) for an example using actual random data). Therefore, I like to put my data through a Random Forest algorithm and check the OOB error, and then maybe reach for a tuned boosted tree to squeeze every last bit of performance out.\ntidymodels default is to use a probability tree, even for classification problems. This isn’t normally a problem for most people, because you will have a train and test set, and estimate performance on the test set using AUC. However, it is a problem if you just want to see the OOB error from the random forest, because it is reported differently for probability vs classification.\nLets run an example using the tidymodels cell data set.\n\nlibrary(tidymodels)\nlibrary(modeldata)\nlibrary(skimr)\ndata(cells, package = \"modeldata\")\nlibrary(ranger)\ntidymodels_prefer()\n\ncells$case = NULL\nset.seed(1234)\nranger(class ~ ., data = cells, min.node.size = 10, classification = TRUE)\n\nRanger result\n\nCall:\n ranger(class ~ ., data = cells, min.node.size = 10, classification = TRUE) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             17.29 % \n\n\nHere we can see that we get an OOB error of 17%, which isn’t too shabby. Now, let’s setup a workflow to do the same thing via tidymodels parsnip.\n\nrf_spec = rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nrf_recipe = recipe(class ~ ., data = cells) %>%\n  step_dummy(class, -class)\n\nset.seed(1234)\nworkflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_spec) %>%\n  fit(data = cells)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1198456 \n\n\nHere we see the OOB error is 12% (0.119), which is not significantly different than the 17% above, but still different. Also, the “Type” shows “Probability estimation” instead of “Classification estimation”.\nIf we run ranger again with a “probability” instead of “classification”, do we match up with the result above?\n\nset.seed(1234)\nranger(class ~ ., data = cells, min.node.size = 10, probability = TRUE)\n\nRanger result\n\nCall:\n ranger(class ~ ., data = cells, min.node.size = 10, probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.119976 \n\n\nThat is much closer to the tidymodels result! Great! Except, it’s a misestimation of the true OOB error for classification. How do we get what we want while using the tidymodels framework?\nI couldn’t find the answer, and the above looked like a bug, so I filed one on the parsnip github (“Ranger \"Classification\" Mode Still Looks Like \"Probability\"” 2021). Julia Silge helpfully provided the solution to my problem.\n\nrf_spec_class = rand_forest() %>%\n  set_engine(\"ranger\", probability = FALSE) %>%\n  set_mode(\"classification\")\n\nset.seed(1234)\nworkflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_spec_class) %>%\n  fit(data = cells)\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, probability = ~FALSE,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             16.54 % \n\n\nAha! Now we are much closer to the original value of 17%, and the “Type” is “Classification”.\nI know in this case, the differences in OOB error are honestly not that much different, but in my recent project, they differed by 20%, where I had a 45% using classification, and 25% using probability. Therefore, I was being fooled by the tidymodels framework investigation, and then wondering why my final AUC on a tuned model was only hitting just > 55%.\nSo remember, this isn’t how I would run the model for final classification and estimation of AUC on a test set, but if you want the OOB errors for a quick “feel” of your data, it’s very useful.\n\n\n\n\nReferences\n\nFlight, Robert M. 2015. “Deciphering Life: One Bit at a Time: Random Forest Vs PLS on Random Data.” https://rmflight.github.io/posts/2015-12-12-random-forest-vs-pls-on-random-data/.\n\n\n“Random Forest Wiki Page.” 2021. https://en.wikipedia.org/wiki/Random_forest.\n\n\n“Ranger \"Classification\" Mode Still Looks Like \"Probability\".” 2021. https://github.com/tidymodels/parsnip/issues/546.\n\n\n“Tidymodels.” 2021. https://www.tidymodels.org/.\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{mflight2021,\n  author = {Robert M Flight},\n  title = {Random {Forest} {Classification} {Using} {Parsnip}},\n  date = {2021-08-30},\n  url = {https://rmflight.github.io/posts/2021-08-30-random-forest-classification-using-parsnip},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobert M Flight. 2021. “Random Forest Classification Using\nParsnip.” August 30, 2021. https://rmflight.github.io/posts/2021-08-30-random-forest-classification-using-parsnip."
  }
]